[
  {
    "objectID": "publications.html",
    "href": "publications.html",
    "title": "Publications",
    "section": "",
    "text": "Publications\n\n\n\n\n\nAbu Arqub, S., Greene, R., Greene, S., Laing, K., Kuo, C.-L., Godoy, L. da C., & Uribe, F. (2023). Ridge mini-implants, a versatile biomechanical anchorage device whose success is significantly enhanced by splinting: A clinical report. Progress in Orthodontics, 24(1), 27.\n\n\nAbu-Arqub, S., Ahmida, A., Godoy, L. da C., Kuo, C.-L., Upadhyay, M., & Yadav, S. (2023). Insight into clear aligner therapy protocols and preferences among members of the american association of orthodontists in the united states and canada. The Angle Orthodontist.\n\n\nAndreyeva, T., Moore, T. E., Godoy, L. da C., & Kenney, E. L. (2023). Federal nutrition assistance for young children: Underutilized and unequally accessed. American Journal of Preventive Medicine. https://doi.org/https://doi.org/10.1016/j.amepre.2023.09.008\n\n\nArqub, S. A., Bashir, R., Obeng, K., Godoy, L. da C., Kuo, C.-L., Upadhyay, M., & Yadav, S. (2023). Survival and failure rate of lower lingual bonded retainers: A retrospective cohort evaluation. Orthodontics & Craniofacial Research, 26(2), 256‚Äì264. https://doi.org/https://doi.org/10.1111/ocr.12608\n\n\nDuong, C., Zhu, Q., Aseltine Jr, R. H., Kuo, C.-L., Godoy, L. da C., & Kaufman, B. (2023). A survey on cone-beam computed tomography usage among endodontists in the united states. Journal of Endodontics.\n\n\nFrank, S., Ibrahim, B., Feng, R., Bidra, A., Lafreniere, D., Kuo, C.-L., Godoy, L. da C., & Falcone, T. E. (2023). Tolerability of nasal and oral povidone-iodine antisepsis for in-office procedures. Clinical Otolaryngology, 48(4), 696‚Äì699. https://doi.org/https://doi.org/10.1111/coa.14045\n\n\nFryc, G. A., Godoy, L. da C., Kuo, C.-L., & Lurie, A. G. (2023). Prevalence of likely retro-odontoid pseudotumor in patients receiving dental CBCT examinations. Oral Surgery, Oral Medicine, Oral Pathology and Oral Radiology. https://doi.org/https://doi.org/10.1016/j.oooo.2023.11.005\n\n\nHarandi, M. T., Abu Arqub, S., Warren, E., Kuo, C.-L., Godoy, L. da C., Mehta, S., Feldman, J., Upadhyay, M., & Yadav, S. (2023). Assessment of clear aligner accuracy of 2 clear aligners systems. American Journal of Orthodontics and Dentofacial Orthopedics. https://doi.org/https://doi.org/10.1016/j.ajodo.2023.05.028\n\n\nJesus, M. de, Maheshwary, A., Kumar, M., Cunha Godoy, L. da, Kuo, C.-L., & Grover, P. (2023). Association of electrocardiographic and echocardiographic variables with neurological outcomes after ischemic stroke. American Heart Journal Plus: Cardiology Research and Practice, 100313.\n\n\nKuo, C.-L., Liu, R., Godoy, L. da C., Pilling, L. C., Fortinsky, R. H., & Brugge, D. (2023). Association between residential exposure to air pollution and incident coronary heart disease is not mediated by leukocyte telomere length: A UK biobank study. Toxics, 11(6). https://doi.org/10.3390/toxics11060489\n\n\nLeonard, J. F., Taxel, P., Kuo, C.-L., Godoy, L. da C., & Freilich, M. (2023). Dental implant and bone augmentation treatment in bone-compromised patients: Oral health-related quality of life outcomes. The Journal of Prosthetic Dentistry. https://doi.org/10.1016/j.prosdent.2023.01.011\n\n\nTurshudzhyan, A., Godoy, L. da C., Kuo, C.-L., & Wu, G. Y. (2023). Alpha feto-protein expression trends for screening early hepatocellular carcinoma. Gene Expression, 000, 0‚Äì0.\n\n\nArqub, S. A., Banankhah, S., Sharma, R., Godoy, L. da C., Kuo, C.-L., Ahmed, M., Alfardan, M., & Uribe, F. (2022). Association between initial complexity, frequency of refinements, treatment duration, and outcome in invisalign orthodontic treatment. American Journal of Orthodontics and Dentofacial Orthopedics. https://doi.org/10.1016/j.ajodo.2022.06.017\n\n\nGodoy, L. da C. (2022). Smile: Spatial misalignment: Interpolation, linkage, and estimation. https://CRAN.R-project.org/package=smile\n\n\nGodoy, L. da C., Assun√ß√£o, R. M., & Butler, K. A. (2022). Testing the spatial association of different types of polygons. Spatial Statistics, 51, 100695. https://doi.org/10.1016/j.spasta.2022.100695\n\n\nGodoy, L. da C., Prates, M. O., & Yan, J. (2022). An unified framework for point-level, areal, and mixed spatial data: The hausdorff-gaussian process. arXiv. https://doi.org/10.48550/ARXIV.2208.07900\n\n\nHariharan, A., Arqub, S. A., Gandhi, V., Godoy, L. da C., Kuo, C.-L., & Uribe, F. (2022). Evaluation of interproximal reduction in individual teeth, and full arch assessment in clear aligner therapy: Digital planning versus 3D model analysis after reduction. Progress in Orthodontics, 23(1), 1‚Äì10.\n\n\nKumar, M., Patil, S., Godoy, L. da C., Kuo, C.-L., Swede, H., Kuchel, G. A., & Chen, K. (2022). Demand ischemia as a predictor of mortality in older patients with delirium. Frontiers in Cardiovascular Medicine, 9.\n\n\nPrates, M. O., Azevedo, D. R. M., Godoy, L. da C., & Bandyopadhyay, D. (2022). Can gaussian markov random fields handle spatial confounding? Journal of the Indian Statistical Association.\n\n\nArqub, S. A., Voldman, R., Ahmida, A., Kuo, C.-L., Godoy, L. da C., Nasrawi, Y., Al-Khateeb, S. N., & Uribe, F. (2021). Patients‚Äô perceptions of orthodontic treatment experiences during COVID-19: A cross-sectional study. Progress in Orthodontics, 22(1), 1‚Äì12.\n\n\nBoutrous, M. L., Maseto, N., Kuo, C.-L., Godoy, L. da C., & Amankwah, K. (2021). The use of multiple carotid stents is associated with increased incidence of developing in-stent stenosis on long-term follow-up. Journal of Vascular Surgery, 74(3), e240‚Äìe241.\n\n\nHuynh, C., Godoy, L. da C., Kuo, C.-L., Smeds, M., & Amankwah, K. S. (2021). Examining the development of operative autonomy in vascular surgery training and when trainees and program directors agree and disagree. Annals of Vascular Surgery, 74, 1‚Äì10.\n\n\nLin, G., Murase, J. E., Murrell, D. F., Godoy, L. da C., & Grant-Kels, J. M. (2021). The impact of gender in mentor-mentee success: Results from the women‚Äôs dermatologic society mentorship survey. International Journal of Women‚Äôs Dermatology."
  },
  {
    "objectID": "posts/lmnet/2021-06-23-lmnnet.html",
    "href": "posts/lmnet/2021-06-23-lmnnet.html",
    "title": "Estimating regression coefficients using a Neural Network (from scratch)",
    "section": "",
    "text": "The idea behind the Neural Networks models, as its nomenclature suggests, is to mimic the way human brain learns to execute of some tasks. Some works in the literature (Cheng and Titterington (1994), Stern (1996), Warner and Misra (1996)) attribute of the first attempts to build a ‚ÄúNeural Network emulator‚Äù to McCulloch and Pitts (1943). The popularity of this method in the past decades was held down by the computation intensive calculations needed for such procedures. However, the computation resources advances in the last few years allied to the algorithmic nature of Neural Networks have contributed to the adoption of the methodology by computer scientists. These days, this models are very popular in the industry and are applied to several interesting applications such as speech recognition, image classification, and automatic text translation."
  },
  {
    "objectID": "posts/lmnet/2021-06-23-lmnnet.html#intro",
    "href": "posts/lmnet/2021-06-23-lmnnet.html#intro",
    "title": "Estimating regression coefficients using a Neural Network (from scratch)",
    "section": "",
    "text": "The idea behind the Neural Networks models, as its nomenclature suggests, is to mimic the way human brain learns to execute of some tasks. Some works in the literature (Cheng and Titterington (1994), Stern (1996), Warner and Misra (1996)) attribute of the first attempts to build a ‚ÄúNeural Network emulator‚Äù to McCulloch and Pitts (1943). The popularity of this method in the past decades was held down by the computation intensive calculations needed for such procedures. However, the computation resources advances in the last few years allied to the algorithmic nature of Neural Networks have contributed to the adoption of the methodology by computer scientists. These days, this models are very popular in the industry and are applied to several interesting applications such as speech recognition, image classification, and automatic text translation."
  },
  {
    "objectID": "posts/lmnet/2021-06-23-lmnnet.html#neural-network-regression",
    "href": "posts/lmnet/2021-06-23-lmnnet.html#neural-network-regression",
    "title": "Estimating regression coefficients using a Neural Network (from scratch)",
    "section": "Neural Network Regression",
    "text": "Neural Network Regression\nA neural network is a highly parametrized model that, provided we have enough data, can approximate any functional relationship between a set of features1 ùê±\\mathbf{x} and a response variable yy (Efron and Hastie (2016), pages 151-152). Although there are several possible structures for neural networks, for this post we are going to consider only the feed-forward2 neural networks. In order to explain how these neural networks are designed, let‚Äôs consider its graphical representation (see Figure¬†1). We have vertices, which are called a units (or neurons), ordered horizontally by layers. An edge coming from one vertex can only be connected to vertices associated with ‚Äúhigher‚Äù layers. These connections represent a information flow from left to right (hence, the name feed-forward), where each unit computed by 1) giving weights to each of its inputs, 2) calculating the dot product between weights and inputs, 3) adding a constant( usually referred to as bias) to it, and, finally, 4) applying an element-wise activation function f(‚ãÖ)f(\\cdot) to it. These activation functions are used to establish non-linear relationships between units.\nThe number of hidden layers as well as the number of units associated with every layer can both be regard as tuning parameters. The design and architecture of a neural network is a complex task. In summary, when having a single hidden layer, the number of units associated with the hidden layer determines the number of parameters associated with the model. Efron and Hastie (2016) suggest that, under this scenario, it is better to consider several units for the hidden layer and use some kind of regularization to avoid overfitting. Penalizations analogous to the Ridge and Lasso penalty for linear models are often used in the regularization context for neural networks (Hastie, Tibshirani, and Wainwright (2015), pages 210-211).\nAn important remark regarding the neural network models is that they are ‚Äúpure prediction algorithms‚Äù. That is, these models are focused only on prediction, neglecting the estimation, as pointed by Efron (2020). The strategy is simple and consists in searching for high predictive accuracy. That being said, these algorithms make no assumption on the probability distribution of the data and, as one of the consequences of losing these assumptions, it is not possible to make interval predictions or to calculate confidence intervals for the ‚Äúestimated‚Äù parameters.\n\n\n\n\n\n\n\n\nFigure¬†1: A feed-forward neural network with a single hidden layer.\n\n\n\n\n\n\nSingle neuron feed-forward networks\nA single neuron feed-forward network does not possess any hidden layer in its structure. The absence of hidden layers makes these models resemble the statistical models we are most used to, like, for example, the linear regression and logistic regression. By analyzing the graphical representation of a single layer feed-forward network (Figure¬†2), it is easy to see that by taking the identity as the activation function, the functional relationship between ùê±\\mathbf{x} and yy considered by the neural network is equivalent to the one used for the general linear model. Considering the same representation, if we take f(x)=logit(x)f(x) = \\textrm{logit}(x) (sigmoid function, according to the neural network models literature) and y‚àà{0,1}y \\in \\{ 0, 1 \\}, then the neural network provides the same relationship between ùê±\\mathbf{x} and yy as the one used by the logistic regression.\n\n\n\n\n\n\n\n\nFigure¬†2: A single layer feed-forward neural network.\n\n\n\n\n\nAlthough the functional relationship between ùê±\\mathbf{x} and yy assumed by the single layer neural network coincides with some statistical models, we cannot promptly claim an equivalence between models because the way the neural networks learn, that is estimates, the weights can lead to different solutions depending on the loss and cost functions selected, we are going to talk more about these functions in the next section.\n\n\nActivation functions\nActivation functions are applied in every ‚ÄúLayer connection‚Äù in neural network models. Suppose, for example, we have a design matrix $\\mathbf{X} \\in {\\rm\nI\\!R}^{n \\times p}$, and a response variable ùê≤\\mathbf{y}. Then, given appropriate choices of the K‚àí1K - 1 (one for each layer connection), the mathematical model, for a single observation, behind the neural network, can be written in a vectorial notation as follows ùê≥(k)=ùêñ(k‚àí1)ùêö(k‚àí1)\n\\mathbf{z}^{(k)} = \\mathbf{W}^{(k - 1)} \\mathbf{a}^{(k - 1)}\n ùêö(k)=f(k)(ùê≥(k)),\n\\mathbf{a}^{(k)} = f_{(k)} \\left( \\mathbf{z}^{(k)} \\right),\n where $\\mathbf{W}^{(k - 1)} \\in {\\rm I\\!R}^{m_{k - 1} \\times m_{k}}$ is the matrix of weights that go from from the layer Lk‚àí1L_{k - 1} to the layer LkL_{k}, $\\mathbf{a}^{(k)} \\in {\\rm I\\!R}^{m_k \\times m_{k + 1}}$ matrix of units at layer LkL_k, and f(k)f_{(k)} is a (element-wise) activation function used at the layer LkL_k. Note that, when k=0k = 0, then ùêö(0)=ùêó\\mathbf{a}^{(0)} = \\mathbf{X}. Observe that mkm_k is the number of units in the layer kk and, consequently, for the input and output layers, respectively, we have m0=pm_0 = p and mK=1m_K = 1.\nFrom this example, it is clear that we can apply different activation functions when connecting different layers. Nevertheless, the activation for one layer is the same along all of its units.\nAlthough, theoretically, there exists no restriction on which functions to use as activation function, we want these functions to be at least one time differentiable. This is due to the fact that most of the methods used to find the optimal weights are based on gradients. Another aspect to be considered when choosing an activation function is the domain of the output variable yy. That is, if y‚àà[0,1]y \\in [0, 1], we want an activation function that maps real values to the [0,1][0, 1] interval. In summary, for the output layer, we use a activation function that makes predictions on the same domain as the output variable, while, for hidden layers, we have no restrictions on the activation functions, besides the ones already mentioned.\nSome commonly used link functions are the logit\\textrm{logit}, or sigmoid, function, defined as f(x)=11+e‚àíx,\nf(x) = \\frac{1}{1 + e^{-x}},\n the hyperbolic tangent function, referred to as tanh\\textrm{tanh}, f(x)=ez‚àíe‚àízez+e‚àíz.\nf(x) = \\frac{e^z - e^{-z}}{e^{z} + e^{-z}}.\n Note that the tanh\\textrm{tanh} is mapping from the real line to the (‚àí1,1)(-1, 1) interval. The Rectified Linear Unit (ReLU) is also a popular choice and is defined as f(x)=x+=max(0,x),\nf(x) = x_{+} = \\max(0, x),\n the main advantage of this function is a cheap to compute gradient. A different version of the ReLU called leaky ReLU is also quite popular, its definition is given as follows f(x)=x+=max(.01*x,x),\nf(x) = x_{+} = \\max(.01 * x, x),\n\nThese are only some examples of commonly used activation functions and they are illustrated in Figure¬†3. The user does need to be restrict to these options since there are several other functions implemented in the software available to compute neural networks. However, if you want to use a activation function that is not implemented yet, you may have to implement your own version for the algorithm.\n\n\n\n\n\n\n\n\nFigure¬†3: The most popular activation functions (figure inspired by Figure 18.6 from Efron and Hastie (2016) ).\n\n\n\n\n\nAlthough there are no restrictions on the functions used as activation functions in the hidden layers (besides being differentiable functions), it is not advisable to use the identity function because it implies a waste of computational power. This is due to the fact that using a linear function in a hidden layer, makes the units from that layer a linear combination of the units from the previous layer. To make this clear, let‚Äôs prove that a Neural Network model with a single hidden layer collapses to a Generalized Linear Model when the identity function is used as the activation function.\nSuppose a nn-dimensional vector ùê≤\\mathbf{y} is assumed to follow a distribution ùí´\\mathcal{P}, where ùí´\\mathcal{P} belongs to the exponential family of distributions. Then, given a design matrix $\\mathbf{X} \\in\n{\\rm I\\!R}^{n \\times p}$, the Generalized Linear Model for ùê≤\\mathbf{y} is composed by the random component, given by the probability density function associated with the distribution ùí´\\mathcal{P}, the systematic component, defined by ùõà=ùêóùõÉ,(1)\n\\boldsymbol{\\eta} = \\mathbf{X} \\boldsymbol{\\beta},\n \\qquad(1) and a (canonical) link function g(‚ãÖ)g(\\cdot) such that ùõç=g(ùõà).(2)\n\\boldsymbol{\\mu} = g(\\boldsymbol{\\eta}).\n \\qquad(2) Once we estimate the parameters ùõÉ\\boldsymbol{\\beta}, we have ùê≤ÃÇ=g(ùêóùõÉÃÇ).\n\\hat{\\mathbf{y}} = g( \\mathbf{X} \\hat{\\boldsymbol{\\beta}} ).\n\nDefine now our Neural Network model having a single hidden layer with MM units. The activation function for the hidden layer is fh(x)=g(x)f_h(x) = g(x), that is, the same as the identity function. The weights we want to find are $\\mathbf{W}^{(1)} \\in {\\rm I\\!R}^{p \\times 1}$, and $\\mathbf{W}^{(2)} \\in\n{\\rm I\\!R}^{M \\times n}$. The activation function for the activation layer is the previously mentioned canonical link function. Finally, let the loss be the deviance residual associated with the distribution ùí´\\mathcal{P}, and the cost function be the average of the losses. Then, the mathematical representation of the Neural Network becomes ùê≥(1)=ùêóùêñ(1)=ùêö(1),(3)\n\\mathbf{z}^{(1)} = \\mathbf{X} \\mathbf{W}^{(1)} = \\mathbf{a}^{(1)},\n \\qquad(3) because the activation function for the hidden layer is the identity. Then, we have ùê≥(2)=ùêö(1)ùêñ(2)(4)\n\\mathbf{z}^{(2)} = \\mathbf{a}^{(1)} \\mathbf{W}^{(2)}\n \\qquad(4) ùê≤=ùêö(2)=g(ùê≥(ùüê)).(5)\n\\mathbf{y} = \\mathbf{a}^{(2)} = g( \\mathbf{\\mathbf{z}^{(2)}} ).\n \\qquad(5) However, note that, by combining 3, 4, and 5 we get $$\\begin{align*}\n\\mathbf{y} & = g( \\mathbf{\\mathbf{z}^{(2)}} ) \\\\\n& = g( \\mathbf{a}^{(1)} \\mathbf{W}^{(2)} ) \\\\\n& = g( \\mathbf{X} \\underbrace{\\mathbf{W}^{(1)} \\mathbf{W}^{(2)}}_{{\\rm I\\!R}_{p\n\\times 1}} ),\n\\end{align*}$$ which yields to optimal weights (see Fitting a Neural Network and Backpropagation, for more information on how to fit a neural network model) satisfying $$\n\\underbrace{\\mathbf{W}^{(1)} \\mathbf{W}^{(2)}}_{{\\rm I\\!R}_{p\n\\times 1}} = \\hat{\\boldsymbol{\\beta}},\n$$ where ùõÉÃÇ\\hat{\\boldsymbol{\\beta}} is the Maximum Likelihood Estimator for ùõÉ\\boldsymbol{\\beta} that can be obtained using the Iterative Reweighted Least Squares for the model defined by the probability density function associated with the distribution ùí´\\mathcal{P}, the systematic component 1 and a (canonical) link function 2.\n\n\nCost functions\nWhenever we want to fit a neural network to a dataset we need to specify a Cost function, which is usually based on loss functions. A loss function, in the context of Neural Network models, measures how far our predictions f(ùê±;ùêñ)f(\\mathbf{x}; \\mathbf{W}) are from the true value yy. Examples of commonly used loss functions, for a single observation, are the mean square error loss and the binomial deviance defined, respectively, as L(ùê∞,ùê±;y)=12(f(ùê±;ùê∞)‚àíy)2,(6)\nL(\\mathbf{w}, \\mathbf{x}; y) = \\frac{1}{2} (f(\\mathbf{x}; \\mathbf{w}) - y)^{2},\n \\qquad(6) and L(ùêñ,ùê±;y)=ylog(yf(ùê±,ùê∞))+(1‚àíy)log(1‚àíy1‚àíf(ùê±,ùê∞)).(7)\nL(\\mathbf{W}, \\mathbf{x}; y) = y \\log \\left( \\frac{y}{f(\\mathbf{x}, \\mathbf{w})}\n\\right) + \n(1 - y) \\log \\left( \\frac{1 - y}{1 - f(\\mathbf{x}, \\mathbf{w})} \\right).\n \\qquad(7) The loss function 6 is usually employed when the output (response) variable assumes continuous values, while the 7 is used for binary output variables.\nAfter choosing an appropriate loss function, the cost function is defined as the average of the loss function over all the observation, that is C(ùê≤;ùê±,ùêñ)=1n‚àëi=1nL(ùê∞ùê¢,ùê±ùê¢;ùê≤ùê¢)+ŒªJ(ùêñ),\nC(\\mathbf{y}; \\mathbf{x}, \\mathbf{W}) = \\frac{1}{n} \\sum_{i = 1}^{n}\nL(\\mathbf{w_i, \\mathbf{x}_i; y_i}) + \\lambda J(\\mathbf{W}),\n where J(ùêñ)J(\\mathbf{W}) is a non-negative regularization term and Œª‚â•0\\lambda \\geq\n0 is a tuning parameter.\nIn practice, we may have a regularization term for each layer, each one having its own Œª\\lambda. Some commonly used regularization terms are J(ùêñ)=12‚àëk=1K‚àí1‚à•ùê∞(k)‚à•2,\nJ(\\mathbf{W}) = \\frac{1}{2} \\sum_{k = 1}^{K - 1} \\lVert \\mathbf{w}^{(k)} \\rVert^2,\n and J(ùêñ)=12‚àëk=1K‚àí1‚à•ùê∞(k)‚à•,\nJ(\\mathbf{W}) = \\frac{1}{2} \\sum_{k = 1}^{K - 1} \\lVert \\mathbf{w}^{(k)} \\rVert,\n where KK is the number of layers of our neural network model, and ùê∞(k)\\mathbf{w}^{(k)} is the vector of weights from the units in the layer LkL_k to the layer Lk+1L_{k + 1}. Note that, these two regularization terms are analogous to the Ridge and Lasso penalizations, and they play the exact same role in neural networks as its analogous versions do for the linear models (Efron and Hastie 2016). Mixtures of these two regularization terms, as in the elastic net (Zou and Hastie 2005), are also common.\n\n\nFitting a Neural Network\nSupposing a user has set the number of layers, units, an activation function and a loss function, to fit a neural network we seek the set of weights ùêñ={ùêñ(1),‚Ä¶,ùêñ(k‚àí1)}\\mathbf{W}\n= \\{ \\mathbf{W}^{(1)}, \\ldots, \\mathbf{W}^{(k - 1)} \\} such that the cost function is minimized, that is minùêñ{ùíû(ùê≤;ùêó,ùêñ)}. \n\\min_{\\mathbf{W}} \\left \\{ \\mathcal{C}(\\mathbf{y}; \\mathbf{X}, \\mathbf{W}) \\right \\}.\n Therefore, the neural network fit has turned into an optimization problem. The most common algorithm used to solve this optimization problem is the Backpropagation algorithm, which is described in the next section for a general situation.\n\n\nBackpropagation\nBackpropagation (or gradient descent) is the method used to find the weights which minimize the chosen cost and loss functions for a given neural network. It is an iterative algorithm that is guaranteed to converge whenever the cost function has a single local minima (Efron and Hastie 2016). However, even if the cost function does not have a single local minima, the algorithm works fairly well. The updates for a weight matrix, ùêñ(k)\\mathbf{W}^{(k)} let‚Äôs say, is done as follows ùêñ(k)=ùêñ(k)‚àíŒ±‚àÇùíû(ùê≤;ùêó,ùêñ)‚àÇùêñ(k),(8)\n\\mathbf{W}^{(k)} = \\mathbf{W}^{(k)} - \\alpha \\frac{\\partial\n\\mathcal{C}(\\mathbf{y}; \\mathbf{X}, \\mathbf{W})}{\\partial \\mathbf{W}^{(k)}},\n \\qquad(8) where Œ±\\alpha is a tuning parameter called learning rate. The name backpropagation comes from the fact that the derivatives (or gradients) are computed according to something called a computation graph in a backward fashion. It is heavily based on the chain rule for differentiation.\nGiven initial values for the ùêñ\\mathbf{W} matrices, the method repeats the update rule 8 until convergence. Provided that the columns of the design matrix are rescaled to mean 0 and variance 1, Hastie, Tibshirani, and Friedman (2009) suggest the use of random starting values for the weights as uniform random variables on the interval [‚àí.75,.75][-.75, .75]."
  },
  {
    "objectID": "posts/lmnet/2021-06-23-lmnnet.html#sec:imple",
    "href": "posts/lmnet/2021-06-23-lmnnet.html#sec:imple",
    "title": "Estimating regression coefficients using a Neural Network (from scratch)",
    "section": "Implementation",
    "text": "Implementation\nI created functions for the implementation of a Neural Network with a single hidden layer model for generic activation functions. The implementation considers the cost function defined as C(ùê≤;ùêó,ùêò)=1n‚à•ùê≤‚àíùê≤ÃÇ‚à•2.\nC(\\mathbf{y}; \\mathbf{X}, \\mathbf{Y}) = \\frac{1}{n} \\lVert \\mathbf{y} -\n\\hat{\\mathbf{y}} \\rVert^2.\n\nThe inputs for the implemented function are:\n\nA design matrix ùêó\\mathbf{X}, including the columns of ones for the intercept;\nA column vector ùê≤\\mathbf{y} containing the response variable;\nThe number of units for the hidden layer;\nThe activation function for the hidden layer;\nThe activation function for the output layer;\nA scalar for the learning rate Œ±\\alpha;\nTwo control parameters for the convergence of the algorithm. The maximum number of iterations allowed, and a relative error œµ\\epsilon which controls when to stop the iteration algorithm.\n\nThe function returns a list of size 5. Its first element is the predicted vector for ùê≤\\mathbf{y}, the second contains the values of the cost function for each iteration of the algorithm. The third position of this list stores the weight matrices ùêñ(1)\\mathbf{W}^{(1)} and ùêñ(2)\\mathbf{W}^{(2)}, while the last two positions store the number of iterations until attain the convergence and a string indicating whether the algorithm converged or not, respectively.\nSee below the implementation of some activation functions (and their derivatives)\n\n##--- activation functions and their derivatives ----\n\n## ReLU\nrelu &lt;- function(x) {\n    pmax(x, 0)\n}\n\n## derivative leaky ReLU\nd_relu &lt;- function(x) {\n    ifelse(x &gt; 0, 1, 0)\n}\n\n## leaky ReLU\nlrelu &lt;- function(x) {\n    pmax(x * .01, x)\n}\n\n## derivative leaky ReLU\nd_lrelu &lt;- function(x) {\n    ifelse(x &gt; 0, 1, .01)\n}\n\n## derivative tanh\nd_tanh &lt;- function(x) {\n    1 - (tanh(x)^2)\n}\n\n## derivative logit\nd_logit &lt;- function(x) {\n    plogis(x) * ( 1 - plogis(x) )\n}\n\n## derivative identity\nd_ident &lt;- function(x) {\n    pmax( -2 * abs(x), 1 )\n}\n\nNow, let‚Äôs implement some helper functions to fit our neural network models. First, the cost function used in our examples is given by\n\n## cost function\ncost_f &lt;- function(y, yhat) {\n    crossprod(yhat - y) / NROW(y)\n}\n\nThe implementation of the functions that will need to be executed at each step of the optimization algorithm are defined below. compute_nn computes the hidden layers given the matrix of covariates (or features) X, the list containing the the weights W associated to each layer connection, and two activation functions act_hidden and act_out for the hidden and output layers, respectively (this is a the implementation for a 2 layers network). The compute_grad function computes the gradient and needs some further information like y (the response variable), n the sample size, and the derivatives of the activation functions. update_aux and update_w are helper functions used to update the weights.\n\n##--- functiosn to fit the neural network ----\n\n## computing the forward step of the neural network\ncompute_nn &lt;- function(X, W, act_hidden, act_out) {\n    Z &lt;- vector(mode = \"list\", length = 2)\n    \n    Z[[1]] &lt;- X %*% W[[1]]\n    A &lt;- act_hidden(Z[[1]])\n\n    Z[[2]] &lt;- A %*% W[[2]]\n\n    return( list(y = act_out(Z[[2]]),\n                 z = Z) )\n}\n\n## computing the gradient of the neural network\ncompute_grad &lt;- function(y, X, W, act_hidden, act_out,\n                         d1_hidden, d1_out, n) {\n    nn    &lt;- compute_nn(X, W, act_hidden, act_out)\n    aux_out &lt;- (nn$y - y) * d1_out(nn$z[[2]])\n    aux_hid &lt;- tcrossprod(aux_out, W[[2]]) *\n        d1_hidden(nn$z[[1]])\n    \n    return(\n        list(crossprod(X, aux_hid) / n,\n             crossprod(act_hidden(nn$z[[1]]), aux_out) / n)\n    )\n}\n\n## aux function for updating W\nupdate_aux &lt;- function(w, dw, alpha) {\n    w - alpha * dw\n}\n\n## update the weights of a neural network\nupdate_w &lt;- function(W, alpha, y, X, act_hidden, act_out,\n                     d1_hidden, d1_out, n) {\n\n    grad_w &lt;- compute_grad(y, X, W, act_hidden, act_out,\n                           d1_hidden, d1_out, n)\n    \n    return( Map(f = update_aux, w = W,\n                dw = grad_w, alpha = alpha) )\n}\n\nFinally, all these functions previously describer are used to build the fit_nn function (which is used to compute the optimal weights for the neural network). The alpha is the Œ±\\alpha previously mentioned in this post, maxit and eps are parameters used in the optimization process. The first one stands for the maximum number of iterations to be used in the optimization process, while the second stand for the ‚Äúoptimization error‚Äù. That is, if, from one iteration to another, the change between the weights does not exceed eps, then we consider that the algorithm converged and a (maybe local) optimum has been found.\n\nfit_nn &lt;- function(y, X, hid_units,\n                   act_hidden, act_out,\n                   d1_hidden, d1_out,\n                   alpha = .25,\n                   maxit = 500L,\n                   eps   = 1e-05) {\n    m &lt;- hid_units\n    p &lt;- ncol(X)\n    N &lt;- NROW(y)\n\n    \n    W &lt;- list(matrix(runif(m * p, -.75, .75),\n                     ncol = m, nrow = p),\n              matrix(runif(m, -.75, .75), ncol = 1))\n\n    nn   &lt;- vector(mode = \"list\", length = maxit)\n    cost &lt;- vector(mode = \"numeric\", length = maxit)\n\n    ## initialiazing\n    nn[[1]] &lt;- compute_nn(X, W, act_hidden, act_out)\n\n    cost[1] &lt;- cost_f(y, nn[[1]]$y)\n    \n    for( i in seq_len(maxit)[-1] ) {\n        W &lt;- update_w(W, alpha, y, X,\n                      act_hidden, act_out,\n                      d1_hidden, d1_out,\n                      n = N)\n        \n        nn[[i]] &lt;- compute_nn(X, W, act_hidden, act_out)\n        cost[i] &lt;- cost_f(y, nn[[i]]$y)\n        \n        if( abs(cost[i] - cost[i - 1]) &lt; eps ) {\n            output &lt;- list(nn   = nn[[i - 1]],\n                           cost = cost[1:(i - 1)],\n                           W    = W,\n                           it   = (i - 1),\n                           conv = \"yes\")\n            break\n        }\n    }\n\n    if( i == maxit ) {\n        output &lt;- list(yhat = nn[[maxit]]$y,\n                       cost = cost[1:maxit],\n                       W    = W,\n                       it   = maxit,\n                       conv = \"no\")\n    }\n\n    return(output)\n}\n\nHaving all these functions, we can play with some numerical examples!"
  },
  {
    "objectID": "posts/lmnet/2021-06-23-lmnnet.html#sec:ne",
    "href": "posts/lmnet/2021-06-23-lmnnet.html#sec:ne",
    "title": "Estimating regression coefficients using a Neural Network (from scratch)",
    "section": "Numerical Examples",
    "text": "Numerical Examples\n\nExample 1: Equivalence between Neural Network and Linear Model\nConsider a simulated dataset where ùê≤‚àºN(ùêóùõÉ,œÉ2ùêàn),\n\\mathbf{y} \\sim N(\\mathbf{X} \\boldsymbol{\\beta}, \\sigma^2 \\mathbf{I}_n),\n where $\\mathbf{X} \\in {\\rm I\\!R}^{n \\times 3}$, with the first column being the intercept term. To simulate the model we used ùõÉ=(2,3,1.5)\\boldsymbol{\\beta} = (2, 3,\n1.5). Additionally, suppose n=2000n = 2000.\nConsidering the identity function as the activation function for both layers, the goal here is to show that the ùêñ(1)ùêñ(2)=ùõÉÃÇ\\mathbf{W}^{(1)} \\mathbf{W}^{(2)} =\n\\hat{\\boldsymbol{\\beta}}, where ùõÉÃÇ\\hat{\\boldsymbol{\\beta}} is the least squares solution for a linear model established as ùê≤=ùêóùõÉ\\mathbf{y} = \\mathbf{X}\n\\boldsymbol{\\beta}, and ùêñ(1),ùêñ(2)\\mathbf{W}^{(1)}, \\, \\mathbf{W}^{(2)} are the optimal weights according to the Neural Network fitted to the data, as proved in the subsection @ref(subsec:act).\nTable¬†1 displays the results from the simulated example. The two different approaches have yielded the exactly same results. If we were to make predictions, the two methods would provide the same predicted values under these circumstances.\n\n\n\n\nTable¬†1: Comparing the LS solution and the product of the neural network weight matrices.\n\n\n\n\n\n\nùõÉÃÇ\\hat{\\boldsymbol{\\beta}}\nùêñ(1)ùêñ(2)\\mathbf{W}^{(1)} \\mathbf{W}^{(2)}\n\n\n\n\n2.996\n2.996\n\n\n3.004\n3.004\n\n\n1.512\n1.512\n\n\n\n\n\n\n\n\nSee below the code used on this example.\n\n##--- numerical examples ----\n\n##--- example 1 ----\n\nset.seed(123)\n\nn &lt;- 2000\n\nx1 &lt;- rnorm(n)\nx2 &lt;- as.numeric( scale( rexp(n) ) )\n\ny &lt;- 3 + 3 * x1 + 1.5 * x2 + rnorm(n, sd = .5)\n\nmy_x &lt;- cbind( rep(1, n), x1, x2 )\ncolnames(my_x) &lt;- NULL\n\ndt &lt;- as.data.frame( cbind(y, my_x[, 2:3]) )\nnames(dt) &lt;- c(\"y\", \"x1\", \"x2\")\n\nm &lt;- 6\n\nfit_1 &lt;-\n    fit_nn(y = y, X = my_x,\n           hid_units = m,\n           act_hidden = identity,\n           act_out    = identity,\n           d1_hidden  = d_ident,\n           d1_out     = d_ident,\n           alpha = .05,\n           maxit = 1000L,\n           eps   = 1e-16)\n\nbeta_hat &lt;- coef(lm(y ~ x1 + x2, data = dt))\n\ntbl_1 &lt;- as.data.frame(cbind(beta_hat,\n                             fit_1$W[[1]] %*% fit_1$W[[2]]))\nnames(tbl_1) &lt;- c(\"$\\\\hat{\\\\boldsymbol{\\\\beta}}$\",\n                  \"$\\\\mathbf{W}^{(1)} \\\\mathbf{W}^{(2)}$\")\nrownames(tbl_1) &lt;- NULL\n\n\n\nExample 2: Nonlinear relationship and number of hidden units\nConsider now the following model yi=Œ≤0+Œ≤1(x2)+Œµi.\ny_i = \\beta_0 + \\beta_1 (x^2) + \\varepsilon_i.\n\nIn practice, we do not know before-hand the relationship between the response and explanatory variables is not linear. In fig-fit-nn2, we show the fitted curves the linear model and for neural networks under different settings for a dataset simulated from this example. The Neural Network deals nicely with the nonlinearity at the cost of possibly overfit the data.\n\n\n\n\n\n\n\n\nFigure¬†4: Different models fitted to the same simulated dataset.\n\n\n\n\n\nSee the code used on this example below.\n\n##--- example 2 ----\n\nset.seed(124)\n\nx12 &lt;- rnorm(n)\n\ny2 &lt;- 5 - 2.5 * (x12^2) + rnorm(n, sd = .5)\n\nmy_x2 &lt;- cbind(rep(1, n), x12)\ncolnames(my_x2) &lt;- NULL\n\ndt2 &lt;- as.data.frame( cbind(y2, my_x2[, 2]) )\nnames(dt2) &lt;- c(\"y\", \"x1\")\n\nn_pred &lt;- 4000\n\n## fitting linear model\n\nmy_lm2 &lt;- lm(y ~ x1, data = dt2)\n\n## fitting neural network with tanh as the activation function for the hidden\n## layer - 5 hidden units\nfit_2 &lt;-\n    fit_nn(y = y2, X = my_x2,\n           hid_units  = 5,\n           act_hidden = tanh,\n           act_out    = identity,\n           d1_hidden  = d_tanh,\n           d1_out     = d_ident,\n           alpha = .05,\n           maxit = 1000L,\n           eps   = 1e-04)\n\n## fitting neural network with tanh as the activation function for the hidden\n## layer - 15 hidden units\nfit_3 &lt;-\n    fit_nn(y = y2, X = my_x2,\n           hid_units  = 15,\n           act_hidden = tanh,\n           act_out    = identity,\n           d1_hidden  = d_tanh,\n           d1_out     = d_ident,\n           alpha = .05,\n           maxit = 1000L,\n           eps   = 1e-04)\n\n## fitting neural network with leaky ReLU as the activation function for the\n## hidden layer - 10 hidden units\nfit_4 &lt;-\n    fit_nn(y = y2, X = my_x2,\n           hid_units  = 10,\n           act_hidden = lrelu,\n           act_out    = identity,\n           d1_hidden  = d_lrelu,\n           d1_out     = d_ident,\n           alpha = .05,\n           maxit = 1000L,\n           eps   = 1e-04)\n\npred_data &lt;- data.frame(x = seq(from = min(x12), to = max(x12),\n                                length.out = n_pred))\n\npred_data &lt;- transform(pred_data,\n                       lm = coef(my_lm2)[[1]] + coef(my_lm2)[[1]] * x)\n\npred_data &lt;- transform(pred_data,\n                       nn_tanh_1 =\n                           compute_nn(X = cbind(rep(1, 4000),\n                                                x),\n                                      W = fit_2$W,\n                                      act_hidden = tanh,\n                                      act_out = identity)$y)\n\npred_data &lt;- transform(pred_data,\n                       nn_tanh_2 =\n                           compute_nn(X = cbind(rep(1, 4000),\n                                                x),\n                                      W = fit_3$W,\n                                      act_hidden = tanh,\n                                      act_out = identity)$y)\n\npred_data &lt;- transform(pred_data,\n                       nn_lrelu =\n                           compute_nn(X = cbind(rep(1, 4000),\n                                                x),\n                                      W = fit_4$W,\n                                      act_hidden = lrelu,\n                                      act_out = identity)$y)\n\nsetDT(pred_data)\n\npred_data &lt;- melt(pred_data, id = 1,\n                  value.name = \"pred\",\n                  variable.name = \"method\")\n\npred_data[, method := fcase(method == \"nn_tanh_1\", \"tanh - 5\",\n                            method == \"nn_tanh_2\", \"tanh - 15\",\n                            method == \"nn_lrelu\", \"leaky ReLU - 10\",\n                            default = \"lm\")]\n\nggplot(data = pred_data) +\n    geom_point(data = dt2, aes(x = x1, y = y),\n               alpha = .5) +\n    geom_line(aes(x = x, y = pred, color = method),\n              lwd = 1.05) +\n    scale_color_discrete(name = NULL) +\n    theme_bw() +\n    theme(\n        legend.position = \"bottom\",\n        legend.margin = margin(6, 6, 6, 6)\n    ) +\n    labs(x = \"X\", y = \"Y\")"
  },
  {
    "objectID": "posts/lmnet/2021-06-23-lmnnet.html#final-thoughts",
    "href": "posts/lmnet/2021-06-23-lmnnet.html#final-thoughts",
    "title": "Estimating regression coefficients using a Neural Network (from scratch)",
    "section": "Final Thoughts",
    "text": "Final Thoughts\nThe Neural Network Regression models are very interesting but certainly are not magical as it is sold in the market. By the end of the day, these models consist of simple linear algebra allied to the use of element-wise nonlinear functions and optimization algorithms. Speaking on optimization algorithm, the gradient descent looks like a fixed-point iteration algorithm. These kind of algorithms have the advantage of not need the second derivative of the functions, however their convergence can be slow. I believe that using different learning rates for different parameters could improve the speed on which the algorithm converges.\nAlthough these models do not make any distributional assumption on the data, we can easily make it more suitable for certain distributions by working with the cost and activation functions on an appropriate fashion.\nThere are several variants of these models suited for different problems, like text and image classification, for example. The idea is the same, what changes is the way the researchers deal with the hidden layers. I think an interesting application is to try to use neural networks to estimate non-parametrically covariance matrices for spatial data."
  },
  {
    "objectID": "posts/lmnet/2021-06-23-lmnnet.html#footnotes",
    "href": "posts/lmnet/2021-06-23-lmnnet.html#footnotes",
    "title": "Estimating regression coefficients using a Neural Network (from scratch)",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nFeatures are the name given for predictors in the neural networks literature‚Ü©Ô∏é\nSometimes referred to as multi-layer-perceptron, and back-propagation.‚Ü©Ô∏é"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Lucas Godoy",
    "section": "",
    "text": "Hello there, I‚Äôm Lucas, a Ph.D.¬†candidate studying Statistics at the UConn Department. My research interests primarily revolve around statistical computing, Bayesian inference, spatial statistics, and exploring innovative applications of statistical methodologies. Apart from that, I enjoy learning about programming languages and emerging technologies.\nIf you browse through this website, you‚Äôll find some of my blog posts, mainly discussing R or statistical concepts, as well as my CV, publications, and ongoing projects."
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n\n\n\n\n\nUsing jupyter on an HPC cluster\n\n\n\n\n\n\n\n\nJun 9, 2022\n\n\nLucas Godoy\n\n\n4 min\n\n\n\n\n\n\n\nAll models are wrong\n\n\n\n\n\n\n\n\nAug 15, 2021\n\n\nLucas Godoy\n\n\n2 min\n\n\n\n\n\n\n\nEstimating regression coefficients using a Neural Network (from scratch)\n\n\n\n\n\n\n\n\nJul 28, 2021\n\n\nLucas Godoy\n\n\n48 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "defense/defense.html#spatial-data",
    "href": "defense/defense.html#spatial-data",
    "title": "HAUSDORFF‚ÄìGAUSSIAN PROCESS WITH SPATIAL AND SPATIOTEMPORAL APPLICATIONS",
    "section": "Spatial Data",
    "text": "Spatial Data\n\nTaking into account spatial dependence possibly present in data is a foremost aspect of spatial statistics.\nStatistical inference depends heavily on the spatial structure/geometry of the observed spatial data.\n\n\n\n\nGeometry\nBranch\n\n\n\n\nPoint-referrenced\nGeostatistics\n\n\n\nPoint patterns\n\n\nAreal units/polygons\nLattice/Areal models\n\n\nMixed\nSpatial Data Fusion"
  },
  {
    "objectID": "defense/defense.html#areal-data",
    "href": "defense/defense.html#areal-data",
    "title": "HAUSDORFF‚ÄìGAUSSIAN PROCESS WITH SPATIAL AND SPATIOTEMPORAL APPLICATIONS",
    "section": "Areal Data",
    "text": "Areal Data\n\n\nFigure¬†1: Example of areal data."
  },
  {
    "objectID": "defense/defense.html#point-refereced-data",
    "href": "defense/defense.html#point-refereced-data",
    "title": "HAUSDORFF‚ÄìGAUSSIAN PROCESS WITH SPATIAL AND SPATIOTEMPORAL APPLICATIONS",
    "section": "Point-refereced Data",
    "text": "Point-refereced Data\n\n\nFigure¬†2: Example of point-referrenced data."
  },
  {
    "objectID": "defense/defense.html#fused-data",
    "href": "defense/defense.html#fused-data",
    "title": "HAUSDORFF‚ÄìGAUSSIAN PROCESS WITH SPATIAL AND SPATIOTEMPORAL APPLICATIONS",
    "section": "Fused Data",
    "text": "Fused Data\n\n\nFigure¬†3: Example of fused data."
  },
  {
    "objectID": "defense/defense.html#general-spatial-model",
    "href": "defense/defense.html#general-spatial-model",
    "title": "HAUSDORFF‚ÄìGAUSSIAN PROCESS WITH SPATIAL AND SPATIOTEMPORAL APPLICATIONS",
    "section": "General Spatial Model",
    "text": "General Spatial Model\nThe model below is flexible enough to analyze most spatial data structures (Cressie 1993)\n\\[\\{ Z(\\mathbf{s}) \\; : \\; \\mathbf{s} \\in D \\},\\]\nwhere \\(D\\) is an index set.\n\nAreal models: index set is countable.\nGeostatistics: index set is a continuum.\n\n\n\nChallenges arise when working with data from different sources:\n\nChange of support\nSpatial misalignment\nData fusion itself."
  },
  {
    "objectID": "defense/defense.html#gaussian-process",
    "href": "defense/defense.html#gaussian-process",
    "title": "HAUSDORFF‚ÄìGAUSSIAN PROCESS WITH SPATIAL AND SPATIOTEMPORAL APPLICATIONS",
    "section": "Gaussian Process",
    "text": "Gaussian Process\n\nGaussian Process (GP): a stochastic process such that all its finite-dimensional marginal distributions are multivariate normal distributions.\nGP components: A mean and a covariance functions defined as, respectively, \\(\\mathbb{E}[Z(\\mathbf{s})] =\nm(\\mathbf{s})\\) and \\({\\rm Cov}(Z(\\mathbf{s}_1), Z(\\mathbf{s}_2)) = C(\\mathbf{s}_1,\n\\mathbf{s}_2)\\).\nCommon assumptions are stationarity and isotropy.\nImportance in spatial statistics: predominant foundation of geostatistical modeling.\n\n\nLoosely speaking, this means the GP has stationary mean, variance, and a correlation function such that the correlation between two data points depends solely on their distance.\nThe main reasons these models are not applied to areal data are:\n\nCalculating distance between areal spatial units is non-trivial;\nThe commonly used areal models are computationally efficient."
  },
  {
    "objectID": "defense/defense.html#areal-data-and-the-car-model",
    "href": "defense/defense.html#areal-data-and-the-car-model",
    "title": "HAUSDORFF‚ÄìGAUSSIAN PROCESS WITH SPATIAL AND SPATIOTEMPORAL APPLICATIONS",
    "section": "Areal Data and the CAR Model",
    "text": "Areal Data and the CAR Model\n\n\\(Z_k \\mid Z_{-k} \\sim \\mathcal{N}\\left (\\frac{\\psi} {m_k} \\sum_{i \\sim k} Z_i,\n\\frac{\\tau^2}{m_k} \\right)\\)\nSpecial case: ICAR.\nExtensions of the ICAR model, providing a measure of spatial dependence have been developed (Dean et al. 2001; Leroux et al. 2000).\nThe directed acyclic graph auto-regressive (DAGAR) model (Datta et al. 2019) proposes a new way to construct precision matrices using a DAG derived from the original adjacency matrix.\n\n\n\nCAR-like models are usually parametrized by the conditional precision.\n\\(m_k\\) = # of neighbors\n\\(\\psi\\) spatial dependence parameter\n\\(i \\sim k\\) neighbors"
  },
  {
    "objectID": "defense/defense.html#areal-models-limitations",
    "href": "defense/defense.html#areal-models-limitations",
    "title": "HAUSDORFF‚ÄìGAUSSIAN PROCESS WITH SPATIAL AND SPATIOTEMPORAL APPLICATIONS",
    "section": "Areal Models: Limitations",
    "text": "Areal Models: Limitations\n\nOut-of-sample predictions: Non-trivial, it may be necessary to refit the model.\nInterpretability: Relationship between the spatial dependence parameter and correlation between neighbors is counterintuitive.\nHeteroscedasticity: Marginal SD depend on the number of neighbors.\nExtensions for change of support and data fusion problems are cumbersome and there exists no consensus in the literature in how to approach such problems.\n\n\n\nspatial dependence parameter can be negative and the correlation with neighbors be positive.\nspatial dependence parameter very high and moderate correlation with neighbors"
  },
  {
    "objectID": "defense/defense.html#fused-data-1",
    "href": "defense/defense.html#fused-data-1",
    "title": "HAUSDORFF‚ÄìGAUSSIAN PROCESS WITH SPATIAL AND SPATIOTEMPORAL APPLICATIONS",
    "section": "Fused Data",
    "text": "Fused Data\n\nGP at the point-referrenced level and a stochastic integral at the areal level (AGP) (Moraga et al. 2017): \\[Z(\\mathbf{s}_i) = \\begin{cases} {\\mathcal{A}(\\mathbf{s}_i)}^{-1}\n\\int_{\\mathbf{s}_i} Z(\\mathbf{s}) \\mathrm{d}\\mathbf{s}, & \\text{if }\n\\mathcal{A}(\\mathbf{s}_i) &gt; 0, \\\\ Z(\\mathbf{s}_i), & \\; \\text{otherwise,}\n\\end{cases}\\]\nCovariance (Johnson et al. 2020): \\[\\mathrm{Cov}(Z(\\mathbf{s}_i), Z(\\mathbf{s}_j)) = {(\\mathcal{A}(\\mathbf{s}_i)\n\\mathcal{A}(\\mathbf{s}_j))}^{-1} \\int_{\\mathbf{s}_j} \\int_{\\mathbf{s}_i}\nC(\\mathbf{x}, \\mathbf{y}) \\mathrm{d} \\mathbf{x} \\mathrm{d} \\mathbf{y}\\]\nArbitrary precision: The stochastic integrals are evaluated numerically and depend on a user-defined grid."
  },
  {
    "objectID": "defense/defense.html#data-fusion-models-limitations",
    "href": "defense/defense.html#data-fusion-models-limitations",
    "title": "HAUSDORFF‚ÄìGAUSSIAN PROCESS WITH SPATIAL AND SPATIOTEMPORAL APPLICATIONS",
    "section": "Data Fusion Models: Limitations",
    "text": "Data Fusion Models: Limitations\n\nComputing: Although satisfactory approximations exist, it becomes computationally prohibitive for moderately large datasets.\nApproximations: Inferences may drastically change depending on the precision of the approximations.\nBiases: It is hard to quantify the impact of the discretizations on inferences (Gon√ßalves and Gamerman 2018)."
  },
  {
    "objectID": "defense/defense.html#research-questions",
    "href": "defense/defense.html#research-questions",
    "title": "HAUSDORFF‚ÄìGAUSSIAN PROCESS WITH SPATIAL AND SPATIOTEMPORAL APPLICATIONS",
    "section": "Research Questions",
    "text": "Research Questions\n\nThe main research questions we are interested in are:\n\nCan we propose a model for spatial data that accomodates areal, point-referrenced, and fused data?\nIf so, is this model competitive when compared to specialized models?\n\nProposal: A GP defined on a flexible index set.\nMain challenge: Defining a valid correlation function."
  },
  {
    "objectID": "defense/defense.html#distances-between-sets",
    "href": "defense/defense.html#distances-between-sets",
    "title": "HAUSDORFF‚ÄìGAUSSIAN PROCESS WITH SPATIAL AND SPATIOTEMPORAL APPLICATIONS",
    "section": "Distances between Sets",
    "text": "Distances between Sets\n\nMetric space: \\((D, d)\\), where \\(D\\) is a spatial region of interest.\nDistance between a point and a set: \\(d(x, A) = \\inf_{a \\in A}\nd(x, a)\\), where \\(d(x, y)\\) is the distance between any two elements \\(x, y \\in\nD\\).\nDirected Hausdorff distance: \\[{\\vec h}(A, B) = \\sup_{a \\in A}\nd(a, B)\\]\nHausdorff distance: \\[h(A, B) = \\max \\left \\{ \\vec{h}(A, B),\n\\vec{h}(B, A) \\right \\}\\]\n\n\n\nSymmetric Hausdorff distance: the greater of the two directed Hausdorff distances.\nNote that if \\(A\\) and \\(B\\) are both singletons, then \\(h(A, B) = d(A, B)\\).\n\nMeric Properties: (1) Symmetry: \\(d(x, y) = d(y, x)\\); (2) Nonnegativeness: \\(d(x,\ny) \\geq 0\\) and \\(d(x, x) = 0\\); (3)Positiveness: \\(d(x, y) = 0 \\implies x = y\\); (4) Triangle inequality: \\(d(x, y) \\leq d(x, z) + d(z, y)\\)"
  },
  {
    "objectID": "defense/defense.html#hausdorff-distance-for-spatial-data-analysis",
    "href": "defense/defense.html#hausdorff-distance-for-spatial-data-analysis",
    "title": "HAUSDORFF‚ÄìGAUSSIAN PROCESS WITH SPATIAL AND SPATIOTEMPORAL APPLICATIONS",
    "section": "Hausdorff Distance for Spatial Data Analysis",
    "text": "Hausdorff Distance for Spatial Data Analysis\n\nStudy region: In spatial statistics, \\(D\\) is tipically a closed and bounded subset of \\(\\mathbb{R}^2\\).\nIn this context, the Hausdorff distance is a metric (Sendov 2004).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMetric on \\(D \\setminus \\varnothing\\)\nThe Hausdorff distance ability to account for spatial units‚Äô shapes, sizes and orientation(Min et al. 2007) renders it an interesting tool to achieve our goals.\nMoreover, it can distinguish between overlapping, nested, and disjointed regions.\nIn the figure: (1) The dashed lines denote the Hausdorff distances; (2) The infimum distance between to sets is zero for the three cases; (3) Distance between centroids is the same for the first two figures."
  },
  {
    "objectID": "defense/defense.html#beyond-traditional-modeling-the-hgp",
    "href": "defense/defense.html#beyond-traditional-modeling-the-hgp",
    "title": "HAUSDORFF‚ÄìGAUSSIAN PROCESS WITH SPATIAL AND SPATIOTEMPORAL APPLICATIONS",
    "section": "Beyond Traditional Modeling: The HGP",
    "text": "Beyond Traditional Modeling: The HGP\n\nIsotropic stochastic process: \\(\\{ Z(\\mathbf{s}) \\, : \\,\n\\mathbf{s} \\in \\mathcal{B}(D) \\},\\) where \\(D \\subset \\mathbb{R}^2\\).\nFlexible index set: \\(\\mathcal{B}(D)\\) contains all closed and bounded subsets of \\(D\\).\nThe HGP: The stochastic process \\(Z(\\cdot)\\) is an HGP if:\n\nIt is a GP\nIts correlation function is a function of the Hausdorff distance between sets (in our case, spatial units).\n\nThis process is suitable to analyze point-referrenced, areal, and mixed spatial data.\n\n\n\nIdeally, we want bounded, compact, and non-empty sets for the Hausdorff distance to be a metric..\nIn \\(\\mathbb{R}^2\\), a compact subset is bounded.\nempty is a subset of any set -&gt; it is bounded bc a subset of a bounded set is bounded itself."
  },
  {
    "objectID": "defense/defense.html#structure-of-an-hgp",
    "href": "defense/defense.html#structure-of-an-hgp",
    "title": "HAUSDORFF‚ÄìGAUSSIAN PROCESS WITH SPATIAL AND SPATIOTEMPORAL APPLICATIONS",
    "section": "Structure of an HGP",
    "text": "Structure of an HGP\nThe HGP, denoted \\(\\mathrm{HGP}\\{m(\\mathbf{s}), v(\\mathbf{s}), r(h)\\}\\), formulation depends on three functions:\n\nThe mean and SD functions, defined as \\[m(\\mathbf{s}) = \\mathbb{E}[Z(\\mathbf{s})], \\quad \\text{and} \\quad v(\\mathbf{s}) =\n\\sqrt{\\mathrm{Var}(Z(\\mathbf{s}))},\\] respectively.\nCorrelation function: \\(r(h) =\n\\mathrm{Cor}(Z(\\mathbf{s}_1), Z(\\mathbf{s}_2)),\\) where \\(h\\) denotes the Hausdorff distance between \\(\\mathbf{s}_1, \\mathbf{s}_2 \\in\n\\mathcal{B}(D)\\).\nInduced covariance function: \\(\\mathrm{Cov}(Z(\\mathbf{s}_1),\nZ(\\mathbf{s}_2)) = v(\\mathbf{s}_1) v(\\mathbf{s}_2) r(h(\\mathbf{s}_1,\n\\mathbf{s}_2))\\)\n\n\n\nMean and SD functions can be informed by covariates.\nThe ensure the validity of the process, the function \\(r(\\cdot)\\) must be positive definite.\nComplex functional parametric space.\nInference is simplified (or possible) by assuming parametric forms to those functions."
  },
  {
    "objectID": "defense/defense.html#the-vmathbfs-function",
    "href": "defense/defense.html#the-vmathbfs-function",
    "title": "HAUSDORFF‚ÄìGAUSSIAN PROCESS WITH SPATIAL AND SPATIOTEMPORAL APPLICATIONS",
    "section": "The \\(v(\\mathbf{s})\\) function",
    "text": "The \\(v(\\mathbf{s})\\) function\nA useful way to define this function is as follows: \\[v(\\mathbf{s}) = \\exp \\{\n\\alpha_0 + \\alpha_1 w(\\mathbf{s}) \\},\\] where \\(w(\\mathbf{s})\\) is a covariate available for any \\(\\mathbf{s} \\in \\mathcal{B}(D)\\).\n\nSpecial cases useful in practice:\n\nHomoscedastic: \\(w(\\mathbf{s}) = 0\\)\nData Fusion: \\(w(\\mathbf{s}) =\n\\mathbb{1}(\\mathcal{A}(\\mathbf{s}) &gt; 0)\\).\nArea dependent: \\(w(\\mathbf{s}) = \\mathcal{A}(\\mathbf{s})\\)\n\nAlthough flexible, one has to be careful when choosing this function to ensure the process validity (Palacios and Steel 2006).\n\n\n\nThe SD function allows the HGP to accommodate both homoscedastic and heteroscedastic scenarios."
  },
  {
    "objectID": "defense/defense.html#ensuring-validity-correlation-functions-for-the-hgp",
    "href": "defense/defense.html#ensuring-validity-correlation-functions-for-the-hgp",
    "title": "HAUSDORFF‚ÄìGAUSSIAN PROCESS WITH SPATIAL AND SPATIOTEMPORAL APPLICATIONS",
    "section": "Ensuring Validity: Correlation Functions for the HGP",
    "text": "Ensuring Validity: Correlation Functions for the HGP\nFor a valid process, its correlation function must satisfy the following properties:\n\nDiminish with increasing distance: \\(\\lim_{h \\to \\infty}r(h) = 0\\).\nBounded from above by 1: \\(r(0) = 1\\).\nPositive-definiteness: yields positive-definite correlation matrices for all its finite-dimensional marginal distributions.\nUnfortunately, functions that are guaranteed to be positive definite on \\((\\lVert \\cdot \\rVert_2, \\mathbb{R}^2)\\) are not necessarily positive definite on other metric spaces (Li et al. 2023)."
  },
  {
    "objectID": "defense/defense.html#the-powered-exponential-correlation-pec-function",
    "href": "defense/defense.html#the-powered-exponential-correlation-pec-function",
    "title": "HAUSDORFF‚ÄìGAUSSIAN PROCESS WITH SPATIAL AND SPATIOTEMPORAL APPLICATIONS",
    "section": "The Powered Exponential Correlation (PEC) Function",
    "text": "The Powered Exponential Correlation (PEC) Function\n\nPEC function: \\[r(h; \\phi, \\nu) = \\exp\\left \\{ -\n\\frac{h^{\\nu}}{\\phi^{\\nu}}\\right \\},\\] where \\(\\nu\\) is a smoothness parameter and \\(\\phi\\) governs the range of the spatial dependence.\nParametrization: We reparametrize this function with \\(\\rho =\n{\\log(10)}^{1 / \\nu} \\phi\\).\nInterpretation: \\(\\rho\\) is the distance at which the spatial correlation reduces to \\(0.10\\).\n\n\n\nexplain \\(\\rho\\)."
  },
  {
    "objectID": "defense/defense.html#visualizing-the-pec-function",
    "href": "defense/defense.html#visualizing-the-pec-function",
    "title": "HAUSDORFF‚ÄìGAUSSIAN PROCESS WITH SPATIAL AND SPATIOTEMPORAL APPLICATIONS",
    "section": "Visualizing the PEC function",
    "text": "Visualizing the PEC function"
  },
  {
    "objectID": "defense/defense.html#theoretical-foundation-positive-definiteness-of-the-pec",
    "href": "defense/defense.html#theoretical-foundation-positive-definiteness-of-the-pec",
    "title": "HAUSDORFF‚ÄìGAUSSIAN PROCESS WITH SPATIAL AND SPATIOTEMPORAL APPLICATIONS",
    "section": "Theoretical Foundation: Positive Definiteness of the PEC",
    "text": "Theoretical Foundation: Positive Definiteness of the PEC\n\n\n\nProposition\n\n\nLet \\(h = h(\\mathbf{s}, \\mathbf{s}')\\) be the Hausdorff distance between two spatial units, denoted \\(\\mathbf{s}, \\mathbf{s}' \\in \\mathcal{B}(D)\\), where \\(D\n\\subset \\mathbb{R}^2\\). The powered exponential correlation function \\(\\exp \\{ -\nh^{\\nu} / \\phi^{\\nu} \\}\\) is positive definite for \\(\\nu \\in (1/2, 1)\\).\n\n\n\n\nThe proposition above guarantees the validity of the HGP equipped with a PEC function with \\(\\nu \\in\n(1/2, 1)\\).\nThe proof is based on embedding the Hausdorff distance into a high-dimensional \\(L_1\\) normed Euclidean space, and using the fact that the exponential correlation function is positive definite on this space."
  },
  {
    "objectID": "defense/defense.html#practical-validation-empirical-assessment-of-positive-definiteness",
    "href": "defense/defense.html#practical-validation-empirical-assessment-of-positive-definiteness",
    "title": "HAUSDORFF‚ÄìGAUSSIAN PROCESS WITH SPATIAL AND SPATIOTEMPORAL APPLICATIONS",
    "section": "Practical Validation: Empirical Assessment of Positive Definiteness",
    "text": "Practical Validation: Empirical Assessment of Positive Definiteness\n\n\n\nthe graph displays the smallest eigenvalue of the correlation matrix induced by \\(\\rho\\) (x-axis) and \\(\\nu\\) (y-axis).\nThese matrices are based on the two spatial applications we will discuss in the next chapter.\nThe transparent regions indicate the regions of the parametric space where rho is smaller than 0."
  },
  {
    "objectID": "defense/defense.html#hgp-recap",
    "href": "defense/defense.html#hgp-recap",
    "title": "HAUSDORFF‚ÄìGAUSSIAN PROCESS WITH SPATIAL AND SPATIOTEMPORAL APPLICATIONS",
    "section": "HGP Recap",
    "text": "HGP Recap\n\nFlexibility: A process that handles point-referrenced, areal, and mixed spatial data by construction.\nHausdorff distance: Enables HGP‚Äôs correlation function to account for the shape, size, and orientation of spatial objects.\nValidity: Using a PEC function ensures the HGP is a valid process.\nNext Up: Simulations and real-world applications will demonstrate the HGP‚Äôs robustness.\n\n\n\n\n\n\nflowchart LR\n    A(Flexible index set) ==&gt; B(GP)\n    B ==&gt; E(((HGP)))\n    C(Hausdorff distance) ==&gt; D(PEC)\n    D ==&gt; E"
  },
  {
    "objectID": "defense/defense.html#outline",
    "href": "defense/defense.html#outline",
    "title": "HAUSDORFF‚ÄìGAUSSIAN PROCESS WITH SPATIAL AND SPATIOTEMPORAL APPLICATIONS",
    "section": "Outline",
    "text": "Outline\n\nSpatial GLMM and Bayesian Inference\nSimulation study for:\n\nAreal data\nFused data\n\nApplications\n\nAreal data and disease mapping\nFused data and air pollution"
  },
  {
    "objectID": "defense/defense.html#notation",
    "href": "defense/defense.html#notation",
    "title": "HAUSDORFF‚ÄìGAUSSIAN PROCESS WITH SPATIAL AND SPATIOTEMPORAL APPLICATIONS",
    "section": "Notation",
    "text": "Notation\n\nSet of spatial locations: \\(\\mathcal{S} = \\{ \\mathbf{s}_1, \\ldots, \\mathbf{s}_n\n\\}\\), where \\(\\mathbf{s}_i \\in \\mathcal{B}(D)\\) and \\(D \\subset \\mathbb{R}^2\\).\nResponse variable: \\(\\mathbf{Y} = {(Y(\\mathbf{s}_1), \\ldots,\nY(\\mathbf{s}_n))}^\\top\\).\nCovariates: \\(\\mathbf{X} = (\\mathbf{X}_1, \\ldots, \\mathbf{X}_p)\\), where \\(\\mathbf{X}_{j} = (x_{j1}, \\ldots, x_{jn})^\\top\\) and \\(\\mathbf{x}_i =\n(x_{1i}, \\ldots, x_{pi})\\).\nSpatial random effects: \\(\\mathbf{Z} = {(Z(\\mathbf{s}_1), \\ldots,\nZ(\\mathbf{s}_n))}^\\top\\).\nDensity functions (for response and latent variables): \\(p(\\cdot \\mid \\cdot)\\)\nPrior and posterior distributions: \\(\\pi(\\cdot)\\) and \\(\\pi(\\cdot\n\\mid \\cdot)\\)\n\n\n\nRealizations of random variables are represented by lowercase letters.\nModel paramers are greek letters."
  },
  {
    "objectID": "defense/defense.html#spatial-glmm",
    "href": "defense/defense.html#spatial-glmm",
    "title": "HAUSDORFF‚ÄìGAUSSIAN PROCESS WITH SPATIAL AND SPATIOTEMPORAL APPLICATIONS",
    "section": "Spatial GLMM",
    "text": "Spatial GLMM\nA generalized linear mixed model (GLMM) can be written as \\[\\begin{aligned}\n& Y(\\mathbf{s}_i) \\mid \\mathbf{x}_i, z(\\mathbf{s}_i) \\overset{{\\rm ind}}{\\sim}\n  f(\\cdot \\mid \\mu_i, \\omega) \\\\\n& g(\\mu_i) = g(\\mathbf{x}_i \\boldsymbol{\\beta} +\n  z(\\mathbf{s}_i))  = \\mathbb{E}[Y(\\mathbf{s}_i) \\mid \\mathbf{x}_i,\n  z(\\mathbf{s}_i)].\n\\end{aligned}\\]\n\nProbability distribution: \\(f(\\cdot)\\)\nLink function: \\(g(\\cdot)\\)\nConditional mean: \\(\\mu_i = \\mathbb{E}[Y(\\mathbf{s}_i) \\mid\n\\mathbf{x}_i, z(\\mathbf{s}_i)]\\)\nModel parameters: \\(\\theta = {\\{\\boldsymbol{\\beta}^\\top,\n\\boldsymbol{\\sigma}^\\top, \\delta^\\top, \\omega^\\top \\}}^\\top\\)\nLikelihood: \\(p(\\mathbf{y} \\mid \\mathbf{z}, \\theta) = \\prod_{i =\n1}^n f(y(\\mathbf{s}_i) \\mid \\mu_i, \\omega)\\)\n\n\n\nUnder a frequentist perspective, we consider the random effects to follow an HGP.\nFor Bayesian inference, we use the HGP as a prior for the random effects.\nResponse variable \\(Y\\) at a spatial unit \\(s\\) is modeled with an appropriate distribution (e.g., Poisson, Bernoulli).\nCovariates (\\(X\\)) and spatial random effects (\\(Z\\)) explain variation in the conditional mean (\\(\\mu\\)).\nThe HGP defines the spatial random effects (\\(Z\\)), capturing spatial correlation with the Hausdorff distance and a correlation function."
  },
  {
    "objectID": "defense/defense.html#bayesian-estimation",
    "href": "defense/defense.html#bayesian-estimation",
    "title": "HAUSDORFF‚ÄìGAUSSIAN PROCESS WITH SPATIAL AND SPATIOTEMPORAL APPLICATIONS",
    "section": "Bayesian Estimation",
    "text": "Bayesian Estimation\n\nPriors: \\(\\mathbf{Z} \\sim \\mathrm{HGP}\\{0, v(\\cdot), r(\\cdot)\n\\}\\), \\(\\boldsymbol{\\beta} \\sim \\mathcal{N}(\\mathbf{0}, 10^2\n\\mathbf{I})\\), \\(\\rho \\sim \\mathrm{Exp}(a_\\rho)\\), \\(\\nu = 0.7\\).\n\nHomoscedastic model (\\(v(\\mathbf{s}) = \\sigma\\)): \\(\\sigma \\sim t_{+}(3)\\)\nHeteroscedastic model (\\(\\log(v(\\mathbf{s})) = \\alpha_0 + \\alpha_1\nw(\\mathbf{s})\\)): \\((\\alpha_0, \\alpha_1)^\\top \\sim \\mathcal{N}(\\mathbf{0}, 10^2\n\\mathbf{I})\\)\n\nPosterior: \\(\\pi(\\theta \\mid \\mathbf{y}, \\mathbf{z}) \\propto\np(\\mathbf{y} \\mid \\mathbf{z}, \\theta) p(\\mathbf{z} \\mid \\theta) \\pi(\\theta)\\)\nMCMC sampler: No-U-Turn (Homan and Gelman 2014).\nConvergence assessment: traceplots and split-\\({\\hat{R}}\\) (Vehtari et al. 2021).\n\n\n\nPoint and interval estimates: median and percentiles (\\(0.025\\) and \\(0.975\\)) of the marginal MCMC samples.\nParameters initialized by random sampling from their priors.\nRandom effects initialized from a standard normal distribution.\n\\(a_\\rho\\) is chosen so that: \\(\\mathbb{P}(\\rho &gt; U) = p_\\rho\\). In particular, \\(a_\\rho = - \\log(p_\\rho) / \\rho_0\\).\n\\(\\nu\\) hard to be estimated."
  },
  {
    "objectID": "defense/defense.html#goodness-of-fit-and-predictions",
    "href": "defense/defense.html#goodness-of-fit-and-predictions",
    "title": "HAUSDORFF‚ÄìGAUSSIAN PROCESS WITH SPATIAL AND SPATIOTEMPORAL APPLICATIONS",
    "section": "Goodness-of-fit and Predictions",
    "text": "Goodness-of-fit and Predictions\n\nGoodness-of-fit criteria: DIC, WAIC, and LOOIC (lower values indicate better fit)\n\n\\(\\Delta_G = G(\\text{M}) - G(\\text{HGP})\\), where \\(G\\) is a model comparison criterion and \\(M\\) is a competing method.\n\nUnobserved outcome at \\(m\\) locations: \\(\\mathbf{Y}^\\ast =\n(y(\\mathbf{s}^\\ast_1), \\ldots, y(\\mathbf{s}^\\ast_m))^\\top\\).\nPosterior predictive distributions: \\(p(\\mathbf{y}^{\\ast} \\mid\n\\mathbf{y}) = \\int p(\\mathbf{y}^{\\ast} \\mid \\mathbf{z}^{\\ast}, \\theta)\np(\\mathbf{z}^{\\ast} \\mid \\mathbf{z}, \\theta) \\pi(\\theta \\mid \\mathbf{y}) {\\rm\nd} \\theta\\)\nMonte Carlo samples from \\(p(\\mathbf{y}^{\\ast} \\mid \\mathbf{y})\\) are obtained using the draws from the posterior \\(\\pi(\\theta \\mid \\mathbf{y})\\)\n\n\n\n\\(\\Delta_G &gt; 0\\) favors the proposed method.\n\nPredictions:\n\nuse the properties of GP and multivariate Normal distribution to obtain the closed-form distribution of the vector of spatial random effects \\(\\mathbf{Z}^\\ast\\) (Diggle et al. 1998);\nsample \\(\\mathbf{z}^\\ast_{(b)}\\) from the distribution derived in the previous step;\nsample \\(\\mathbf{y}^{\\ast}_{(b)}\\) from \\(p(\\mathbf{y}^{\\ast} \\mid \\theta_{(b)},\n\\mathbf{z}^{\\ast}_{(b)})\\), where \\(\\theta_{(b)}\\) is the \\(b\\)-th MCMC sample of \\(\\theta\\)."
  },
  {
    "objectID": "defense/defense.html#simulation-studies-setup",
    "href": "defense/defense.html#simulation-studies-setup",
    "title": "HAUSDORFF‚ÄìGAUSSIAN PROCESS WITH SPATIAL AND SPATIOTEMPORAL APPLICATIONS",
    "section": "Simulation Studies: Setup",
    "text": "Simulation Studies: Setup\n\nGoal: Evaluate HGP‚Äôs ability to adapt to different scenarios and compare its performance to specialized models.\n200 simulated datasets for each combination of data generation process and scenario.\nEstimation assessment: MAPE, RMSE, frequentist coverage of the credible interval (CP).\nPredictions assessment: bias, RMSP, interval score (Gneiting and Raftery 2007, IS), frequentist coverate of the prediction intervals (CPP).\n\n\n\nOverview: Emphasize the motivation behind the simulations ‚Äì to demonstrate the HGP‚Äôs ability to handle realistic scenarios and to compare its performance to established spatial models. Explain that simulations provide a controlled environment to assess model behavior.\n\n\nSimulated data on different maps and under different generating models. For each combination of map (data configuration) and generating model, we simulate 200 datasets.\nParameter used to simulate data were set to resemble the applications.\nIS: \\({\\rm IS}(y; l, u) = u - l + \\frac{2}{\\alpha} (l - y) \\mathbb1}\\{ y &lt; l\n\\} + \\frac{2}{\\alpha} (y - u) \\mathbb1}\\{ y &gt; u \\}\\)\nIS takes into account the width of the prediction intervals (PI) and a penalization factor proportional to how far a PI is from its target."
  },
  {
    "objectID": "defense/defense.html#simulation-studies-scenarios",
    "href": "defense/defense.html#simulation-studies-scenarios",
    "title": "HAUSDORFF‚ÄìGAUSSIAN PROCESS WITH SPATIAL AND SPATIOTEMPORAL APPLICATIONS",
    "section": "Simulation Studies: Scenarios",
    "text": "Simulation Studies: Scenarios"
  },
  {
    "objectID": "defense/defense.html#simulation-study-areal-data",
    "href": "defense/defense.html#simulation-study-areal-data",
    "title": "HAUSDORFF‚ÄìGAUSSIAN PROCESS WITH SPATIAL AND SPATIOTEMPORAL APPLICATIONS",
    "section": "Simulation Study: Areal Data",
    "text": "Simulation Study: Areal Data\n\nData model: Poisson GLMM, single covariate, log link function, and a spatial random effect.\nWe fit Poisson GLMMs with a homoscedastic HGP prior on the random effect.\nAssessment:\n\nParameter estimation when generating data from HGP random effects.\nHow good are the estimates of the regression coefficient under random effect misspecification?\nGoodness-of-fit comparison with DAGAR priors on the random effect.\n\n\n\n\nOverview: Explain how areal data is simulated to mimic real-world complexities. Clearly distinguish between using the HGP for simulation and for fitting. Briefly outline the BYM model as a comparison point.\n\n\nDescribe data generation processes: aggregated GPs, BYM models.\nDAGAR state-of-art specialized model\nHighlight the use of maps with different spatial configurations."
  },
  {
    "objectID": "defense/defense.html#areal-random-effects",
    "href": "defense/defense.html#areal-random-effects",
    "title": "HAUSDORFF‚ÄìGAUSSIAN PROCESS WITH SPATIAL AND SPATIOTEMPORAL APPLICATIONS",
    "section": "Areal Random Effects",
    "text": "Areal Random Effects\n\nData Generation:\n\nBesag-York-Molli√© (BYM, Besag et al. 1991): \\(\\mathbf{Z} \\sim\n\\mathcal{N}(\\mathbf{0}, \\sigma^{2}((1 - \\zeta) \\mathbf{I} + \\zeta\n\\mathbf{Q}^{-})\\), where \\(\\mathbf{Q}\\) is the precision matrix from the ICAR model and \\(\\zeta\\) is a mixing parameter.\nAggregated GP (AGP): \\(Z(\\mathbf{s}_i) =\n{\\mathcal{A}(\\mathbf{s}_i)}^{-1} \\int_{\\mathbf{s}_i} K(\\mathbf{s}) \\mathrm{d}\n\\mathbf{s}\\), where \\(\\{ K(\\mathbf{s}) \\, : \\, \\mathbf{s} \\in D \\}\\) is a (point-level) zero-mean, second order stationary and isotropic GP.\n\nCompeting Method:\n\nDAGAR: \\(\\mathbf{Z} \\sim \\mathcal{N}(\\mathbf{0}, \\Lambda(\\tau,\n\\psi))\\), where \\(\\Lambda\\) is a precision matrix, \\(\\tau\\) is the conditional precision, and \\(\\psi\\) measures spatial dependence.\n\n\n\nDAGAR priors: \\[\n\\begin{aligned}\n  \\pi(\\beta_0) & \\propto 1 \\\\\n  \\beta_1 & \\sim N(0, 1000^2) \\\\\n  \\tau & \\sim Gamma(2, 1) \\\\\n  \\psi & \\sim U(0, 1).\n\\end{aligned}\n\\] - Precision matrix is defined based on the order of the observations and adjacency matrix (and the two parameters)"
  },
  {
    "objectID": "defense/defense.html#areal-simulation-estimation",
    "href": "defense/defense.html#areal-simulation-estimation",
    "title": "HAUSDORFF‚ÄìGAUSSIAN PROCESS WITH SPATIAL AND SPATIOTEMPORAL APPLICATIONS",
    "section": "Areal Simulation: Estimation",
    "text": "Areal Simulation: Estimation\n\nGenerating data from HGP:\n\nCP of the hardest parameter to estimate (\\(\\rho\\)) was \\(92.5\\%\\) in the worst case scenario.\nFor all the other parameters, the coverage was extremely close to the nominal level.\n\nRegression parameter under misspecification:\n\nLargest (worst) MAPE was \\(6.6\\%\\).\nCP of the credible interval suggests reliable estimates.\n\n\n\n\nWorst performance under scenario 3. This may be explained by the fact that we fit an homoscedastic model and the two generating processes are heteroscedastic under that scenario."
  },
  {
    "objectID": "defense/defense.html#areal-simulation-gof",
    "href": "defense/defense.html#areal-simulation-gof",
    "title": "HAUSDORFF‚ÄìGAUSSIAN PROCESS WITH SPATIAL AND SPATIOTEMPORAL APPLICATIONS",
    "section": "Areal Simulation: GOF",
    "text": "Areal Simulation: GOF\nHGP outperforms DAGAR when simulating data from a AGP random effect.\n\n\n\nFor DIC we always win."
  },
  {
    "objectID": "defense/defense.html#simulation-study-data-fusion",
    "href": "defense/defense.html#simulation-study-data-fusion",
    "title": "HAUSDORFF‚ÄìGAUSSIAN PROCESS WITH SPATIAL AND SPATIOTEMPORAL APPLICATIONS",
    "section": "Simulation Study: Data Fusion",
    "text": "Simulation Study: Data Fusion\n\nData model: Gaussian GLMM, no covariates, identity link function, and a spatial random effect.\nWe fit the models with a heteroscedastic HGP as prior on the random effect.\nCompeting method: AGP1 and AGP2 priors on the random effect. The latter employes a finer grid/mesh resolution to approximate integrals numerically.\nParameters estimation when generating data from HGP random effects.\nModel comparison: Predictions assessment.\n\n\n\nOverview: Highlight the unique aspect of data fusion simulations ‚Äì working with different spatial data types. Explain the competing models and why they‚Äôre relevant comparisons. Stress the importance of accurate predictions and uncertainty assessment in data fusion contexts.\n\n\npoint-referrenced data have different SDs\nPC prior on \\(\\tau\\)."
  },
  {
    "objectID": "defense/defense.html#competing-method-meshes",
    "href": "defense/defense.html#competing-method-meshes",
    "title": "HAUSDORFF‚ÄìGAUSSIAN PROCESS WITH SPATIAL AND SPATIOTEMPORAL APPLICATIONS",
    "section": "Competing Method: Meshes",
    "text": "Competing Method: Meshes"
  },
  {
    "objectID": "defense/defense.html#competing-method-further-details",
    "href": "defense/defense.html#competing-method-further-details",
    "title": "HAUSDORFF‚ÄìGAUSSIAN PROCESS WITH SPATIAL AND SPATIOTEMPORAL APPLICATIONS",
    "section": "Competing Method: Further Details",
    "text": "Competing Method: Further Details\n\nThe model employs a Mat√©rn covariance function (at point level) with smoothness parameter fixed at 1. The other two parameters are the SD \\(\\sigma\\) and the practical range \\(\\rho\\). Both are analogous to the parameters with the same name in the HGP model.\nInferences are conducted using the INLA (Rue et al. 2009) and the stochastic partial differential equation (Lindgren et al. 2011, SPDE) method."
  },
  {
    "objectID": "defense/defense.html#data-fusion-simulation-estimation",
    "href": "defense/defense.html#data-fusion-simulation-estimation",
    "title": "HAUSDORFF‚ÄìGAUSSIAN PROCESS WITH SPATIAL AND SPATIOTEMPORAL APPLICATIONS",
    "section": "Data Fusion Simulation: Estimation",
    "text": "Data Fusion Simulation: Estimation\n\nThe HGP inherits same problems from geostatistical models.\nThere are no weakly consistent estimators for the variance and spatial dependence parameters (Zhang 2004)\nPredictions are still optimal (Zhang 2004)\nThe small scale variation parameter had a low coverage in the credible intervals (\\(79.5\\%\\) for scenario 2).\nThe MAPE for the \\(\\sigma\\), \\(\\sigma_a\\), and \\(\\rho\\) parameters fluctuated around \\(10\\%\\) in both scenarios.\n\n\n\nConsistency and concentration of the posteiors Schervish.\nThis limitations partially explains the estimation results."
  },
  {
    "objectID": "defense/defense.html#data-fusion-simulation-prediction",
    "href": "defense/defense.html#data-fusion-simulation-prediction",
    "title": "HAUSDORFF‚ÄìGAUSSIAN PROCESS WITH SPATIAL AND SPATIOTEMPORAL APPLICATIONS",
    "section": "Data Fusion Simulation: Prediction",
    "text": "Data Fusion Simulation: Prediction\n\nUncertainty of the predictions: The HGP handles uncertainty in its predictions better than the AGP1 model, even when the latter reflects the true data generation process.\n\n\n\n\nHGP as an AGP Approximation: The HGP delivers satisfactory approximations to an AGP model.\nHGP Advantages: The HGP offers potential computational advantages for large datasets while maintaining clear parameter interpretation.\n\n\n\nPoint Estimates: AGP2 model consistently deliver point estimates (single value predictions) with minimal bias.\nThe proposed methodology excelled in the predictions.\nAGP1 underestimating variability: evident in both correctly specified and misspecified cases.\nHGP avoids the need for defining a grid resolution used in numerical calculations."
  },
  {
    "objectID": "defense/defense.html#respiratory-disease-hospitalization",
    "href": "defense/defense.html#respiratory-disease-hospitalization",
    "title": "HAUSDORFF‚ÄìGAUSSIAN PROCESS WITH SPATIAL AND SPATIOTEMPORAL APPLICATIONS",
    "section": "Respiratory Disease Hospitalization",
    "text": "Respiratory Disease Hospitalization\n\nSample units: 134 intermediate zones (IZ), where the \\(i\\)-th IZ is denoted \\(\\mathbf{s}_i\\).\nNumber of hospitalizations: \\(\\mathbf{Y} = {(Y(\\mathbf{s}_1),\n\\ldots, Y(\\mathbf{s}_n))}^\\top\\).\nExpected number of hospitalizations based on the national age- and sex-standardized rates: \\(E_i\\).\nPercentage of people classified as income deprived: \\(\\mathbf{X}\\).\n\n\\[\\begin{aligned}\n& (Y(\\mathbf{s}_i) \\mid x_i, z(\\mathbf{s}_i)) \\sim \\text{Poisson}(E_i \\mu_i) \\\\\n& \\log(\\mu_i) = \\beta_0 + x_i \\beta_1 + z(\\mathbf{s}_i)\n\\end{aligned}\\]\n\n\nData from the north portion of the river Clyde in the Great Glasgow and Clyde health board in Scotland.\nAssumption: Conditional on \\(\\mathbf{Z}\\), \\(\\mathbf{Y}\\) are mutually independent.\nExplain goals of the analysis‚Äìestimate risks adjusted for income deprivation, compare methods.\n\\(\\mu_i\\) is the expected SIR at region \\(i\\).\nOutline the modeling choices (HGP, DAGAR, BYM), briefly mention priors."
  },
  {
    "objectID": "defense/defense.html#standardized-incidence-ratio-sir",
    "href": "defense/defense.html#standardized-incidence-ratio-sir",
    "title": "HAUSDORFF‚ÄìGAUSSIAN PROCESS WITH SPATIAL AND SPATIOTEMPORAL APPLICATIONS",
    "section": "Standardized Incidence Ratio (SIR)",
    "text": "Standardized Incidence Ratio (SIR)"
  },
  {
    "objectID": "defense/defense.html#disease-mapping-gof",
    "href": "defense/defense.html#disease-mapping-gof",
    "title": "HAUSDORFF‚ÄìGAUSSIAN PROCESS WITH SPATIAL AND SPATIOTEMPORAL APPLICATIONS",
    "section": "Disease Mapping: GOF",
    "text": "Disease Mapping: GOF\n\n\n\n\nHGP\nDAGAR\nBYM\n\n\n\n\nLOOIC\n1081.5\n1084.0\n1091.1\n\n\nWAIC\n1038.7\n1032.2\n1041.1\n\n\nDIC\n1048.8\n1050.4\n1050.6\n\n\n\n\n\n\nBetter fit in 2 out of 3 metrics\nFitted values are similar among the three models\nSome regions with very low risks are shrunk toward a ‚Äúregional mean‚Äù"
  },
  {
    "objectID": "defense/defense.html#disease-mapping-estimation",
    "href": "defense/defense.html#disease-mapping-estimation",
    "title": "HAUSDORFF‚ÄìGAUSSIAN PROCESS WITH SPATIAL AND SPATIOTEMPORAL APPLICATIONS",
    "section": "Disease Mapping: Estimation",
    "text": "Disease Mapping: Estimation\n\n\n\n\n\n\n\n\n\n\n\nHGP\nDAGAR\nBYM\n\n\n\n\n\\(\\beta_0\\)\n‚Äì0.208 (‚Äì0.265, ‚Äì0.127)\n‚Äì0.261 (‚Äì0.459, ‚Äì0.140)\n‚Äì0.219 (‚Äì0.257, ‚Äì0.183)\n\n\n\\(\\beta_1\\)\n0.325 (0.285, 0.366)\n0.313 (0.258, 0.370)\n0.326 (0.286, 0.367)\n\n\n\\(\\sigma\\)\n0.190 (0.154, 0.242)\n0.300 (0.217, 0.489)\n0.238 (0.176, 0.347)\n\n\n\\(\\rho\\)\n2.360 (0.140, 8.144)\n\n\n\n\n\\(\\psi\\)\n\n0.435 (0.063, 0.830)\n\n\n\n\\(\\zeta\\)\n\n\n0.428 (0.101, 0.811)\n\n\n\n\n\n\nmedian (95% CI)\nsimilar estimates for covariate\nHGP indicates weak spatial correlation\nDAGAR and BYM spatial dependence are not helpful as they indicate the spatial correlation goes from weak to strong (according to the CI)"
  },
  {
    "objectID": "defense/defense.html#disease-mapping-spatial-dependence",
    "href": "defense/defense.html#disease-mapping-spatial-dependence",
    "title": "HAUSDORFF‚ÄìGAUSSIAN PROCESS WITH SPATIAL AND SPATIOTEMPORAL APPLICATIONS",
    "section": "Disease Mapping: Spatial Dependence",
    "text": "Disease Mapping: Spatial Dependence\n\n\n\nHGP provides more insight into spatial dependence\nWe could also plot the correlation function itself"
  },
  {
    "objectID": "defense/defense.html#disease-mapping-insights",
    "href": "defense/defense.html#disease-mapping-insights",
    "title": "HAUSDORFF‚ÄìGAUSSIAN PROCESS WITH SPATIAL AND SPATIOTEMPORAL APPLICATIONS",
    "section": "Disease Mapping: Insights",
    "text": "Disease Mapping: Insights\n\nHGP Advantages: The HGP provides a simpler and more intuitive understanding of spatial dependence compared to classical methods.\nAssumptions: The HGP‚Äôs core assumptions about spatial relationships differ from those used in specialized models for areal data.\nSpatial Correlation: In the HGP, spatial correlation decreases as the Hausdorff distance between areas increases.\nSpecialized Models Complexity: Areal data models often rely on more complex spatial relationships based on detailed factors like shared borders and the number of neighboring units."
  },
  {
    "objectID": "defense/defense.html#data-analysis-air-pollution-in-ventura-and-los-angeles",
    "href": "defense/defense.html#data-analysis-air-pollution-in-ventura-and-los-angeles",
    "title": "HAUSDORFF‚ÄìGAUSSIAN PROCESS WITH SPATIAL AND SPATIOTEMPORAL APPLICATIONS",
    "section": "Data Analysis: Air Pollution in Ventura and Los Angeles",
    "text": "Data Analysis: Air Pollution in Ventura and Los Angeles\n\n\n\n\nPoint-referrenced data from 19 measurement stations available daily from 1999 to date;\nSatellite-derived estimates (2010‚Äì2012) at 184 areal units.\nPM2.5: \\(\\mathbf{Y} = {(Y(\\mathbf{s}_1), \\ldots,\nY(\\mathbf{s}_n))}^\\top\\).\nModel: \\((Y(\\mathbf{s}_i) \\mid z(\\mathbf{s}_i)) \\sim\n\\mathcal{N}(\\beta_0 + z(\\mathbf{s}_i), \\tau^2)\\)\n\n\n\n¬†\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ngoals: (1); estimate model parameters using both sources. (2) use satellite data to help with interpolation.\nsquare areas (average size \\(\\approx\\) 101.95 km2)\ndata fusion challenges\nadaptability of the hgp to different problems.\nPm25 scale is micrometre per cubic meter: one one-millionth of a meter.\nconditional on \\(z\\) the \\(y\\)s are mutually independent.\nPriors on random effects: AGP1 and 2; heteroscedastic HGP with different variances for areal and point-referrenced data."
  },
  {
    "objectID": "defense/defense.html#air-pollution-meshes",
    "href": "defense/defense.html#air-pollution-meshes",
    "title": "HAUSDORFF‚ÄìGAUSSIAN PROCESS WITH SPATIAL AND SPATIOTEMPORAL APPLICATIONS",
    "section": "Air Pollution: Meshes",
    "text": "Air Pollution: Meshes"
  },
  {
    "objectID": "defense/defense.html#air-pollution-gof-and-10-fold-cv",
    "href": "defense/defense.html#air-pollution-gof-and-10-fold-cv",
    "title": "HAUSDORFF‚ÄìGAUSSIAN PROCESS WITH SPATIAL AND SPATIOTEMPORAL APPLICATIONS",
    "section": "Air Pollution: GOF and 10-fold CV",
    "text": "Air Pollution: GOF and 10-fold CV\n\n\n\n\n\nHGP\nAGP1\nAGP2\n\n\n\n\nLOOIC\n505.1\n746.7\n‚Äì473.5\n\n\nWAIC\n507.2\n745.3\n‚Äì876.2\n\n\nDIC\n468.7\n740.7\n‚Äì819.3"
  },
  {
    "objectID": "defense/defense.html#air-pollution-gof-and-10-fold-cv-1",
    "href": "defense/defense.html#air-pollution-gof-and-10-fold-cv-1",
    "title": "HAUSDORFF‚ÄìGAUSSIAN PROCESS WITH SPATIAL AND SPATIOTEMPORAL APPLICATIONS",
    "section": "Air Pollution: GOF and 10-fold CV",
    "text": "Air Pollution: GOF and 10-fold CV\n\n\n\n\n\n\n\n\n\n\n\nType\nModel\nBias\nRMSP\nIS\n\n\n\n\nOverall\nHGP\n‚Äì0.019\n0.214\n4.802\n\n\n\nAGP1\n‚Äì0.005\n0.333\n15.114\n\n\n\nAGP2\n‚Äì0.281\n0.525\n13.648\n\n\nPoint\nHGP\n‚Äì0.718\n2.048\n12.686\n\n\n\nAGP1\n‚Äì3.298\n3.829\n111.838\n\n\n\nAGP2\n‚Äì3.536\n4.095\n45.034\n\n\nPolygon\nHGP\n0.003\n0.111\n3.829\n\n\n\nAGP1\n0.301\n0.324\n6.034\n\n\n\nAGP2\n0.015\n0.251\n10.918"
  },
  {
    "objectID": "defense/defense.html#air-pollution-estimation",
    "href": "defense/defense.html#air-pollution-estimation",
    "title": "HAUSDORFF‚ÄìGAUSSIAN PROCESS WITH SPATIAL AND SPATIOTEMPORAL APPLICATIONS",
    "section": "Air Pollution: Estimation",
    "text": "Air Pollution: Estimation\n\n\n\n\n\n\n\n\n\n\n\nHGP\nAGP1\nAGP2\n\n\n\n\n\\(\\beta\\)\n5.605 (4.688, 6.449)\n6.219 (2.160, 10.100)\n6.187 (5.879, 6.478)\n\n\n\\(\\rho\\)\n1.383 (0.782, 2.36)\n1.316 (0.514, 3.024)\n0.063 (0.046, .083)\n\n\n\\(\\sigma\\)\n3.849 (2.921, 5.181)\n1.703 (0.960, 2.910)\n2.399 (2.024, 2.837)\n\n\n\\(\\tau\\)\n0.178 (0.073, 0.312)\n1.394 (1.242, 1.563)\n0.539 (0.394, 0.719)\n\n\n\\(\\sigma_a\\)\n1.241 (1.037, 1.505)\n\n\n\n\n\n\n\n\n\\(\\rho\\) rescaled to 100s of km.\nstaggering difference in estimates of the same model with different meshes."
  },
  {
    "objectID": "defense/defense.html#air-pollution-change-of-support",
    "href": "defense/defense.html#air-pollution-change-of-support",
    "title": "HAUSDORFF‚ÄìGAUSSIAN PROCESS WITH SPATIAL AND SPATIOTEMPORAL APPLICATIONS",
    "section": "Air Pollution: Change-of-Support",
    "text": "Air Pollution: Change-of-Support\n\n\n\nFigure confirms what we learned from 10-fold CV and simulation studies.\nHGP allows to use data from different sources seamlessly\nNo dependence on meshes/grids\nApparently, a parsimonious compromise between the two meshes.\nOnce again, the AGP seems highly dependend on the mesh and this is not highlighted in the literature."
  },
  {
    "objectID": "defense/defense.html#highlights",
    "href": "defense/defense.html#highlights",
    "title": "HAUSDORFF‚ÄìGAUSSIAN PROCESS WITH SPATIAL AND SPATIOTEMPORAL APPLICATIONS",
    "section": "Highlights",
    "text": "Highlights\n\nFlexibility & Interpretability: Models diverse spatial data types and provides meaningful spatial parameters, regardless of unit size or shape.\nPredictions: The HGP delivers accurate predictions\nAreal Data: The model delivers valuable insights on the spatial dependence and accounts for sample units‚Äô size, shape, and orientation.\nData Fusion: The HGP simplifies the modeling process eliminating the need of a user-defined grid or mesh. Consequently, avoids computationally intensive numerical integrations with arbitrary precision, making inference seamless."
  },
  {
    "objectID": "defense/defense.html#tuberculosis-in-brazil",
    "href": "defense/defense.html#tuberculosis-in-brazil",
    "title": "HAUSDORFF‚ÄìGAUSSIAN PROCESS WITH SPATIAL AND SPATIOTEMPORAL APPLICATIONS",
    "section": "Tuberculosis in Brazil",
    "text": "Tuberculosis in Brazil\n\nTB in Context: Tuberculosis (TB) remains a major global health threat, second in infectious disease mortality only to COVID-19.\nBrazilian TB Trends: Brazil‚Äôs national TB incidence declined only slightly from 2006‚Äì2015. Rio Grande do Sul (RS) reported significantly higher incidence than the national average in 2021, with the eastern region even more affected.\nSpatial Dependence: Studies demonstrate strong spatial dependence of TB infections (and co-infections) in Brazil, both nationwide and in specific cities. Time dependence has been largely overlooked in the literature.\nRisk Factors: TB risk factors include densely populated areas, poverty, substance abuse, and incarceration.\n\n\n\nairborne infectious disease\nWe focus on 54 municipalities in the Eastern part of the state (briefly explain why: zero-inflation, big n.)"
  },
  {
    "objectID": "defense/defense.html#objectives",
    "href": "defense/defense.html#objectives",
    "title": "HAUSDORFF‚ÄìGAUSSIAN PROCESS WITH SPATIAL AND SPATIOTEMPORAL APPLICATIONS",
    "section": "Objectives",
    "text": "Objectives\n\nReliable incidence estimates:\n\nTB incidence across the study period, incorporating location-specific factors.\nSmaller municipalities benefit from ‚Äúborrowed strength‚Äù based on Hausdorff distance from neighbors, improving estimate reliability.\nResults enable the calculation of SIRs to pinpoint high-risk areas.\n\nForecasts:\n\nTB incidence rates one year ahead.\nThese predictions offer crucial insights for proactive public health planning."
  },
  {
    "objectID": "defense/defense.html#model",
    "href": "defense/defense.html#model",
    "title": "HAUSDORFF‚ÄìGAUSSIAN PROCESS WITH SPATIAL AND SPATIOTEMPORAL APPLICATIONS",
    "section": "Model",
    "text": "Model\n\nSample units: 54 municipalities, across 11 years (2011 to 2021). We use 2022 to assess the quality of predictions.\nNumber of TB cases: \\(Y_t(\\mathbf{s}_i) = Y(\\mathbf{s}_i, t)\\) at location \\(\\mathbf{s}_i\\) and time \\(t\\).\nPopulation: \\(P_t(\\mathbf{s}_i)\\).\nFive covariates and two way interactions between presence of prison and continuous variables (except IDESE).\nThe priors are the same as in the spatial setting. Here, we also placed a prior on \\(\\nu\\).\n\n\\[\\begin{aligned}\n& (Y_t(\\mathbf{s}_i) \\mid \\mathbf{X}_{t}(\\mathbf{s}_i), z_t(\\mathbf{s}_i))\n  \\overset{{\\rm ind}}{\\sim}\n  \\text{Poisson}(P_t(\\mathbf{s}_i) \\mu_{it}) \\\\\n& \\log(\\mu_{it}) = \\beta_0 + \\mathbf{X}^\\top_{t}(\\mathbf{s}_i) \\beta + z_t(\\mathbf{s}_i)\n\\end{aligned}\\]\n\n\n\\(\\mu_it\\): rate\nA weakly informative Beta prior with mode at 0.75 and standard deviation around 0.22.\nPrior sensitivity using Importance Sampling and power-scaling."
  },
  {
    "objectID": "defense/defense.html#spatiotemporal-trend",
    "href": "defense/defense.html#spatiotemporal-trend",
    "title": "HAUSDORFF‚ÄìGAUSSIAN PROCESS WITH SPATIAL AND SPATIOTEMPORAL APPLICATIONS",
    "section": "Spatiotemporal Trend",
    "text": "Spatiotemporal Trend\n\n\n\n\nAnother COVARIATE: Presence of prison. Around 24% of the municipalities had at least one prison. Incarcerated population was not available at a yearly basis.\n\nCases per 100k inhabitants.\n2015 and 2016: 2014 - World Cup; 2016 Olympic Games.\n2019 COVID.\nIn general, we can see a spatial pattern and a decreasing trend."
  },
  {
    "objectID": "defense/defense.html#explanatory-variables",
    "href": "defense/defense.html#explanatory-variables",
    "title": "HAUSDORFF‚ÄìGAUSSIAN PROCESS WITH SPATIAL AND SPATIOTEMPORAL APPLICATIONS",
    "section": "Explanatory Variables",
    "text": "Explanatory Variables\n\n\n\npop. dens: log and then scaled.\nidese: scaled (IDESE is released yearly in RS and it is similar to IDH).\nhomicide rates: EB then log-transformed then scaled\nHSDR: scaled.\nTransformations: Avoiding long-tails and highly influential observations"
  },
  {
    "objectID": "defense/defense.html#random-effects",
    "href": "defense/defense.html#random-effects",
    "title": "HAUSDORFF‚ÄìGAUSSIAN PROCESS WITH SPATIAL AND SPATIOTEMPORAL APPLICATIONS",
    "section": "Random Effects",
    "text": "Random Effects\n\nNo random effects\nUnstructured: Random intercept per municipality (iid Normal)\nTime dependent: \\(\\mathrm{AR}(1)\\)\nSpatial: Homoscedastic HGP\nSeparable spatiotemporal: \\(\\mathrm{AR}(1)\\) + Homoscedastic HGP, Multivariate \\(\\mathrm{AR}(1)\\) with HGP errors\nSeparable spatiotemporal and unstructured: Multivariate \\(\\mathrm{AR}(1)\\) with HGP errors + random intercept per municipality\n\n\n\nModel comparison criteria are the same as in the last chapter\nPC prior for the AR coefficient\nBest model was the more complex (by a lot)\nCompared results with analogous models having other spatial structures"
  },
  {
    "objectID": "defense/defense.html#results-gof-and-predictive-performance",
    "href": "defense/defense.html#results-gof-and-predictive-performance",
    "title": "HAUSDORFF‚ÄìGAUSSIAN PROCESS WITH SPATIAL AND SPATIOTEMPORAL APPLICATIONS",
    "section": "Results: GOF and Predictive Performance",
    "text": "Results: GOF and Predictive Performance\n\n\n\n\n\n\n\n\n\n\n\n\n\nLOOIC\nWAIC\nDIC\nRMSP\nIS\n\n\n\n\nBYM\n3606.1\n3449.9\n3505.4\n123.3\n176.6\n\n\nDAGAR\n3520.9\n3447.5\n3443.6\n22.4\n88.8\n\n\nHGP\n3516.1\n3447.7\n3450.4\n21.1\n87.8\n\n\n\n\n\n\nHGP vs.¬†Specialized Models: When compared to specialized models like DAGAR, the HGP demonstrates highly competitive GOF performance.\nMinor Differences: While DAGAR slightly outperformed on some fit criteria, the overall goodness-of-fit differences between the HGP and DAGAR were minimal.\nPredictive Power: The HGP excels in predictive performance, outperforming competitors on both RMSP (point prediction accuracy) and IS (interval prediction accuracy)."
  },
  {
    "objectID": "defense/defense.html#results-relative-risks",
    "href": "defense/defense.html#results-relative-risks",
    "title": "HAUSDORFF‚ÄìGAUSSIAN PROCESS WITH SPATIAL AND SPATIOTEMPORAL APPLICATIONS",
    "section": "Results: Relative Risks",
    "text": "Results: Relative Risks\n\n\n\n\nParameter\nEstimate\n\n\n\n\n\\(\\exp(\\beta_1)\\)\n2.34 (1.70, 3.19)\n\n\n\\(\\exp(\\beta_2)\\)\n1.33 (1.15, 1.56)\n\n\n\\(\\exp(\\beta_3)\\)\n1.03 (0.99, 1.07)\n\n\n\\(\\exp(\\beta_4)\\)\n0.97 (0.93, 1.00)\n\n\n\\(\\exp(\\beta_5)\\)\n0.99 (0.92, 1.07)\n\n\n\\(\\exp(\\beta_2 + \\beta_{21})\\)\n1.75 (1.18, 2.52)\n\n\n\\(\\exp(\\beta_3 + \\beta_{31})\\)\n2.25 (1.63, 3.09)\n\n\n\\(\\exp(\\beta_4 + \\beta_{41})\\)\n2.51 (1.83, 3.46)\n\n\n\n\n\n\nrelative risk estimates derived from regression coefficients.\n(1): dummy for prison presence; (2) pop/km2; (3) homicides; (4) high school;\n\nIDESE;\n\nDouble-digit indices represent interactions between variables\nMunicipalities with prisons have a 2.34 times higher average TB incidence, even when controlling for other factors.\nhomicide rates and hsdr not important in prison-free municipalities but worsen the risk in municipalities with prisons."
  },
  {
    "objectID": "defense/defense.html#spatiotemporal-dependence",
    "href": "defense/defense.html#spatiotemporal-dependence",
    "title": "HAUSDORFF‚ÄìGAUSSIAN PROCESS WITH SPATIAL AND SPATIOTEMPORAL APPLICATIONS",
    "section": "Spatiotemporal Dependence",
    "text": "Spatiotemporal Dependence\n\n\n\nImportance of spatiotemporal term\nWithout the municipality random intercept we get even stronger spatial dependence.\nNot possible to make such graph for other areal models."
  },
  {
    "objectID": "defense/defense.html#small-municipalities",
    "href": "defense/defense.html#small-municipalities",
    "title": "HAUSDORFF‚ÄìGAUSSIAN PROCESS WITH SPATIAL AND SPATIOTEMPORAL APPLICATIONS",
    "section": "Small Municipalities",
    "text": "Small Municipalities"
  },
  {
    "objectID": "defense/defense.html#forecast",
    "href": "defense/defense.html#forecast",
    "title": "HAUSDORFF‚ÄìGAUSSIAN PROCESS WITH SPATIAL AND SPATIOTEMPORAL APPLICATIONS",
    "section": "Forecast",
    "text": "Forecast"
  },
  {
    "objectID": "defense/defense.html#forecast-1",
    "href": "defense/defense.html#forecast-1",
    "title": "HAUSDORFF‚ÄìGAUSSIAN PROCESS WITH SPATIAL AND SPATIOTEMPORAL APPLICATIONS",
    "section": "Forecast",
    "text": "Forecast"
  },
  {
    "objectID": "defense/defense.html#highlights-1",
    "href": "defense/defense.html#highlights-1",
    "title": "HAUSDORFF‚ÄìGAUSSIAN PROCESS WITH SPATIAL AND SPATIOTEMPORAL APPLICATIONS",
    "section": "Highlights",
    "text": "Highlights\n\nTailored an HGP extension for spatiotemporal disease mapping.\nCompetitive with specialized models\nIt helps to gain insights into spatiotemporal disease mapping through spatiotemporal correlation functions.\nMore reliable estimates of risk factors\nOut-of-sample predictions to inform public policies"
  },
  {
    "objectID": "defense/defense.html#conclusion",
    "href": "defense/defense.html#conclusion",
    "title": "HAUSDORFF‚ÄìGAUSSIAN PROCESS WITH SPATIAL AND SPATIOTEMPORAL APPLICATIONS",
    "section": "Conclusion",
    "text": "Conclusion\nThe HGP offers a powerful and versatile model for spatial analysis. Advantages include:\n\nCommon framework for diverse spatial data types.\nCompetitive performance when compared to specialized models.\nDirectly interpretable spatial parameters.\nReliable and accurate predictions."
  },
  {
    "objectID": "defense/defense.html#future-work-and-limitations",
    "href": "defense/defense.html#future-work-and-limitations",
    "title": "HAUSDORFF‚ÄìGAUSSIAN PROCESS WITH SPATIAL AND SPATIOTEMPORAL APPLICATIONS",
    "section": "Future work and Limitations",
    "text": "Future work and Limitations\n\nExtensions:\n\nnon-Euclidean spaces\nBig data\n\nLimitations:\n\nDifficulties in obtaining spectral densities for correlation functions\nBig ‚Äún‚Äù problem inherited from geostatistics\nUnclear how to incorporate anisotropy"
  },
  {
    "objectID": "defense/defense.html#references",
    "href": "defense/defense.html#references",
    "title": "HAUSDORFF‚ÄìGAUSSIAN PROCESS WITH SPATIAL AND SPATIOTEMPORAL APPLICATIONS",
    "section": "",
    "text": "References\n\n\n\n\n\n\n\n\nBesag, J., York, J., and Molli√©, A. (1991), ‚ÄúBayesian image restoration, with two applications in spatial statistics,‚Äù Annals of the Institute of Statistical Mathematics, 43, 1‚Äì20.\n\n\nCressie, N. (1993), Statistics for spatial data, Wiley series in probability and statistics, Wiley.\n\n\nDatta, A., Banerjee, S., Hodges, J. S., and Gao, L. (2019), ‚ÄúSpatial disease mapping using directed acyclic graph auto-regressive (DAGAR) models,‚Äù Bayesian analysis, NIH Public Access, 14, 1221.\n\n\nDean, C., Ugarte, M., and Militino, A. (2001), ‚ÄúDetecting interaction between random region and fixed age effects in disease mapping,‚Äù Biometrics, Wiley Online Library, 57, 197‚Äì202.\n\n\nDiggle, P. J., Tawn, J. A., and Moyeed, R. A. (1998), ‚ÄúModel-based geostatistics,‚Äù Journal of the Royal Statistical Society Series C: Applied Statistics, Oxford University Press, 47, 299‚Äì350.\n\n\nGneiting, T., and Raftery, A. E. (2007), ‚ÄúStrictly proper scoring rules, prediction, and estimation,‚Äù Journal of the American statistical Association, Taylor & Francis, 102, 359‚Äì378.\n\n\nGon√ßalves, F. B., and Gamerman, D. (2018), ‚ÄúExact Bayesian inference in spatiotemporal Cox processes driven by multivariate Gaussian processes,‚Äù Journal of the Royal Statistical Society: Series B (Statistical Methodology), Wiley Online Library, 80, 157‚Äì175.\n\n\nHoman, M. D., and Gelman, A. (2014), ‚ÄúThe No-U-turn sampler: Adaptively setting path lengths in hamiltonian Monte Carlo,‚Äù Journal of Machine Learning Research, JMLR.org, 15, 1593‚Äì1623.\n\n\nJohnson, O., Diggle, P., and Giorgi, E. (2020), ‚ÄúDealing with spatial misalignment to model the relationship between deprivation and life expectancy: A model-based geostatistical approach,‚Äù International Journal of Health Geographics, Springer, 19, 1‚Äì13.\n\n\nLeroux, B. G., Lei, X., and Breslow, N. (2000), ‚ÄúEstimation of disease rates in small areas: A new mixed model for spatial dependence,‚Äù in Statistical models in epidemiology, the environment, and clinical trials, Springer, pp. 179‚Äì191.\n\n\nLi, D., Tang, W., and Banerjee, S. (2023), ‚ÄúInference for Gaussian processes with Mat√©rn covariogram on compact Riemannian manifolds,‚Äù Journal of Machine Learning Research, 24, 1‚Äì26.\n\n\nLindgren, F., Rue, H., and Lindstr√∂m, J. (2011), ‚ÄúAn explicit link between Gaussian fields and Gaussian Markov random fields: The stochastic partial differential equation approach,‚Äù Journal of the Royal Statistical Society: Series B (Statistical Methodology), 73, 423‚Äì498. https://doi.org/https://doi.org/10.1111/j.1467-9868.2011.00777.x.\n\n\nMin, D., Zhilin, L., and Xiaoyong, C. (2007), ‚ÄúExtended Hausdorff distance for spatial objects in GIS,‚Äù International Journal of Geographical Information Science, Taylor & Francis, 21, 459‚Äì475.\n\n\nMoraga, P., Cramb, S. M., Mengersen, K. L., and Pagano, M. (2017), ‚ÄúA geostatistical model for combined analysis of point-level and area-level data using INLA and SPDE,‚Äù Spatial Statistics, Elsevier, 21, 27‚Äì41.\n\n\nPalacios, M. B., and Steel, M. F. J. (2006), ‚ÄúNon-Gaussian Bayesian geostatistical modeling,‚Äù Journal of the American Statistical Association, Taylor & Francis, 101, 604‚Äì618.\n\n\nRue, H., Martino, S., and Chopin, N. (2009), ‚ÄúApproximate Bayesian inference for latent Gaussian models by using integrated nested Laplace approximations,‚Äù Journal of the Royal Statistical Society: Series B (Statistical Methodology), Wiley Online Library, 71, 319‚Äì392.\n\n\nSendov, B. (2004), ‚ÄúHausdorff distance and image processing,‚Äù Russian Mathematical Surveys, IOP Publishing, 59, 319. https://doi.org/10.1070/RM2004v059n02ABEH000721.\n\n\nVehtari, A., Gelman, A., Simpson, D., Carpenter, B., and B√ºrkner, P.-C. (2021), ‚ÄúRank-normalization, folding, and localization: An improved \\(\\hat{R}\\) for assessing convergence of MCMC (with discussion),‚Äù Bayesian Analysis, International Society for Bayesian Analysis, 16, 667‚Äì718.\n\n\nZhang, H. (2004), ‚ÄúInconsistent estimation and asymptotically equal interpolations in model-based geostatistics,‚Äù Journal of the American Statistical Association, Taylor & Francis, 99, 250‚Äì261."
  },
  {
    "objectID": "posts/jupyter_hpc/jupyter_hpc.html",
    "href": "posts/jupyter_hpc/jupyter_hpc.html",
    "title": "Using jupyter on an HPC cluster",
    "section": "",
    "text": "This post was originally written for the CBC-UCONN HC wiki\nThe purpose of this post is to enable cluster users to run jupyter on the cluster interactively, enabling them to conduct data analysis and visualization. There are different ‚Äúflavors‚Äù of jupyter notebooks, the most appropriate are going to be pointed out at picking a container."
  },
  {
    "objectID": "posts/jupyter_hpc/jupyter_hpc.html#preliminaries",
    "href": "posts/jupyter_hpc/jupyter_hpc.html#preliminaries",
    "title": "Using jupyter on an HPC cluster",
    "section": "Preliminaries",
    "text": "Preliminaries\nWe assume that the user has access to ssh through a terminal. In addition, it is necessary to have SingularityCE (singularity for short) installed on a computer on which you have root privileges.\n\nIn order to use singularity on a Windows (or Mac) machine, a Linux Virtual Machine(VM) needs to be set up. Setting up a VM and installing SingularityCE os beyond the scope of this document.\n\nIn addition, singularity is assumed to be available on the HPC that you have access to. Usually, users have to run\n\nmodule load singularity/&lt;version&gt;\n\nbefore using it.\nFamiliarity with containers is helpful but not necessary. Loosely speaking, a container allows us to ‚Äúisolate‚Äù a set of tools and software in order to guarantee code reproducibility and portability. Moreover, singularity was developed (among other reasons) to integrate these tools with HPC clusters."
  },
  {
    "objectID": "posts/jupyter_hpc/jupyter_hpc.html#pick-cont",
    "href": "posts/jupyter_hpc/jupyter_hpc.html#pick-cont",
    "title": "Using jupyter on an HPC cluster",
    "section": "Picking a container",
    "text": "Picking a container\nThe Jupyter Docker Stacks contains several useful docker containers that can be easily used to build singularity containers.\nA list of the containers (along with their principal features) maintained by the Jupyter team can be found here. Some of these containers are detailed below\n\njupyter/r-notebook: a container containing a basic installation for Machine Learning using R.\njupyter/scipy-notebook: contains popular libraries for scientific computing using python.\njupyter/tensorflow-notebook: this is the jupyter/scipy-notebook with tensorflow installed on it.\njupyter/datascience-notebook: includes libraries for data analysis from the julia, python, andR` communities."
  },
  {
    "objectID": "posts/jupyter_hpc/jupyter_hpc.html#converting-a-docker-into-a-singularity-container",
    "href": "posts/jupyter_hpc/jupyter_hpc.html#converting-a-docker-into-a-singularity-container",
    "title": "Using jupyter on an HPC cluster",
    "section": "Converting a docker into a singularity container",
    "text": "Converting a docker into a singularity container\nOnce you have chosen a container suitable for your needs (and have root access to a machine with singularity), a singularity container can be generated by executing the following chunk of code in the terminal.\n\n## singularity pull &lt;choose-a-name&gt;.sif docker://jupyter/&lt;preferred-notebook&gt;\nsingularity pull mycontainer.sif docker://jupyter/datascience-notebook\n\nIn the example above, I choose to use the datascience-notebook. After doing so, the .sif file generated by singularity needs to be transferred to the cluster. My personal preference is to use either scp or rsync, for example\n\nrsync mycontainer.sif &lt;username&gt;@&lt;hpc-url&gt;:&lt;location&gt;"
  },
  {
    "objectID": "posts/jupyter_hpc/jupyter_hpc.html#using-the-singularity-container-on-the-cluster",
    "href": "posts/jupyter_hpc/jupyter_hpc.html#using-the-singularity-container-on-the-cluster",
    "title": "Using jupyter on an HPC cluster",
    "section": "Using the singularity container on the cluster",
    "text": "Using the singularity container on the cluster\nAfter transferring the .sif file to the cluster, follow the following steps. Firstly, set up the VPN and log-in to the cluster using ssh, then navigate to the location where you transferred the container (.sif) to. Next, you will have to start a interactive job. If the workload manager used in the HPC that you have access to is SLURM, this can be done either with srun or fisbatch. To start an interactive job with srun use\n\nsrun --partition=&lt;partition-name&gt; --qos=&lt;queue-name&gt; --mem=64G --pty bash\n\nThe same task can be achieved with fisbatch (if available) with\n\nfisbatch --partition=&lt;partition-name&gt; --qos=&lt;queue-name&gt; --mem=64G\n\nEither of these commands will allocate your job to a specific node. It is important to save the name of the node that your job has been allocated to. Next, load singularity on that node as follows\n\nmodule load singularity/&lt;version&gt;\n\nThe penultimate step is to start the jupyter instance. It is done as follows\n\nsingularity exec --nv mycontainer.sif jupyter notebook --no-browser --ip='*'\n\nAfter executing the last chunk of code, the terminal will be ‚Äúbusy‚Äù and will provide three URLs, they will look somewhat like ‚Äúhttp://127.0.0.1:8888/‚Äù this. Copy the last address provided by the output. The last step before being able to access the notebook through the provided address is to create a ssh tunnel. To do so, open another terminal window and execute\n\nssh -NL localhost:8888:&lt;node&gt;:8888 &lt;username&gt;@&lt;hpc-url&gt;\n\nwhere &lt;node&gt; should be replaced by the node to which the job submitted using srun (or fisbatch) was submitted to. This tunnel will keep the other terminal window busy to.\nFinally, copy the address provided by the notebook (e.g., ‚Äúhttp://127.0.0.1:8888/‚Äù) and paste it into your browser."
  },
  {
    "objectID": "posts/models/models.html",
    "href": "posts/models/models.html",
    "title": "All models are wrong",
    "section": "",
    "text": "George Box‚Äôs 1976 paper named ‚ÄúScience and Statistics‚Äù is famous for that ‚Äúall models are wrong‚Äù quote. Unfortunately, The number of people who know this quote is larger than those who have read the paper entirely.\nThe paper provides interesting (and insightful) discussions that still apply to the field of statistics nowadays. In my opinion, one of the most remarkable things Dr.¬†Box pointed out is about ‚ÄúMathematistry,‚Äù he defines (or explains) it as follows: &gt; ‚ÄúMathematistry is characterized by development of theory for theory‚Äôs sake, &gt; which since it seldom touches down with practice, has a tendency to redefine &gt; the problem rather than solve it.‚Äù\nLater on, in the same paper, he mentions how this ‚Äúmalady‚Äù (in his words) is harmful to statistics as a field. &gt; ‚ÄúAn even more serious consequence of mathematistry concerns the training of &gt; statisticians. We have recently been passing through a period where nothing &gt; very much was expected of the statistician. A great deal of research money &gt; was available and one had the curious situation where the highest objective of &gt; the teacher of statistics was to produce a student who would be another &gt; teacher of statistics. It was thus possible for successive generations of &gt; teachers to be produced with no practical knowledge of the subject whatever. &gt; Although statistics departments in universities are now commonplace there &gt; continues to be a severe shortage of statisticians competent to deal with real &gt; problems. But such are needed.‚Äù\nWe still face these challenges (on many others he mentioned in that paper) in Statistics. Therefore, it would be nice to share this to make people think about it."
  }
]