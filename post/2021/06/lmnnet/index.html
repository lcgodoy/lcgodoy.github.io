<!DOCTYPE html>
<html lang="en-us">

<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="generator" content="Source Themes Academic 4.5.0">

  

  
  
  
  
  
    
    
    
  
  

  <meta name="author" content="Lucas Godoy">

  
  
  
    
  
  <meta name="description" content="Despite the amount of “fancy” terms, Neural Network literature shares several
elements with the statistical literature. This post aims to provide a gentle
introduction of Neural Network Regression for statisticians. Additionally, we
elucidade how the famous Backpropagation algorithm is used to estimate the
parameters associated with a Neural Network model
">

  
  <link rel="alternate" hreflang="en-us" href="lcgodoy.github.io/post/2021/06/lmnnet/">

  


  
  
  
  <meta name="theme-color" content="rgb(218, 77, 77)">
  

  
  
  
  
    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css" integrity="sha256-uFVgMKfistnJAfoCUQigIl+JfUaP47GrRKjf6CTPVmw=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/all.min.css" integrity="sha256-+N4/V/SbAFiW1MPBCXnfnP9QSN3+Keu+NlB+0ev/YKQ=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.css" integrity="sha256-Vzbj7sDDS/woiFS3uNKo8eIuni59rjyNGtXfstRzStA=" crossorigin="anonymous">

    
    
    
      
    
    
      
      
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.10/styles/monokai-sublime.min.css" crossorigin="anonymous" title="hl-light">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.10/styles/monokai-sublime.min.css" crossorigin="anonymous" title="hl-dark" disabled>
      
    

    

    

  

  
  
  
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Montserrat:400,700%7CRoboto:400,400italic,700%7CRoboto+Mono&display=swap">
  

  
  
  
  
  <link rel="stylesheet" href="/lcgodoy.github.io/css/academic.css">

  




  


  

  <link rel="manifest" href="/lcgodoy.github.io/index.webmanifest">
  <link rel="icon" type="image/png" href="/lcgodoy.github.io/img/icon-32.png">
  <link rel="apple-touch-icon" type="image/png" href="/lcgodoy.github.io/img/icon-192.png">

  <link rel="canonical" href="lcgodoy.github.io/post/2021/06/lmnnet/">

  
  
  
  
    
    
  
  
  <meta property="twitter:card" content="summary">
  
  <meta property="twitter:site" content="@ldcgodoy">
  <meta property="twitter:creator" content="@ldcgodoy">
  
  <meta property="og:site_name" content="lcgodoy">
  <meta property="og:url" content="lcgodoy.github.io/post/2021/06/lmnnet/">
  <meta property="og:title" content="Estimating regression coefficients using a Neural Network (from scratch) | lcgodoy">
  <meta property="og:description" content="Despite the amount of “fancy” terms, Neural Network literature shares several
elements with the statistical literature. This post aims to provide a gentle
introduction of Neural Network Regression for statisticians. Additionally, we
elucidade how the famous Backpropagation algorithm is used to estimate the
parameters associated with a Neural Network model
"><meta property="og:image" content="lcgodoy.github.io/img/logo2.png">
  <meta property="twitter:image" content="lcgodoy.github.io/img/logo2.png"><meta property="og:locale" content="en-us">
  
    
      <meta property="article:published_time" content="2021-07-28T00:00:00&#43;00:00">
    
    <meta property="article:modified_time" content="2022-09-06T00:00:00&#43;00:00">
  

  


    






  






<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "lcgodoy.github.io/post/2021/06/lmnnet/"
  },
  "headline": "Estimating regression coefficients using a Neural Network (from scratch)",
  
  "datePublished": "2021-07-28T00:00:00Z",
  "dateModified": "2022-09-06T00:00:00Z",
  
  "author": {
    "@type": "Person",
    "name": "Lucas Godoy"
  },
  
  "publisher": {
    "@type": "Organization",
    "name": "lcgodoy",
    "logo": {
      "@type": "ImageObject",
      "url": "lcgodoy.github.io/img/logo2.png"
    }
  },
  "description": "Despite the amount of “fancy” terms, Neural Network literature shares several\nelements with the statistical literature. This post aims to provide a gentle\nintroduction of Neural Network Regression for statisticians. Additionally, we\nelucidade how the famous Backpropagation algorithm is used to estimate the\nparameters associated with a Neural Network model\n"
}
</script>

  

  


  

<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/cookieconsent2/3.0.3/cookieconsent.min.css">
<script src="https://cdnjs.cloudflare.com/ajax/libs/cookieconsent2/3.0.3/cookieconsent.min.js"></script>
<script>
  window.addEventListener("load", function(){
    window.cookieconsent.initialise({
      "palette": {
        "popup": {
          "background": "rgb(218, 77, 77)",
          "text": "#fff"
        },
        "button": {
          "background": "#fff",
          "text": "rgb(218, 77, 77)"
        }
      },
      "theme": "classic",
      "content": {
        "message": "This website uses cookies to ensure you get the best experience on our website.",
        "dismiss": "Got it!",
        "link": "Learn more",
        "href": "https://cookies.insites.com"
      }
    })});
</script>



  





  <title>Estimating regression coefficients using a Neural Network (from scratch) | lcgodoy</title>

</head>

<body id="top" data-spy="scroll" data-offset="70" data-target="#TableOfContents" class="dark">

  <aside class="search-results" id="search">
  <div class="container">
    <section class="search-header">

      <div class="row no-gutters justify-content-between mb-3">
        <div class="col-6">
          <h1>Search</h1>
        </div>
        <div class="col-6 col-search-close">
          <a class="js-search" href="#"><i class="fas fa-times-circle text-muted" aria-hidden="true"></i></a>
        </div>
      </div>

      <div id="search-box">
        
        
        
      </div>

    </section>
    <section class="section-search-results">

      <div id="search-hits">
        
      </div>

    </section>
  </div>
</aside>


  
<nav class="navbar navbar-light fixed-top navbar-expand-lg py-0 compensate-for-scrollbar" id="navbar-main">
  <div class="container">

    
      <a class="navbar-brand" href="/lcgodoy.github.io/"><img src="/lcgodoy.github.io/img/logo2.png" alt="lcgodoy"></a>
      
      <button type="button" class="navbar-toggler" data-toggle="collapse"
              data-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
        <span><i class="fas fa-bars"></i></span>
      </button>
      

    
    <div class="collapse navbar-collapse" id="navbar">

      
      
      <ul class="navbar-nav ml-auto">
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/lcgodoy.github.io/#about"><span>Home</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/lcgodoy.github.io/#posts"><span>Blog</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/lcgodoy.github.io/#projects"><span>Projects</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/lcgodoy.github.io/#talks"><span>Talks</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        

        <li class="nav-item">
          <a class="nav-link " href="/lcgodoy.github.io/files/cv.pdf"><span>CV</span></a>
        </li>

        
        

      

        

        

        

        
        <li class="nav-item">
          <a class="nav-link js-dark-toggle" href="#"><i class="fas fa-moon" aria-hidden="true"></i></a>
        </li>
        

      </ul>

    </div>
  </div>
</nav>


  <article class="article">

  












  

  
  
  
<div class="article-container pt-3">
  <h1>Estimating regression coefficients using a Neural Network (from scratch)</h1>

  

  
    


<div class="article-metadata">

  
  

  
  <span class="article-date">
    
    
      
          Last updated on
      
    
    Sep 6, 2022
  </span>
  

  

  
  <span class="middot-divider"></span>
  <span class="article-reading-time">
    21 min read
  </span>
  

  
  
  
  <span class="middot-divider"></span>
  <a href="lcgodoy.github.io/post/2021/06/lmnnet/#disqus_thread"></a>
  

  
  
  <span class="middot-divider"></span>
  <span class="article-categories">
    <i class="fas fa-folder mr-1"></i><a href="lcgodoy.github.io/categories/statistics/">Statistics</a>, <a href="lcgodoy.github.io/categories/neural-networks/">Neural Networks</a></span>
  

</div>

    














  
</div>



  <div class="article-container">

    <div class="article-style">
      



<div id="intro" class="section level2">
<h2>Intro</h2>
<p>The idea behind the Neural Networks models, as its nomenclature suggests, is to
mimic the way human brain learns to execute of some tasks. Some works in the
literature (<span class="citation">Cheng and Titterington (<a href="#ref-cheng1994neural" role="doc-biblioref">1994</a>)</span>, <span class="citation">Stern (<a href="#ref-stern1996neural" role="doc-biblioref">1996</a>)</span>, <span class="citation">Warner and Misra (<a href="#ref-warner1996understanding" role="doc-biblioref">1996</a>)</span>)
attribute of the first attempts to build a “Neural Network emulator” to
<span class="citation">McCulloch and Pitts (<a href="#ref-mcculloch1943logical" role="doc-biblioref">1943</a>)</span>. The popularity of this method in the past decades was
held down by the computation intensive calculations needed for such
procedures. However, the computation resources advances in the last few years
allied to the algorithmic nature of Neural Networks have contributed to the
adoption of the methodology by computer scientists. These days, this models are
very popular in the industry and are applied to several interesting applications
such as speech recognition, image classification, and automatic text
translation.</p>
</div>
<div id="neural-network-regression" class="section level2">
<h2>Neural Network Regression</h2>
<p>A neural network is a highly parametrized model that, provided we have enough
data, can approximate any functional relationship between a set of
<em>features</em><a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a> <span class="math inline">\(\mathbf{x}\)</span> and a response variable <span class="math inline">\(y\)</span> (<span class="citation">Efron and Hastie (<a href="#ref-efron2016computer" role="doc-biblioref">2016</a>)</span>, pages
151-152). Although there are several possible structures for neural networks,
for this post we are going to consider only the <em>feed-forward</em><a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a> neural
networks. In order to explain how these neural networks are designed, let’s
consider its graphical representation (see Figure <a href="#fig:nn1">1</a>). We have
vertices, which are called a units (or neurons), ordered horizontally by
layers. An edge coming from one vertex can only be connected to vertices
associated with “higher” layers. These connections represent a information flow
from left to right (hence, the name feed-forward), where each unit computed by
1) giving weights to each of its inputs, 2) calculating the dot product between
weights and inputs, 3) adding a constant( usually referred to as <em>bias</em>) to it,
and, finally, 4) applying an element-wise <em>activation</em> function <span class="math inline">\(f(\cdot)\)</span> to
it. These <em>activation functions</em> are used to establish non-linear relationships
between units.</p>
<p>The number of hidden layers as well as the number of units associated with every
layer can both be regard as tuning parameters. The design and architecture of a
neural network is a complex task. In summary, when having a single hidden layer,
the number of units associated with the hidden layer determines the number of
parameters associated with the model. <span class="citation">Efron and Hastie (<a href="#ref-efron2016computer" role="doc-biblioref">2016</a>)</span> suggest that, under
this scenario, it is better to consider several units for the hidden layer and
use some kind of regularization to avoid overfitting. Penalizations analogous to
the Ridge and Lasso penalty for linear models are often used in the
regularization context for neural networks (<span class="citation">Hastie, Tibshirani, and Wainwright (<a href="#ref-hastie2015statistical" role="doc-biblioref">2015</a>)</span>, pages
210-211).</p>
<p>An important remark regarding the neural network models is that they are “pure
prediction algorithms”. That is, these models are focused only on prediction,
neglecting the estimation, as pointed by <span class="citation">Efron (<a href="#ref-efron2020prediction" role="doc-biblioref">2020</a>)</span>. The strategy is
simple and consists in searching for high predictive accuracy. That being said,
these algorithms make no assumption on the probability distribution of the data
and, as one of the consequences of losing these assumptions, it is not possible
to make interval predictions or to calculate confidence intervals for the
“estimated” parameters.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:nn1"></span>
<img src="/lcgodoy.github.io/post/2021/06/2021-06-23-lmnnet_files/figure-html/nn1-1.png" alt="A _feed-forward_ neural network with a single hidden layer." width="90%" />
<p class="caption">
Figure 1: A <em>feed-forward</em> neural network with a single hidden layer.
</p>
</div>
<div id="subsec:single" class="section level3">
<h3>Single neuron feed-forward networks</h3>
<p>A single neuron feed-forward network does not possess any hidden layer in its
structure. The absence of hidden layers makes these models resemble the
statistical models we are most used to, like, for example, the linear regression
and logistic regression. By analyzing the graphical representation of a single
layer feed-forward network (Figure <a href="#fig:nn2">2</a>), it is easy to see that by
taking the identity as the <em>activation function</em>, the functional relationship
between <span class="math inline">\(\mathbf{x}\)</span> and <span class="math inline">\(y\)</span> considered by the neural network is equivalent to
the one used for the general linear model. Considering the same representation,
if we take <span class="math inline">\(f(x) = \textrm{logit}(x)\)</span> (<em>sigmoid</em> function, according to the
neural network models literature) and <span class="math inline">\(y \in \{ 0, 1 \}\)</span>, then the neural
network provides the same relationship between <span class="math inline">\(\mathbf{x}\)</span> and <span class="math inline">\(y\)</span> as the one
used by the logistic regression.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:nn2"></span>
<img src="/lcgodoy.github.io/post/2021/06/2021-06-23-lmnnet_files/figure-html/nn2-1.png" alt="A single layer _feed-forward_ neural network." width="90%" />
<p class="caption">
Figure 2: A single layer <em>feed-forward</em> neural network.
</p>
</div>
<p>Although the functional relationship between <span class="math inline">\(\mathbf{x}\)</span> and <span class="math inline">\(y\)</span> assumed by the
single layer neural network coincides with some statistical models, we cannot
promptly claim an equivalence between models because the way the neural networks
<em>learn</em>, that is estimates, the weights can lead to different solutions
depending on the <em>loss</em> and <em>cost</em> functions selected, we are going to talk more
about these functions in the next section.</p>
</div>
<div id="subsec:act" class="section level3">
<h3>Activation functions</h3>
<p>Activation functions are applied in every “Layer connection” in neural network
models. Suppose, for example, we have a design matrix <span class="math inline">\(\mathbf{X} \in {\rm I\!R}^{n \times p}\)</span>, and a response variable <span class="math inline">\(\mathbf{y}\)</span>. Then, given
appropriate choices of the <span class="math inline">\(K - 1\)</span> (one for each layer connection), the
mathematical model, for a single observation, behind the neural network, can be
written in a vectorial notation as follows</p>
<p><span class="math display">\[\begin{align}
\label{eq:lin-term}
\mathbf{z}^{(k)} &amp; = \mathbf{W}^{(k - 1)} \mathbf{a}^{(k - 1)} \\
\mathbf{a}^{(k)} &amp; = f_{(k)} \left( \mathbf{z}^{(k)} \right),
\label{eq:vec-units}
\end{align}\]</span>
where <span class="math inline">\(\mathbf{W}^{(k - 1)} \in {\rm I\!R}^{m_{k - 1} \times m_{k}}\)</span> is the
matrix of weights that go from from the layer <span class="math inline">\(L_{k - 1}\)</span> to the layer <span class="math inline">\(L_{k}\)</span>,
<span class="math inline">\(\mathbf{a}^{(k)} \in {\rm I\!R}^{m_k \times m_{k + 1}}\)</span> matrix of units at
layer <span class="math inline">\(L_k\)</span>, and <span class="math inline">\(f_{(k)}\)</span> is a (element-wise) activation function used at the
layer <span class="math inline">\(L_k\)</span>. Note that, when <span class="math inline">\(k = 0\)</span>, then <span class="math inline">\(\mathbf{a}^{(0)} = \mathbf{X}\)</span>.
Observe that <span class="math inline">\(m_k\)</span> is the number of units in the layer <span class="math inline">\(k\)</span> and, consequently,
for the input and output layers, respectively, we have <span class="math inline">\(m_0 = p\)</span> and <span class="math inline">\(m_K = 1\)</span>.</p>
<p>From this example, it is clear that we can apply different activation functions
when connecting different layers. Nevertheless, the activation for one layer is
the same along all of its units.</p>
<p>Although, theoretically, there exists no restriction on which functions to use
as activation function, we want these functions to be at least one time
differentiable. This is due to the fact that most of the methods used to find
the optimal weights are based on gradients. Another aspect to be considered when
choosing an activation function is the domain of the output variable <span class="math inline">\(y\)</span>. That
is, if <span class="math inline">\(y \in [0, 1]\)</span>, we want an activation function that maps real values to
the <span class="math inline">\([0, 1]\)</span> interval. In summary, for the output layer, we use a activation
function that makes predictions on the same domain as the output variable,
while, for hidden layers, we have no restrictions on the activation functions,
besides the ones already mentioned.</p>
<p>Some commonly used link functions are the <span class="math inline">\(\textrm{logit}\)</span>, or sigmoid,
function, defined as
<span class="math display">\[
f(x) = \frac{1}{1 + e^{-x}},
\]</span>
the hyperbolic tangent function, referred to as <span class="math inline">\(\textrm{tanh}\)</span>,
<span class="math display">\[
f(x) = \frac{e^z - e^{-z}}{e^{z} + e^{-z}}.
\]</span>
Note that the <span class="math inline">\(\textrm{tanh}\)</span> is mapping from the real line to the <span class="math inline">\((-1, 1)\)</span>
interval.
The Rectified Linear Unit (ReLU) is also a popular choice and is defined as
<span class="math display">\[
f(x) = x_{+} = \max(0, x),
\]</span>
the main advantage of this function is a cheap to compute gradient. A different
version of the ReLU called leaky ReLU is also quite popular, its definition is
given as follows
<span class="math display">\[
f(x) = x_{+} = \max(.01 * x, x),
\]</span></p>
<p>These are only some examples of commonly used activation functions and they are
illustrated in Figure <a href="#fig:act-funs">3</a>. The user does need to be restrict to
these options since there are several other functions implemented in the
software available to compute neural networks. However, if you want to use a
activation function that is not implemented yet, you may have to implement your
own version for the algorithm.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:act-funs"></span>
<img src="/lcgodoy.github.io/post/2021/06/2021-06-23-lmnnet_files/figure-html/act-funs-1.png" alt="The most popular activation functions (figure inspired by Figure 18.6 from @efron2016computer )." width="90%" />
<p class="caption">
Figure 3: The most popular activation functions (figure inspired by Figure 18.6 from <span class="citation">Efron and Hastie (<a href="#ref-efron2016computer" role="doc-biblioref">2016</a>)</span> ).
</p>
</div>
<p>Although there are no restrictions on the functions used as activation functions
in the hidden layers (besides being differentiable functions), it is not
advisable to use the identity function because it implies a waste of
computational power. This is due to the fact that using a linear function in a
hidden layer, makes the units from that layer a linear combination of the units
from the previous layer. To make this clear, let’s prove that a Neural Network
model with a single hidden layer collapses to a Generalized Linear Model when
the identity function is used as the activation function.</p>
<p>Suppose a <span class="math inline">\(n\)</span>-dimensional vector <span class="math inline">\(\mathbf{y}\)</span> is assumed to follow a
distribution <span class="math inline">\(\mathcal{P}\)</span>, where <span class="math inline">\(\mathcal{P}\)</span> belongs to the exponential
family of distributions. Then, given a design matrix <span class="math inline">\(\mathbf{X} \in {\rm I\!R}^{n \times p}\)</span>, the Generalized Linear Model for <span class="math inline">\(\mathbf{y}\)</span> is
composed by the <em>random component</em>, given by the probability density function
associated with the distribution <span class="math inline">\(\mathcal{P}\)</span>, the <em>systematic component</em>,
defined by
<span class="math display">\[\begin{equation}
\label{eq:syst-comp}
\boldsymbol{\eta} = \mathbf{X} \boldsymbol{\beta},
\end{equation}\]</span>
and a (canonical) link function <span class="math inline">\(g(\cdot)\)</span> such that
<span class="math display">\[\begin{equation}
\label{eq:link-f}
\boldsymbol{\mu} = g(\boldsymbol{\eta}).
\end{equation}\]</span>
Once we estimate the parameters <span class="math inline">\(\boldsymbol{\beta}\)</span>, we have
<span class="math display">\[\begin{equation}
\label{eq:pred-glm}
\hat{\mathbf{y}} = g( \mathbf{X} \hat{\boldsymbol{\beta}} ).
\end{equation}\]</span></p>
<p>Define now our Neural Network model having a single hidden layer with <span class="math inline">\(M\)</span>
units. The activation function for the hidden layer is <span class="math inline">\(f_h(x) = g(x)\)</span>, that is,
the same as the identity function. The weights we want to find are
<span class="math inline">\(\mathbf{W}^{(1)} \in {\rm I\!R}^{p \times 1}\)</span>, and <span class="math inline">\(\mathbf{W}^{(2)} \in {\rm I\!R}^{M \times n}\)</span>. The activation function for the activation layer is
the previously mentioned canonical link function. Finally, let the loss be the
deviance residual associated with the distribution <span class="math inline">\(\mathcal{P}\)</span>, and the cost
function be the average of the losses. Then, the mathematical representation of
the Neural Network becomes
<span class="math display">\[\begin{equation}
\label{eq:example-glm-nn-linear}
\mathbf{z}^{(1)} = \mathbf{X} \mathbf{W}^{(1)} = \mathbf{a}^{(1)},
\end{equation}\]</span>
because the activation function for the hidden layer is the identity.
Then, we have
<span class="math display">\[\begin{align}
\label{eq:example-glm-nn-hidden}
&amp; \mathbf{z}^{(2)} = \mathbf{a}^{(1)} \mathbf{W}^{(2)} \\
&amp; \mathbf{y} = \mathbf{a}^{(2)} = g( \mathbf{\mathbf{z}^{(2)}} ).
\label{eq:example-glm-nn-out}
\end{align}\]</span>
However, note that, by combining ,
, and  we get
<span class="math display">\[\begin{align*}
\mathbf{y} &amp; = g( \mathbf{\mathbf{z}^{(2)}} ) \\
&amp; = g( \mathbf{a}^{(1)} \mathbf{W}^{(2)} ) \\
&amp; = g( \mathbf{X} \underbrace{\mathbf{W}^{(1)} \mathbf{W}^{(2)}}_{{\rm I\!R}_{p
\times 1}} ),
\end{align*}\]</span>
which yields to optimal weights (see <a href="#subsec:fit-nn">Fitting a Neural Network</a>
and <a href="#subsec:backpro">Backpropagation</a>, for more information on how to fit a
neural network model) satisfying
<span class="math display">\[
\underbrace{\mathbf{W}^{(1)} \mathbf{W}^{(2)}}_{{\rm I\!R}_{p
\times 1}} = \hat{\boldsymbol{\beta}},
\]</span>
where <span class="math inline">\(\hat{\boldsymbol{\beta}}\)</span> is the Maximum Likelihood Estimator for <span class="math inline">\(\boldsymbol{\beta}\)</span>
that can be obtained using the Iterative Reweighted Least Squares for the model
defined by the probability density function associated with the distribution
<span class="math inline">\(\mathcal{P}\)</span>, the systematic component  and a (canonical)
link function .</p>
</div>
<div id="cost-functions" class="section level3">
<h3>Cost functions</h3>
<p>Whenever we want to fit a neural network to a dataset we need to specify a
<em>Cost</em> function, which is usually based on <em>loss</em> functions. A loss function, in
the context of Neural Network models, measures how far our predictions
<span class="math inline">\(f(\mathbf{x}; \mathbf{W})\)</span> are from the true value <span class="math inline">\(y\)</span>. Examples of commonly
used loss functions, for a single observation, are the mean square error loss
and the binomial deviance defined, respectively, as
<span class="math display">\[\begin{equation}
\label{eq:loss-mse}
L(\mathbf{w}, \mathbf{x}; y) = \frac{1}{2} (f(\mathbf{x}; \mathbf{w}) - y)^{2},
\end{equation}\]</span>
and
<span class="math display">\[\begin{equation}
\label{eq:loss-bdev}
L(\mathbf{W}, \mathbf{x}; y) = y \log \left( \frac{y}{f(\mathbf{x}, \mathbf{w})}
\right) + 
(1 - y) \log \left( \frac{1 - y}{1 - f(\mathbf{x}, \mathbf{w})} \right).
\end{equation}\]</span>
The loss function  is usually employed when the output
(response) variable assumes continuous values, while the  is
used for binary output variables.</p>
<p>After choosing an appropriate loss function, the cost function is defined as the
average of the loss function over all the observation, that is
<span class="math display">\[\begin{equation}
\label{eq:cost-fun}
C(\mathbf{y}; \mathbf{x}, \mathbf{W}) = \frac{1}{n} \sum_{i = 1}^{n}
L(\mathbf{w_i, \mathbf{x}_i; y_i}) + \lambda J(\mathbf{W}),
\end{equation}\]</span>
where <span class="math inline">\(J(\mathbf{W})\)</span> is a non-negative regularization term and <span class="math inline">\(\lambda \geq 0\)</span> is a tuning parameter.</p>
<p>In practice, we may have a regularization term for each layer, each one having
its own <span class="math inline">\(\lambda\)</span>. Some commonly used regularization terms are
<span class="math display">\[
J(\mathbf{W}) = \frac{1}{2} \sum_{k = 1}^{K - 1} \lVert \mathbf{w}^{(k)} \rVert^2,
\]</span>
and
<span class="math display">\[
J(\mathbf{W}) = \frac{1}{2} \sum_{k = 1}^{K - 1} \lVert \mathbf{w}^{(k)} \rVert,
\]</span>
where <span class="math inline">\(K\)</span> is the number of layers of our neural network model, and
<span class="math inline">\(\mathbf{w}^{(k)}\)</span> is the vector of weights from the units in the layer <span class="math inline">\(L_k\)</span> to
the layer <span class="math inline">\(L_{k + 1}\)</span>. Note that, these two regularization terms are analogous
to the Ridge and Lasso penalizations, and they play the exact same role in
neural networks as its analogous versions do for the linear models
<span class="citation">(Efron and Hastie <a href="#ref-efron2016computer" role="doc-biblioref">2016</a>)</span>. Mixtures of these two regularization terms, as in the
elastic net <span class="citation">(Zou and Hastie <a href="#ref-zou2005regularization" role="doc-biblioref">2005</a>)</span>, are also common.</p>
</div>
<div id="subsec:fit-nn" class="section level3">
<h3>Fitting a Neural Network</h3>
<p>Supposing a user has set the number of layers, units, an activation function and
a loss function, to fit a neural network we seek the set of weights <span class="math inline">\(\mathbf{W} = \{ \mathbf{W}^{(1)}, \ldots, \mathbf{W}^{(k - 1)} \}\)</span> such that the cost
function is minimized, that is
<span class="math display">\[ 
\min_{\mathbf{W}} \left \{ \mathcal{C}(\mathbf{y}; \mathbf{X}, \mathbf{W}) \right \}.
\]</span>
Therefore, the neural network fit has turned into an optimization problem. The
most common algorithm used to solve this optimization problem is the
Backpropagation algorithm, which is described in the next section for a general
situation.</p>
</div>
<div id="subsec:backpro" class="section level3">
<h3>Backpropagation</h3>
<p>Backpropagation (or gradient descent) is the method used to find the weights
which minimize the chosen cost and loss functions for a given neural network. It
is an iterative algorithm that is guaranteed to converge whenever the cost
function has a single local minima <span class="citation">(Efron and Hastie <a href="#ref-efron2016computer" role="doc-biblioref">2016</a>)</span>. However, even if the
cost function does not have a single local minima, the algorithm works fairly
well.The updates for a weight matrix, <span class="math inline">\(\mathbf{W}^{(k)}\)</span> let’s say, is done as
follows
<span class="math display">\[\begin{equation}
\label{eq:iter}
\mathbf{W}^{(k)} = \mathbf{W}^{(k)} - \alpha \frac{\partial
\mathcal{C}(\mathbf{y}; \mathbf{X}, \mathbf{W})}{\partial \mathbf{W}^{(k)}},
\end{equation}\]</span>
where <span class="math inline">\(\alpha\)</span> is a tuning parameter called <em>learning rate</em>. The name
backpropagation comes from the fact that the derivatives (or gradients) are
computed according to something called a <em>computation graph</em> in a backward
fashion. It is heavily based on the chain rule for differentiation.</p>
<p>Given initial values for the <span class="math inline">\(\mathbf{W}\)</span> matrices, the method repeats the
update rule  until convergence. Provided that the columns of the
design matrix are rescaled to mean 0 and variance 1, <span class="citation">Hastie, Tibshirani, and Friedman (<a href="#ref-hastie2009elements" role="doc-biblioref">2009</a>)</span> suggest
the use of random starting values for the weights as uniform random variables on
the interval <span class="math inline">\([-.75, .75]\)</span>.</p>
</div>
</div>
<div id="sec:imple" class="section level2">
<h2>Implementation</h2>
<p>I created functions for the implementation of a Neural Network with a single
hidden layer model for generic activation functions. The implementation
considers the cost function defined as
<span class="math display">\[
C(\mathbf{y}; \mathbf{X}, \mathbf{Y}) = \frac{1}{n} \lVert \mathbf{y} -
\hat{\mathbf{y}} \rVert^2.
\]</span></p>
<p>The inputs for the implemented function are:</p>
<ul>
<li><p>A design matrix <span class="math inline">\(\mathbf{X}\)</span>, including the columns of ones for the intercept;</p></li>
<li><p>A column vector <span class="math inline">\(\mathbf{y}\)</span> containing the response variable;</p></li>
<li><p>The number of units for the hidden layer;</p></li>
<li><p>The activation function for the hidden layer;</p></li>
<li><p>The activation function for the output layer;</p></li>
<li><p>A scalar for the learning rate <span class="math inline">\(\alpha\)</span>;</p></li>
<li><p>Two control parameters for the convergence of the algorithm. The maximum
number of iterations allowed, and a relative error <span class="math inline">\(\epsilon\)</span> which controls
when to stop the iteration algorithm.</p></li>
</ul>
<p>The function returns a <code>list</code> of size 5. Its first element is the predicted
vector for <span class="math inline">\(\mathbf{y}\)</span>, the second contains the values of the cost function for
each iteration of the algorithm. The third position of this <code>list</code> stores the
weight matrices <span class="math inline">\(\mathbf{W}^{(1)}\)</span> and <span class="math inline">\(\mathbf{W}^{(2)}\)</span>, while the last two
positions store the number of iterations until attain the convergence and a
string indicating whether the algorithm converged or not, respectively.</p>
<p>See below the implementation of some activation functions (and their
derivatives)</p>
<pre class="r"><code>##--- activation functions and their derivatives ----

## ReLU
relu &lt;- function(x) {
    pmax(x, 0)
}

## derivative leaky ReLU
d_relu &lt;- function(x) {
    ifelse(x &gt; 0, 1, 0)
}

## leaky ReLU
lrelu &lt;- function(x) {
    pmax(x * .01, x)
}

## derivative leaky ReLU
d_lrelu &lt;- function(x) {
    ifelse(x &gt; 0, 1, .01)
}

## derivative tanh
d_tanh &lt;- function(x) {
    1 - (tanh(x)^2)
}

## derivative logit
d_logit &lt;- function(x) {
    plogis(x) * ( 1 - plogis(x) )
}

## derivative identity
d_ident &lt;- function(x) {
    pmax( -2 * abs(x), 1 )
}</code></pre>
<p>Now, let’s implement some helper functions to fit our neural network
models. First, the cost function used in our examples is given by</p>
<pre class="r"><code>## cost function
cost_f &lt;- function(y, yhat) {
    crossprod(yhat - y) / NROW(y)
}</code></pre>
<p>The implementation of the functions that will need to be executed at each step
of the optimization algorithm are defined below. <code>compute_nn</code> computes the
hidden layers given the matrix of covariates (or features) <code>X</code>, the list
containing the the weights <code>W</code> associated to each layer connection, and two
activation functions <code>act_hidden</code> and <code>act_out</code> for the hidden and output
layers, respectively (this is a the implementation for a 2 layers network). The
<code>compute_grad</code> function computes the gradient and needs some further information
like <code>y</code> (the response variable), <code>n</code> the sample size, and the derivatives of
the activation functions. <code>update_aux</code> and <code>update_w</code> are helper functions used
to update the weights.</p>
<pre class="r"><code>##--- functiosn to fit the neural network ----

## computing the forward step of the neural network
compute_nn &lt;- function(X, W, act_hidden, act_out) {
    Z &lt;- vector(mode = &quot;list&quot;, length = 2)
    
    Z[[1]] &lt;- X %*% W[[1]]
    A &lt;- act_hidden(Z[[1]])

    Z[[2]] &lt;- A %*% W[[2]]

    return( list(y = act_out(Z[[2]]),
                 z = Z) )
}

## computing the gradient of the neural network
compute_grad &lt;- function(y, X, W, act_hidden, act_out,
                         d1_hidden, d1_out, n) {
    nn    &lt;- compute_nn(X, W, act_hidden, act_out)
    aux_out &lt;- (nn$y - y) * d1_out(nn$z[[2]])
    aux_hid &lt;- tcrossprod(aux_out, W[[2]]) *
        d1_hidden(nn$z[[1]])
    
    return(
        list(crossprod(X, aux_hid) / n,
             crossprod(act_hidden(nn$z[[1]]), aux_out) / n)
    )
}

## aux function for updating W
update_aux &lt;- function(w, dw, alpha) {
    w - alpha * dw
}

## update the weights of a neural network
update_w &lt;- function(W, alpha, y, X, act_hidden, act_out,
                     d1_hidden, d1_out, n) {

    grad_w &lt;- compute_grad(y, X, W, act_hidden, act_out,
                           d1_hidden, d1_out, n)
    
    return( Map(f = update_aux, w = W,
                dw = grad_w, alpha = alpha) )
}</code></pre>
<p>Finally, all these functions previously describer are used to build the <code>fit_nn</code>
function (which is used to compute the optimal weights for the neural
network). The <code>alpha</code> is the <span class="math inline">\(\alpha\)</span> previously mentioned in this post, <code>maxit</code>
and <code>eps</code> are parameters used in the optimization process. The first one stands
for the maximum number of iterations to be used in the optimization process,
while the second stand for the “optimization error”. That is, if, from one
iteration to another, the change between the weights does not exceed <code>eps</code>, then
we consider that the algorithm converged and a (maybe local) optimum has been
found.</p>
<pre class="r"><code>fit_nn &lt;- function(y, X, hid_units,
                   act_hidden, act_out,
                   d1_hidden, d1_out,
                   alpha = .25,
                   maxit = 500L,
                   eps   = 1e-05) {
    m &lt;- hid_units
    p &lt;- ncol(X)
    N &lt;- NROW(y)

    
    W &lt;- list(matrix(runif(m * p, -.75, .75),
                     ncol = m, nrow = p),
              matrix(runif(m, -.75, .75), ncol = 1))

    nn   &lt;- vector(mode = &quot;list&quot;, length = maxit)
    cost &lt;- vector(mode = &quot;numeric&quot;, length = maxit)

    ## initialiazing
    nn[[1]] &lt;- compute_nn(X, W, act_hidden, act_out)

    cost[1] &lt;- cost_f(y, nn[[1]]$y)
    
    for( i in seq_len(maxit)[-1] ) {
        W &lt;- update_w(W, alpha, y, X,
                      act_hidden, act_out,
                      d1_hidden, d1_out,
                      n = N)
        
        nn[[i]] &lt;- compute_nn(X, W, act_hidden, act_out)
        cost[i] &lt;- cost_f(y, nn[[i]]$y)
        
        if( abs(cost[i] - cost[i - 1]) &lt; eps ) {
            output &lt;- list(nn   = nn[[i - 1]],
                           cost = cost[1:(i - 1)],
                           W    = W,
                           it   = (i - 1),
                           conv = &quot;yes&quot;)
            break
        }
    }

    if( i == maxit ) {
        output &lt;- list(yhat = nn[[maxit]]$y,
                       cost = cost[1:maxit],
                       W    = W,
                       it   = maxit,
                       conv = &quot;no&quot;)
    }

    return(output)
}</code></pre>
<p>Having all these functions, we can play with some numerical examples!</p>
</div>
<div id="sec:ne" class="section level2">
<h2>Numerical Examples</h2>
<div id="example-1-equivalence-between-neural-network-and-linear-model" class="section level3">
<h3>Example 1: Equivalence between Neural Network and Linear Model</h3>
<p>Consider a simulated dataset where
<span class="math display">\[
\mathbf{y} \sim N(\mathbf{X} \boldsymbol{\beta}, \sigma^2 \mathbf{I}_n),
\]</span>
where <span class="math inline">\(\mathbf{X} \in {\rm I\!R}^{n \times 3}\)</span>, with the first column being the
intercept term. To simulate the model we used <span class="math inline">\(\boldsymbol{\beta} = (2, 3, 1.5)\)</span>. Additionally, suppose <span class="math inline">\(n = 2000\)</span>.</p>
<p>Considering the identity function as the activation function for both layers,
the goal here is to show that the <span class="math inline">\(\mathbf{W}^{(1)} \mathbf{W}^{(2)} = \hat{\boldsymbol{\beta}}\)</span>, where <span class="math inline">\(\hat{\boldsymbol{\beta}}\)</span> is the least squares solution for a
linear model established as <span class="math inline">\(\mathbf{y} = \mathbf{X} \boldsymbol{\beta}\)</span>, and
<span class="math inline">\(\mathbf{W}^{(1)}, \, \mathbf{W}^{(2)}\)</span> are the optimal weights according to the
Neural Network fitted to the data, as proved in the subsection
<a href="#subsec:act"><strong>??</strong></a>.</p>
<p>Table <a href="#tab:tbl-01"><strong>??</strong></a> displays the results from the simulated example. The two
different approaches have yielded the exactly same results. If we were to make
predictions, the two methods would provide the same predicted values under these
circumstances.</p>

<p>See below the code used on this example.</p>
<pre class="r"><code>##--- numerical examples ----

##--- example 1 ----

set.seed(123)

n &lt;- 2000

x1 &lt;- rnorm(n)
x2 &lt;- as.numeric( scale( rexp(n) ) )

y &lt;- 3 + 3 * x1 + 1.5 * x2 + rnorm(n, sd = .5)

my_x &lt;- cbind( rep(1, n), x1, x2 )
colnames(my_x) &lt;- NULL

dt &lt;- as.data.frame( cbind(y, my_x[, 2:3]) )
names(dt) &lt;- c(&quot;y&quot;, &quot;x1&quot;, &quot;x2&quot;)

m &lt;- 6

fit_1 &lt;-
    fit_nn(y = y, X = my_x,
           hid_units = m,
           act_hidden = identity,
           act_out    = identity,
           d1_hidden  = d_ident,
           d1_out     = d_ident,
           alpha = .05,
           maxit = 1000L,
           eps   = 1e-16)

beta_hat &lt;- coef(lm(y ~ x1 + x2, data = dt))

tbl_1 &lt;- as.data.frame(cbind(beta_hat,
                             fit_1$W[[1]] %*% fit_1$W[[2]]))
names(tbl_1) &lt;- c(&quot;$\\hat{\\boldsymbol{\\beta}}$&quot;,
                  &quot;$\\mathbf{W}^{(1)} \\mathbf{W}^{(2)}$&quot;)
rownames(tbl_1) &lt;- NULL</code></pre>
</div>
<div id="example-2-nonlinear-relationship-and-number-of-hidden-units" class="section level3">
<h3>Example 2: Nonlinear relationship and number of hidden units</h3>
<p>Consider now the following model
<span class="math display">\[
y_i = \beta_0 + \beta_1 (x^2) + \varepsilon_i.
\]</span></p>
In practice, we do not know before-hand the relationship between the response
and explanatory variables is not linear. In Figure <a href="#fig:fit-nn2">4</a> we show the
fitted curves the linear model and for neural networks under different settings
for a dataset simulated from this example. The Neural Network deals nicely with
the nonlinearity at the cost of possibly overfit the data.
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:fit-nn2"></span>
<img src="/lcgodoy.github.io/post/2021/06/2021-06-23-lmnnet_files/figure-html/fit-nn2-1.png" alt="Different models fitted to the same simulated dataset." width="90%" />
<p class="caption">
Figure 4: Different models fitted to the same simulated dataset.
</p>
</div>
<p>See the code used on this example below.</p>
<pre class="r"><code>##--- example 2 ----

set.seed(124)

x12 &lt;- rnorm(n)

y2 &lt;- 5 - 2.5 * (x12^2) + rnorm(n, sd = .5)

my_x2 &lt;- cbind(rep(1, n), x12)
colnames(my_x2) &lt;- NULL

dt2 &lt;- as.data.frame( cbind(y2, my_x2[, 2]) )
names(dt2) &lt;- c(&quot;y&quot;, &quot;x1&quot;)

n_pred &lt;- 4000

## fitting linear model

my_lm2 &lt;- lm(y ~ x1, data = dt2)

## fitting neural network with tanh as the activation function for the hidden
## layer - 5 hidden units
fit_2 &lt;-
    fit_nn(y = y2, X = my_x2,
           hid_units  = 5,
           act_hidden = tanh,
           act_out    = identity,
           d1_hidden  = d_tanh,
           d1_out     = d_ident,
           alpha = .05,
           maxit = 1000L,
           eps   = 1e-04)

## fitting neural network with tanh as the activation function for the hidden
## layer - 15 hidden units
fit_3 &lt;-
    fit_nn(y = y2, X = my_x2,
           hid_units  = 15,
           act_hidden = tanh,
           act_out    = identity,
           d1_hidden  = d_tanh,
           d1_out     = d_ident,
           alpha = .05,
           maxit = 1000L,
           eps   = 1e-04)

## fitting neural network with leaky ReLU as the activation function for the
## hidden layer - 10 hidden units
fit_4 &lt;-
    fit_nn(y = y2, X = my_x2,
           hid_units  = 10,
           act_hidden = lrelu,
           act_out    = identity,
           d1_hidden  = d_lrelu,
           d1_out     = d_ident,
           alpha = .05,
           maxit = 1000L,
           eps   = 1e-04)

pred_data &lt;- data.frame(x = seq(from = min(x12), to = max(x12),
                                length.out = n_pred))

pred_data &lt;- transform(pred_data,
                       lm = coef(my_lm2)[[1]] + coef(my_lm2)[[1]] * x)

pred_data &lt;- transform(pred_data,
                       nn_tanh_1 =
                           compute_nn(X = cbind(rep(1, 4000),
                                                x),
                                      W = fit_2$W,
                                      act_hidden = tanh,
                                      act_out = identity)$y)

pred_data &lt;- transform(pred_data,
                       nn_tanh_2 =
                           compute_nn(X = cbind(rep(1, 4000),
                                                x),
                                      W = fit_3$W,
                                      act_hidden = tanh,
                                      act_out = identity)$y)

pred_data &lt;- transform(pred_data,
                       nn_lrelu =
                           compute_nn(X = cbind(rep(1, 4000),
                                                x),
                                      W = fit_4$W,
                                      act_hidden = lrelu,
                                      act_out = identity)$y)

setDT(pred_data)

pred_data &lt;- melt(pred_data, id = 1,
                  value.name = &quot;pred&quot;,
                  variable.name = &quot;method&quot;)

pred_data[, method := fcase(method == &quot;nn_tanh_1&quot;, &quot;tanh - 5&quot;,
                            method == &quot;nn_tanh_2&quot;, &quot;tanh - 15&quot;,
                            method == &quot;nn_lrelu&quot;, &quot;leaky ReLU - 10&quot;,
                            default = &quot;lm&quot;)]

ggplot(data = pred_data) +
    geom_point(data = dt2, aes(x = x1, y = y),
               alpha = .5) +
    geom_line(aes(x = x, y = pred, color = method),
              lwd = 1.05) +
    scale_color_discrete(name = NULL) +
    theme_bw() +
    theme(
        legend.position = &quot;bottom&quot;,
        legend.margin = margin(6, 6, 6, 6)
    ) +
    labs(x = &quot;X&quot;, y = &quot;Y&quot;)</code></pre>
</div>
</div>
<div id="final-thoughts" class="section level2">
<h2>Final Thoughts</h2>
<p>The Neural Network Regression models are very interesting but certainly are not
magical as it is sold in the market. By the end of the day, these models consist
of simple linear algebra allied to the use of element-wise nonlinear functions
and optimization algorithms. Speaking on optimization algorithm, the gradient
descent looks like a fixed-point iteration algorithm. These kind of algorithms
have the advantage of not need the second derivative of the functions, however
their convergence can be slow. I believe that using different learning rates for
different parameters could improve the speed on which the algorithm converges.</p>
<p>Although these models do not make any distributional assumption on the data, we
can easily make it more suitable for certain distributions by working with the
cost and activation functions on an appropriate fashion.</p>
<p>There are several variants of these models suited for different problems, like
text and image classification, for example. The idea is the same, what changes
is the way the researchers deal with the hidden layers. I think an interesting
application is to try to use neural networks to estimate non-parametrically
covariance matrices for spatial data.</p>
</div>
<div id="references" class="section level1 unnumbered">
<h1>References</h1>
<div id="refs" class="references">
<div id="ref-cheng1994neural">
<p>Cheng, Bing, and D Michael Titterington. 1994. “Neural Networks: A Review from a Statistical Perspective.” <em>Statistical Science</em>, 2–30.</p>
</div>
<div id="ref-efron2020prediction">
<p>Efron, Bradley. 2020. “Prediction, Estimation, and Attribution.” <em>Journal of the American Statistical Association</em> 115 (530): 636–55.</p>
</div>
<div id="ref-efron2016computer">
<p>Efron, Bradley, and Trevor Hastie. 2016. “Neural Networks and Deep Learning.” In <em>Computer Age Statistical Inference: Algorithms, Evidence, and Data Science</em>. Institute of Mathematical Statistics Monographs. Cambridge University Press. <a href="https://doi.org/10.1017/CBO9781316576533.019">https://doi.org/10.1017/CBO9781316576533.019</a>.</p>
</div>
<div id="ref-hastie2009elements">
<p>Hastie, Trevor, Robert Tibshirani, and Jerome Friedman. 2009. <em>The Elements of Statistical Learning: Data Mining, Inference, and Prediction</em>. Springer Science &amp; Business Media.</p>
</div>
<div id="ref-hastie2015statistical">
<p>Hastie, Trevor, Robert Tibshirani, and Martin Wainwright. 2015. <em>Statistical Learning with Sparsity: The Lasso and Generalizations</em>. CRC press.</p>
</div>
<div id="ref-mcculloch1943logical">
<p>McCulloch, W, and W Pitts. 1943. “A Logical Calculus of the Ideas Imminent in Nervous Activity.” <em>Bulletin of Mathematical Biophisics</em> 5: 115–33.</p>
</div>
<div id="ref-stern1996neural">
<p>Stern, Hal S. 1996. “Neural Networks in Applied Statistics.” <em>Technometrics</em> 38 (3): 205–14.</p>
</div>
<div id="ref-warner1996understanding">
<p>Warner, Brad, and Manavendra Misra. 1996. “Understanding Neural Networks as Statistical Tools.” <em>The American Statistician</em> 50 (4): 284–93.</p>
</div>
<div id="ref-zou2005regularization">
<p>Zou, Hui, and Trevor Hastie. 2005. “Regularization and Variable Selection via the Elastic Net.” <em>Journal of the Royal Statistical Society: Series B (Statistical Methodology)</em> 67 (2): 301–20.</p>
</div>
</div>
</div>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>Features are the name given for predictors in the neural networks
literature<a href="#fnref1" class="footnote-back">↩︎</a></p></li>
<li id="fn2"><p>Sometimes
referred to as <em>multi-layer-perceptron</em>, and <em>back-propagation</em>.<a href="#fnref2" class="footnote-back">↩︎</a></p></li>
</ol>
</div>

    </div>

    







<div class="share-box" aria-hidden="true">
  <ul class="share">
    
      
      
      
        
      
      
      
      <li>
        <a href="https://twitter.com/intent/tweet?url=lcgodoy.github.io/post/2021/06/lmnnet/&amp;text=Estimating%20regression%20coefficients%20using%20a%20Neural%20Network%20%28from%20scratch%29" target="_blank" rel="noopener" class="share-btn-twitter">
          <i class="fab fa-twitter"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://www.facebook.com/sharer.php?u=lcgodoy.github.io/post/2021/06/lmnnet/&amp;t=Estimating%20regression%20coefficients%20using%20a%20Neural%20Network%20%28from%20scratch%29" target="_blank" rel="noopener" class="share-btn-facebook">
          <i class="fab fa-facebook"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="mailto:?subject=Estimating%20regression%20coefficients%20using%20a%20Neural%20Network%20%28from%20scratch%29&amp;body=lcgodoy.github.io/post/2021/06/lmnnet/" target="_blank" rel="noopener" class="share-btn-email">
          <i class="fas fa-envelope"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://www.linkedin.com/shareArticle?url=lcgodoy.github.io/post/2021/06/lmnnet/&amp;title=Estimating%20regression%20coefficients%20using%20a%20Neural%20Network%20%28from%20scratch%29" target="_blank" rel="noopener" class="share-btn-linkedin">
          <i class="fab fa-linkedin-in"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://web.whatsapp.com/send?text=Estimating%20regression%20coefficients%20using%20a%20Neural%20Network%20%28from%20scratch%29%20lcgodoy.github.io/post/2021/06/lmnnet/" target="_blank" rel="noopener" class="share-btn-whatsapp">
          <i class="fab fa-whatsapp"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://service.weibo.com/share/share.php?url=lcgodoy.github.io/post/2021/06/lmnnet/&amp;title=Estimating%20regression%20coefficients%20using%20a%20Neural%20Network%20%28from%20scratch%29" target="_blank" rel="noopener" class="share-btn-weibo">
          <i class="fab fa-weibo"></i>
        </a>
      </li>
    
  </ul>
</div>












  






  
  
  
    
  
  
  <div class="media author-card content-widget-hr">
    
      
      <img class="portrait mr-3" src="/lcgodoy.github.io/authors/admin/avatar_hu7b4a77b70d7ccc12dcfcf06f5fc11ad8_27284_250x250_fill_q90_lanczos_center.jpg" alt="Avatar">
    

    <div class="media-body">
      <h5 class="card-title"><a href="lcgodoy.github.io/">Lucas Godoy</a></h5>
      <h6 class="card-subtitle">PhD Candidate / TA /GA</h6>
      <p class="card-text">I&rsquo;m a PhD Candidate in Stats interested in <code>R</code>, Open Data, and the most diverse applications of statistics.</p>
      <ul class="network-icon" aria-hidden="true">
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="/lcgodoy.github.io/#contact" >
        <i class="fas fa-envelope"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://twitter.com/ldcgodoy" target="_blank" rel="noopener">
        <i class="fab fa-twitter"></i>
      </a>
    </li>
  
    
    
    
    
    
    
    
      
    
    <li>
      <a href="https://scholar.google.com.br/citations?hl=en&amp;pli=1&amp;user=wzIw2_4AAAAJ" target="_blank" rel="noopener">
        <i class="ai ai-google-scholar"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://github.com/lcgodoy" target="_blank" rel="noopener">
        <i class="fab fa-github"></i>
      </a>
    </li>
  
</ul>

    </div>
  </div>




<section id="comments">
  
    
<div id="disqus_thread"></div>
<script>
  let disqus_config = function () {
    
    
    
  };
  (function() {
    if (["localhost", "127.0.0.1"].indexOf(window.location.hostname) != -1) {
      document.getElementById('disqus_thread').innerHTML = 'Disqus comments not available by default when the website is previewed locally.';
      return;
    }
    var d = document, s = d.createElement('script'); s.async = true;
    s.src = 'https://' + "lcgodoy-github-io" + '.disqus.com/embed.js';
    s.setAttribute('data-timestamp', +new Date());
    (d.head || d.body).appendChild(s);
  })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="https://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>


  
</section>






  
  



  </div>
</article>

      

    
    
    
    <script src="/lcgodoy.github.io/js/mathjax-config.js"></script>
    

    
    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js" integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js" integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.js" integrity="sha256-yt2kYMy0w8AbtF89WXb2P1rfjcP/HTHLT7097U8Y5b8=" crossorigin="anonymous"></script>

      

      
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.10/highlight.min.js" integrity="sha256-1zu+3BnLYV9LdiY85uXMzii3bdrkelyp37e0ZyTAQh0=" crossorigin="anonymous"></script>
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.10/languages/bash.min.js"></script>
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.10/languages/cs.min.js"></script>
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.10/languages/json.min.js"></script>
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.10/languages/markdown.min.js"></script>
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.10/languages/r.min.js"></script>
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.10/languages/yaml.min.js"></script>
        
      

      
      
      <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js" integrity="" crossorigin="anonymous" async></script>
      
    

    
    

    
    
    <script>hljs.initHighlightingOnLoad();</script>
    

    

    
    

    

    
    

    
    

    
    
    <script id="dsq-count-scr" src="https://lcgodoy-github-io.disqus.com/count.js" async></script>
    

    
    
    
    
    
    
    
    
    
    
    
    
    <script src="/lcgodoy.github.io/js/academic.min.368ad8701ddf69c34950016f6435139b.js"></script>

    






  
  
  <div class="container">
    <footer class="site-footer">
  

  <p class="powered-by">
    © 2022 &middot; 

    Powered by the
    <a href="https://sourcethemes.com/academic/" target="_blank" rel="noopener">Academic theme</a> for
    <a href="https://gohugo.io" target="_blank" rel="noopener">Hugo</a>.

    
    <span class="float-right" aria-hidden="true">
      <a href="#" class="back-to-top">
        <span class="button_icon">
          <i class="fas fa-chevron-up fa-2x"></i>
        </span>
      </a>
    </span>
    
  </p>
</footer>

  </div>
  

  
<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title">Cite</h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        <pre><code class="tex hljs"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-outline-primary my-1 js-copy-cite" href="#" target="_blank">
          <i class="fas fa-copy"></i> Copy
        </a>
        <a class="btn btn-outline-primary my-1 js-download-cite" href="#" target="_blank">
          <i class="fas fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

</body>
</html>
