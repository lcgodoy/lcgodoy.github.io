<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.29">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Lucas Godoy">
<meta name="dcterms.date" content="2021-07-28">

<title>Estimating regression coefficients using a Neural Network (from scratch) â€“ Lucas Godoy</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-0815c480559380816a4d1ea211a47e91.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-9269ebdfe1b4becf76ac06c4a1560b39.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


<meta property="og:title" content="Estimating regression coefficients using a Neural Network (from scratch) â€“ Lucas Godoy">
<meta property="og:description" content="">
<meta property="og:image" content="https://lcgodoy.me/posts/lmnet/2021-06-23-lmnnet_files/figure-html/fig-nn1-1.png">
<meta property="og:site_name" content="Lucas Godoy">
<meta property="og:image:height" content="594">
<meta property="og:image:width" content="1014">
</head>

<body class="nav-fixed quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Lucas Godoy</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../index.html"> 
<span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../publications.html"> 
<span class="menu-text">Publications</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../blog.html"> 
<span class="menu-text">Blog</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../talks.html"> 
<span class="menu-text">Talks</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../cv.pdf"> 
<span class="menu-text">CV</span></a>
  </li>  
</ul>
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/lcgodoy"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Estimating regression coefficients using a Neural Network (from scratch)</h1>
                                <div class="quarto-categories">
                <div class="quarto-category">statistics</div>
                <div class="quarto-category">neural networks</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Lucas Godoy </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">July 28, 2021</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#intro" id="toc-intro" class="nav-link active" data-scroll-target="#intro">Intro</a></li>
  <li><a href="#neural-network-regression" id="toc-neural-network-regression" class="nav-link" data-scroll-target="#neural-network-regression">Neural Network Regression</a>
  <ul class="collapse">
  <li><a href="#subsec:single" id="toc-subsec:single" class="nav-link" data-scroll-target="#subsec\:single">Single neuron feed-forward networks</a></li>
  <li><a href="#subsec:act" id="toc-subsec:act" class="nav-link" data-scroll-target="#subsec\:act">Activation functions</a></li>
  <li><a href="#cost-functions" id="toc-cost-functions" class="nav-link" data-scroll-target="#cost-functions">Cost functions</a></li>
  <li><a href="#subsec:fit-nn" id="toc-subsec:fit-nn" class="nav-link" data-scroll-target="#subsec\:fit-nn">Fitting a Neural Network</a></li>
  <li><a href="#subsec:backpro" id="toc-subsec:backpro" class="nav-link" data-scroll-target="#subsec\:backpro">Backpropagation</a></li>
  </ul></li>
  <li><a href="#sec:imple" id="toc-sec:imple" class="nav-link" data-scroll-target="#sec\:imple">Implementation</a></li>
  <li><a href="#sec:ne" id="toc-sec:ne" class="nav-link" data-scroll-target="#sec\:ne">Numerical Examples</a>
  <ul class="collapse">
  <li><a href="#example-1-equivalence-between-neural-network-and-linear-model" id="toc-example-1-equivalence-between-neural-network-and-linear-model" class="nav-link" data-scroll-target="#example-1-equivalence-between-neural-network-and-linear-model">Example 1: Equivalence between Neural Network and Linear Model</a></li>
  <li><a href="#example-2-nonlinear-relationship-and-number-of-hidden-units" id="toc-example-2-nonlinear-relationship-and-number-of-hidden-units" class="nav-link" data-scroll-target="#example-2-nonlinear-relationship-and-number-of-hidden-units">Example 2: Nonlinear relationship and number of hidden units</a></li>
  </ul></li>
  <li><a href="#final-thoughts" id="toc-final-thoughts" class="nav-link" data-scroll-target="#final-thoughts">Final Thoughts</a></li>
  <li><a href="#references" id="toc-references" class="nav-link" data-scroll-target="#references">References</a></li>
  </ul>
<div class="toc-actions"><ul><li><a href="https://github.com/lcgodoy/lcgodoy.github.io/issues/new" class="toc-action"><i class="bi bi-github"></i>Report an issue</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">





<section id="intro" class="level2">
<h2 class="anchored" data-anchor-id="intro">Intro</h2>
<p>The idea behind the Neural Networks models, as its nomenclature suggests, is to mimic the way human brain learns to execute of some tasks. Some works in the literature (<span class="citation" data-cites="cheng1994neural">Cheng and Titterington (<a href="#ref-cheng1994neural" role="doc-biblioref">1994</a>)</span>, <span class="citation" data-cites="stern1996neural">Stern (<a href="#ref-stern1996neural" role="doc-biblioref">1996</a>)</span>, <span class="citation" data-cites="warner1996understanding">Warner and Misra (<a href="#ref-warner1996understanding" role="doc-biblioref">1996</a>)</span>) attribute of the first attempts to build a â€œNeural Network emulatorâ€ to <span class="citation" data-cites="mcculloch1943logical">McCulloch and Pitts (<a href="#ref-mcculloch1943logical" role="doc-biblioref">1943</a>)</span>. The popularity of this method in the past decades was held down by the computation intensive calculations needed for such procedures. However, the computation resources advances in the last few years allied to the algorithmic nature of Neural Networks have contributed to the adoption of the methodology by computer scientists. These days, this models are very popular in the industry and are applied to several interesting applications such as speech recognition, image classification, and automatic text translation.</p>
</section>
<section id="neural-network-regression" class="level2">
<h2 class="anchored" data-anchor-id="neural-network-regression">Neural Network Regression</h2>
<p>A neural network is a highly parametrized model that, provided we have enough data, can approximate any functional relationship between a set of <em>features</em><a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>ğ±</mi><annotation encoding="application/x-tex">\mathbf{x}</annotation></semantics></math> and a response variable <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>y</mi><annotation encoding="application/x-tex">y</annotation></semantics></math> (<span class="citation" data-cites="efron2016computer">Efron and Hastie (<a href="#ref-efron2016computer" role="doc-biblioref">2016</a>)</span>, pages 151-152). Although there are several possible structures for neural networks, for this post we are going to consider only the <em>feed-forward</em><a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a> neural networks. In order to explain how these neural networks are designed, letâ€™s consider its graphical representation (see <a href="#fig-nn1" class="quarto-xref">Figure&nbsp;1</a>). We have vertices, which are called a units (or neurons), ordered horizontally by layers. An edge coming from one vertex can only be connected to vertices associated with â€œhigherâ€ layers. These connections represent a information flow from left to right (hence, the name feed-forward), where each unit computed by 1) giving weights to each of its inputs, 2) calculating the dot product between weights and inputs, 3) adding a constant( usually referred to as <em>bias</em>) to it, and, finally, 4) applying an element-wise <em>activation</em> function <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>â‹…</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">f(\cdot)</annotation></semantics></math> to it. These <em>activation functions</em> are used to establish non-linear relationships between units.</p>
<p>The number of hidden layers as well as the number of units associated with every layer can both be regard as tuning parameters. The design and architecture of a neural network is a complex task. In summary, when having a single hidden layer, the number of units associated with the hidden layer determines the number of parameters associated with the model. <span class="citation" data-cites="efron2016computer">Efron and Hastie (<a href="#ref-efron2016computer" role="doc-biblioref">2016</a>)</span> suggest that, under this scenario, it is better to consider several units for the hidden layer and use some kind of regularization to avoid overfitting. Penalizations analogous to the Ridge and Lasso penalty for linear models are often used in the regularization context for neural networks (<span class="citation" data-cites="hastie2015statistical">Hastie, Tibshirani, and Wainwright (<a href="#ref-hastie2015statistical" role="doc-biblioref">2015</a>)</span>, pages 210-211).</p>
<p>An important remark regarding the neural network models is that they are â€œpure prediction algorithmsâ€. That is, these models are focused only on prediction, neglecting the estimation, as pointed by <span class="citation" data-cites="efron2020prediction">Efron (<a href="#ref-efron2020prediction" role="doc-biblioref">2020</a>)</span>. The strategy is simple and consists in searching for high predictive accuracy. That being said, these algorithms make no assumption on the probability distribution of the data and, as one of the consequences of losing these assumptions, it is not possible to make interval predictions or to calculate confidence intervals for the â€œestimatedâ€ parameters.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-nn1" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-nn1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="2021-06-23-lmnnet_files/figure-html/fig-nn1-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-nn1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: A <em>feed-forward</em> neural network with a single hidden layer.
</figcaption>
</figure>
</div>
</div>
</div>
<section id="subsec:single" class="level3">
<h3 class="anchored" data-anchor-id="subsec:single">Single neuron feed-forward networks</h3>
<p>A single neuron feed-forward network does not possess any hidden layer in its structure. The absence of hidden layers makes these models resemble the statistical models we are most used to, like, for example, the linear regression and logistic regression. By analyzing the graphical representation of a single layer feed-forward network (<a href="#fig-nn2" class="quarto-xref">Figure&nbsp;2</a>), it is easy to see that by taking the identity as the <em>activation function</em>, the functional relationship between <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>ğ±</mi><annotation encoding="application/x-tex">\mathbf{x}</annotation></semantics></math> and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>y</mi><annotation encoding="application/x-tex">y</annotation></semantics></math> considered by the neural network is equivalent to the one used for the general linear model. Considering the same representation, if we take <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mtext mathvariant="normal">logit</mtext><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">f(x) = \textrm{logit}(x)</annotation></semantics></math> (<em>sigmoid</em> function, according to the neural network models literature) and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>y</mi><mo>âˆˆ</mo><mo stretchy="false" form="prefix">{</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy="false" form="postfix">}</mo></mrow><annotation encoding="application/x-tex">y \in \{ 0, 1 \}</annotation></semantics></math>, then the neural network provides the same relationship between <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>ğ±</mi><annotation encoding="application/x-tex">\mathbf{x}</annotation></semantics></math> and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>y</mi><annotation encoding="application/x-tex">y</annotation></semantics></math> as the one used by the logistic regression.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-nn2" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-nn2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="2021-06-23-lmnnet_files/figure-html/fig-nn2-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-nn2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2: A single layer <em>feed-forward</em> neural network.
</figcaption>
</figure>
</div>
</div>
</div>
<p>Although the functional relationship between <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>ğ±</mi><annotation encoding="application/x-tex">\mathbf{x}</annotation></semantics></math> and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>y</mi><annotation encoding="application/x-tex">y</annotation></semantics></math> assumed by the single layer neural network coincides with some statistical models, we cannot promptly claim an equivalence between models because the way the neural networks <em>learn</em>, that is estimates, the weights can lead to different solutions depending on the <em>loss</em> and <em>cost</em> functions selected, we are going to talk more about these functions in the next section.</p>
</section>
<section id="subsec:act" class="level3">
<h3 class="anchored" data-anchor-id="subsec:act">Activation functions</h3>
<p>Activation functions are applied in every â€œLayer connectionâ€ in neural network models. Suppose, for example, we have a design matrix <span class="math inline">$\mathbf{X} \in {\rm
I\!R}^{n \times p}$</span>, and a response variable <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>ğ²</mi><annotation encoding="application/x-tex">\mathbf{y}</annotation></semantics></math>. Then, given appropriate choices of the <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>K</mi><mo>âˆ’</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">K - 1</annotation></semantics></math> (one for each layer connection), the mathematical model, for a single observation, behind the neural network, can be written in a vectorial notation as follows <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>ğ³</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>k</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo>=</mo><msup><mi>ğ–</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>k</mi><mo>âˆ’</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow></msup><msup><mi>ğš</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>k</mi><mo>âˆ’</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow></msup></mrow><annotation encoding="application/x-tex">
\mathbf{z}^{(k)} = \mathbf{W}^{(k - 1)} \mathbf{a}^{(k - 1)}
</annotation></semantics></math> <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>ğš</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>k</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo>=</mo><msub><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>k</mi><mo stretchy="true" form="postfix">)</mo></mrow></msub><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>ğ³</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>k</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo stretchy="true" form="postfix">)</mo></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">
\mathbf{a}^{(k)} = f_{(k)} \left( \mathbf{z}^{(k)} \right),
</annotation></semantics></math> where <span class="math inline">$\mathbf{W}^{(k - 1)} \in {\rm I\!R}^{m_{k - 1} \times m_{k}}$</span> is the matrix of weights that go from from the layer <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>L</mi><mrow><mi>k</mi><mo>âˆ’</mo><mn>1</mn></mrow></msub><annotation encoding="application/x-tex">L_{k - 1}</annotation></semantics></math> to the layer <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>L</mi><mi>k</mi></msub><annotation encoding="application/x-tex">L_{k}</annotation></semantics></math>, <span class="math inline">$\mathbf{a}^{(k)} \in {\rm I\!R}^{m_k \times m_{k + 1}}$</span> matrix of units at layer <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>L</mi><mi>k</mi></msub><annotation encoding="application/x-tex">L_k</annotation></semantics></math>, and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>k</mi><mo stretchy="true" form="postfix">)</mo></mrow></msub><annotation encoding="application/x-tex">f_{(k)}</annotation></semantics></math> is a (element-wise) activation function used at the layer <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>L</mi><mi>k</mi></msub><annotation encoding="application/x-tex">L_k</annotation></semantics></math>. Note that, when <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">k = 0</annotation></semantics></math>, then <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>ğš</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>0</mn><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo>=</mo><mi>ğ—</mi></mrow><annotation encoding="application/x-tex">\mathbf{a}^{(0)} = \mathbf{X}</annotation></semantics></math>. Observe that <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>m</mi><mi>k</mi></msub><annotation encoding="application/x-tex">m_k</annotation></semantics></math> is the number of units in the layer <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation></semantics></math> and, consequently, for the input and output layers, respectively, we have <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>m</mi><mn>0</mn></msub><mo>=</mo><mi>p</mi></mrow><annotation encoding="application/x-tex">m_0 = p</annotation></semantics></math> and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>m</mi><mi>K</mi></msub><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">m_K = 1</annotation></semantics></math>.</p>
<p>From this example, it is clear that we can apply different activation functions when connecting different layers. Nevertheless, the activation for one layer is the same along all of its units.</p>
<p>Although, theoretically, there exists no restriction on which functions to use as activation function, we want these functions to be at least one time differentiable. This is due to the fact that most of the methods used to find the optimal weights are based on gradients. Another aspect to be considered when choosing an activation function is the domain of the output variable <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>y</mi><annotation encoding="application/x-tex">y</annotation></semantics></math>. That is, if <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>y</mi><mo>âˆˆ</mo><mrow><mo stretchy="true" form="prefix">[</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy="true" form="postfix">]</mo></mrow></mrow><annotation encoding="application/x-tex">y \in [0, 1]</annotation></semantics></math>, we want an activation function that maps real values to the <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="true" form="prefix">[</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy="true" form="postfix">]</mo></mrow><annotation encoding="application/x-tex">[0, 1]</annotation></semantics></math> interval. In summary, for the output layer, we use a activation function that makes predictions on the same domain as the output variable, while, for hidden layers, we have no restrictions on the activation functions, besides the ones already mentioned.</p>
<p>Some commonly used link functions are the <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtext mathvariant="normal">logit</mtext><annotation encoding="application/x-tex">\textrm{logit}</annotation></semantics></math>, or sigmoid, function, defined as <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mfrac><mn>1</mn><mrow><mn>1</mn><mo>+</mo><msup><mi>e</mi><mrow><mi>âˆ’</mi><mi>x</mi></mrow></msup></mrow></mfrac><mo>,</mo></mrow><annotation encoding="application/x-tex">
f(x) = \frac{1}{1 + e^{-x}},
</annotation></semantics></math> the hyperbolic tangent function, referred to as <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtext mathvariant="normal">tanh</mtext><annotation encoding="application/x-tex">\textrm{tanh}</annotation></semantics></math>, <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mfrac><mrow><msup><mi>e</mi><mi>z</mi></msup><mo>âˆ’</mo><msup><mi>e</mi><mrow><mi>âˆ’</mi><mi>z</mi></mrow></msup></mrow><mrow><msup><mi>e</mi><mi>z</mi></msup><mo>+</mo><msup><mi>e</mi><mrow><mi>âˆ’</mi><mi>z</mi></mrow></msup></mrow></mfrac><mi>.</mi></mrow><annotation encoding="application/x-tex">
f(x) = \frac{e^z - e^{-z}}{e^{z} + e^{-z}}.
</annotation></semantics></math> Note that the <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtext mathvariant="normal">tanh</mtext><annotation encoding="application/x-tex">\textrm{tanh}</annotation></semantics></math> is mapping from the real line to the <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="true" form="prefix">(</mo><mi>âˆ’</mi><mn>1</mn><mo>,</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(-1, 1)</annotation></semantics></math> interval. The Rectified Linear Unit (ReLU) is also a popular choice and is defined as <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><msub><mi>x</mi><mo>+</mo></msub><mo>=</mo><mo>max</mo><mrow><mo stretchy="true" form="prefix">(</mo><mn>0</mn><mo>,</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">
f(x) = x_{+} = \max(0, x),
</annotation></semantics></math> the main advantage of this function is a cheap to compute gradient. A different version of the ReLU called leaky ReLU is also quite popular, its definition is given as follows <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><msub><mi>x</mi><mo>+</mo></msub><mo>=</mo><mo>max</mo><mrow><mo stretchy="true" form="prefix">(</mo><mn>.01</mn><mo>*</mo><mi>x</mi><mo>,</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">
f(x) = x_{+} = \max(.01 * x, x),
</annotation></semantics></math></p>
<p>These are only some examples of commonly used activation functions and they are illustrated in <a href="#fig-act-funs" class="quarto-xref">Figure&nbsp;3</a>. The user does need to be restrict to these options since there are several other functions implemented in the software available to compute neural networks. However, if you want to use a activation function that is not implemented yet, you may have to implement your own version for the algorithm.</p>
<div class="cell">
<div class="cell-output-display">
<div id="fig-act-funs" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-act-funs-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="2021-06-23-lmnnet_files/figure-html/fig-act-funs-1.png" class="img-fluid figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-act-funs-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3: The most popular activation functions (figure inspired by Figure 18.6 from <span class="citation" data-cites="efron2016computer">Efron and Hastie (<a href="#ref-efron2016computer" role="doc-biblioref">2016</a>)</span> ).
</figcaption>
</figure>
</div>
</div>
</div>
<p>Although there are no restrictions on the functions used as activation functions in the hidden layers (besides being differentiable functions), it is not advisable to use the identity function because it implies a waste of computational power. This is due to the fact that using a linear function in a hidden layer, makes the units from that layer a linear combination of the units from the previous layer. To make this clear, letâ€™s prove that a Neural Network model with a single hidden layer collapses to a Generalized Linear Model when the identity function is used as the activation function.</p>
<p>Suppose a <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math>-dimensional vector <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>ğ²</mi><annotation encoding="application/x-tex">\mathbf{y}</annotation></semantics></math> is assumed to follow a distribution <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>ğ’«</mi><annotation encoding="application/x-tex">\mathcal{P}</annotation></semantics></math>, where <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>ğ’«</mi><annotation encoding="application/x-tex">\mathcal{P}</annotation></semantics></math> belongs to the exponential family of distributions. Then, given a design matrix <span class="math inline">$\mathbf{X} \in
{\rm I\!R}^{n \times p}$</span>, the Generalized Linear Model for <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>ğ²</mi><annotation encoding="application/x-tex">\mathbf{y}</annotation></semantics></math> is composed by the <em>random component</em>, given by the probability density function associated with the distribution <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>ğ’«</mi><annotation encoding="application/x-tex">\mathcal{P}</annotation></semantics></math>, the <em>systematic component</em>, defined by <span id="eq-syst-comp"><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>ğ›ˆ</mi><mo>=</mo><mi>ğ—</mi><mi>ğ›ƒ</mi><mo>,</mo><mspace width="2.0em"></mspace><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">
\boldsymbol{\eta} = \mathbf{X} \boldsymbol{\beta},
 \qquad(1)</annotation></semantics></math></span> and a (canonical) link function <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>g</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>â‹…</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">g(\cdot)</annotation></semantics></math> such that <span id="eq-link-f"><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>ğ›</mi><mo>=</mo><mi>g</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>ğ›ˆ</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>.</mi><mspace width="2.0em"></mspace><mrow><mo stretchy="true" form="prefix">(</mo><mn>2</mn><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">
\boldsymbol{\mu} = g(\boldsymbol{\eta}).
 \qquad(2)</annotation></semantics></math></span> Once we estimate the parameters <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>ğ›ƒ</mi><annotation encoding="application/x-tex">\boldsymbol{\beta}</annotation></semantics></math>, we have <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover><mi>ğ²</mi><mo accent="true">Ì‚</mo></mover><mo>=</mo><mi>g</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>ğ—</mi><mover><mi>ğ›ƒ</mi><mo accent="true">Ì‚</mo></mover><mo stretchy="true" form="postfix">)</mo></mrow><mi>.</mi></mrow><annotation encoding="application/x-tex">
\hat{\mathbf{y}} = g( \mathbf{X} \hat{\boldsymbol{\beta}} ).
</annotation></semantics></math></p>
<p>Define now our Neural Network model having a single hidden layer with <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>M</mi><annotation encoding="application/x-tex">M</annotation></semantics></math> units. The activation function for the hidden layer is <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>f</mi><mi>h</mi></msub><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mi>g</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">f_h(x) = g(x)</annotation></semantics></math>, that is, the same as the identity function. The weights we want to find are <span class="math inline">$\mathbf{W}^{(1)} \in {\rm I\!R}^{p \times 1}$</span>, and <span class="math inline">$\mathbf{W}^{(2)} \in
{\rm I\!R}^{M \times n}$</span>. The activation function for the activation layer is the previously mentioned canonical link function. Finally, let the loss be the deviance residual associated with the distribution <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>ğ’«</mi><annotation encoding="application/x-tex">\mathcal{P}</annotation></semantics></math>, and the cost function be the average of the losses. Then, the mathematical representation of the Neural Network becomes <span id="eq-example-glm-nn-linear"><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>ğ³</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo>=</mo><mi>ğ—</mi><msup><mi>ğ–</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo>=</mo><msup><mi>ğš</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo>,</mo><mspace width="2.0em"></mspace><mrow><mo stretchy="true" form="prefix">(</mo><mn>3</mn><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">
\mathbf{z}^{(1)} = \mathbf{X} \mathbf{W}^{(1)} = \mathbf{a}^{(1)},
 \qquad(3)</annotation></semantics></math></span> because the activation function for the hidden layer is the identity. Then, we have <span id="eq-example-glm-nn-hidden"><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>ğ³</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>2</mn><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo>=</mo><msup><mi>ğš</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow></msup><msup><mi>ğ–</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>2</mn><mo stretchy="true" form="postfix">)</mo></mrow></msup><mspace width="2.0em"></mspace><mrow><mo stretchy="true" form="prefix">(</mo><mn>4</mn><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">
\mathbf{z}^{(2)} = \mathbf{a}^{(1)} \mathbf{W}^{(2)}
 \qquad(4)</annotation></semantics></math></span> <span id="eq-example-glm-nn-out"><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>ğ²</mi><mo>=</mo><msup><mi>ğš</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>2</mn><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo>=</mo><mi>g</mi><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>ğ³</mi><mrow><mo stretchy="true" form="prefix" mathvariant="bold">(</mo><mn>ğŸ</mn><mo stretchy="true" form="postfix" mathvariant="bold">)</mo></mrow></msup><mo stretchy="true" form="postfix">)</mo></mrow><mi>.</mi><mspace width="2.0em"></mspace><mrow><mo stretchy="true" form="prefix">(</mo><mn>5</mn><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">
\mathbf{y} = \mathbf{a}^{(2)} = g( \mathbf{\mathbf{z}^{(2)}} ).
 \qquad(5)</annotation></semantics></math></span> However, note that, by combining <a href="#eq-example-glm-nn-linear" class="quarto-xref">3</a>, <a href="#eq-example-glm-nn-hidden" class="quarto-xref">4</a>, and <a href="#eq-example-glm-nn-out" class="quarto-xref">5</a> we get <span class="math display">$$\begin{align*}
\mathbf{y} &amp; = g( \mathbf{\mathbf{z}^{(2)}} ) \\
&amp; = g( \mathbf{a}^{(1)} \mathbf{W}^{(2)} ) \\
&amp; = g( \mathbf{X} \underbrace{\mathbf{W}^{(1)} \mathbf{W}^{(2)}}_{{\rm I\!R}_{p
\times 1}} ),
\end{align*}$$</span> which yields to optimal weights (see <a href="#subsec:fit-nn">Fitting a Neural Network</a> and <a href="#subsec:backpro">Backpropagation</a>, for more information on how to fit a neural network model) satisfying <span class="math display">$$
\underbrace{\mathbf{W}^{(1)} \mathbf{W}^{(2)}}_{{\rm I\!R}_{p
\times 1}} = \hat{\boldsymbol{\beta}},
$$</span> where <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mover><mi>ğ›ƒ</mi><mo accent="true">Ì‚</mo></mover><annotation encoding="application/x-tex">\hat{\boldsymbol{\beta}}</annotation></semantics></math> is the Maximum Likelihood Estimator for <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>ğ›ƒ</mi><annotation encoding="application/x-tex">\boldsymbol{\beta}</annotation></semantics></math> that can be obtained using the Iterative Reweighted Least Squares for the model defined by the probability density function associated with the distribution <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>ğ’«</mi><annotation encoding="application/x-tex">\mathcal{P}</annotation></semantics></math>, the systematic component <a href="#eq-syst-comp" class="quarto-xref">1</a> and a (canonical) link function <a href="#eq-link-f" class="quarto-xref">2</a>.</p>
</section>
<section id="cost-functions" class="level3">
<h3 class="anchored" data-anchor-id="cost-functions">Cost functions</h3>
<p>Whenever we want to fit a neural network to a dataset we need to specify a <em>Cost</em> function, which is usually based on <em>loss</em> functions. A loss function, in the context of Neural Network models, measures how far our predictions <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>ğ±</mi><mo>;</mo><mi>ğ–</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">f(\mathbf{x}; \mathbf{W})</annotation></semantics></math> are from the true value <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>y</mi><annotation encoding="application/x-tex">y</annotation></semantics></math>. Examples of commonly used loss functions, for a single observation, are the mean square error loss and the binomial deviance defined, respectively, as <span id="eq-loss-mse"><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>L</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>ğ°</mi><mo>,</mo><mi>ğ±</mi><mo>;</mo><mi>y</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mfrac><mn>1</mn><mn>2</mn></mfrac><msup><mrow><mo stretchy="true" form="prefix">(</mo><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>ğ±</mi><mo>;</mo><mi>ğ°</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>âˆ’</mo><mi>y</mi><mo stretchy="true" form="postfix">)</mo></mrow><mn>2</mn></msup><mo>,</mo><mspace width="2.0em"></mspace><mrow><mo stretchy="true" form="prefix">(</mo><mn>6</mn><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">
L(\mathbf{w}, \mathbf{x}; y) = \frac{1}{2} (f(\mathbf{x}; \mathbf{w}) - y)^{2},
 \qquad(6)</annotation></semantics></math></span> and <span id="eq-loss-bdev"><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>L</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>ğ–</mi><mo>,</mo><mi>ğ±</mi><mo>;</mo><mi>y</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mi>y</mi><mo>log</mo><mrow><mo stretchy="true" form="prefix">(</mo><mfrac><mi>y</mi><mrow><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>ğ±</mi><mo>,</mo><mi>ğ°</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow></mfrac><mo stretchy="true" form="postfix">)</mo></mrow><mo>+</mo><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo>âˆ’</mo><mi>y</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>log</mo><mrow><mo stretchy="true" form="prefix">(</mo><mfrac><mrow><mn>1</mn><mo>âˆ’</mo><mi>y</mi></mrow><mrow><mn>1</mn><mo>âˆ’</mo><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>ğ±</mi><mo>,</mo><mi>ğ°</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow></mfrac><mo stretchy="true" form="postfix">)</mo></mrow><mi>.</mi><mspace width="2.0em"></mspace><mrow><mo stretchy="true" form="prefix">(</mo><mn>7</mn><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">
L(\mathbf{W}, \mathbf{x}; y) = y \log \left( \frac{y}{f(\mathbf{x}, \mathbf{w})}
\right) + 
(1 - y) \log \left( \frac{1 - y}{1 - f(\mathbf{x}, \mathbf{w})} \right).
 \qquad(7)</annotation></semantics></math></span> The loss function <a href="#eq-loss-mse" class="quarto-xref">6</a> is usually employed when the output (response) variable assumes continuous values, while the <a href="#eq-loss-bdev" class="quarto-xref">7</a> is used for binary output variables.</p>
<p>After choosing an appropriate loss function, the cost function is defined as the average of the loss function over all the observation, that is <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>C</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>ğ²</mi><mo>;</mo><mi>ğ±</mi><mo>,</mo><mi>ğ–</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mfrac><mn>1</mn><mi>n</mi></mfrac><munderover><mo>âˆ‘</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mi>L</mi><mrow><mo stretchy="true" form="prefix">(</mo><mrow><msub><mi>ğ°</mi><mi>ğ¢</mi></msub><mo mathvariant="bold">,</mo><msub><mi>ğ±</mi><mi>ğ¢</mi></msub><mo mathvariant="bold">;</mo><msub><mi>ğ²</mi><mi>ğ¢</mi></msub></mrow><mo stretchy="true" form="postfix">)</mo></mrow><mo>+</mo><mi>Î»</mi><mi>J</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>ğ–</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">
C(\mathbf{y}; \mathbf{x}, \mathbf{W}) = \frac{1}{n} \sum_{i = 1}^{n}
L(\mathbf{w_i, \mathbf{x}_i; y_i}) + \lambda J(\mathbf{W}),
</annotation></semantics></math> where <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>J</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>ğ–</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">J(\mathbf{W})</annotation></semantics></math> is a non-negative regularization term and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Î»</mi><mo>â‰¥</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">\lambda \geq
0</annotation></semantics></math> is a tuning parameter.</p>
<p>In practice, we may have a regularization term for each layer, each one having its own <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Î»</mi><annotation encoding="application/x-tex">\lambda</annotation></semantics></math>. Some commonly used regularization terms are <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>J</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>ğ–</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mfrac><mn>1</mn><mn>2</mn></mfrac><munderover><mo>âˆ‘</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>K</mi><mo>âˆ’</mo><mn>1</mn></mrow></munderover><msup><mrow><mo stretchy="true" form="prefix">â€–</mo><msup><mi>ğ°</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>k</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo stretchy="true" form="postfix">â€–</mo></mrow><mn>2</mn></msup><mo>,</mo></mrow><annotation encoding="application/x-tex">
J(\mathbf{W}) = \frac{1}{2} \sum_{k = 1}^{K - 1} \lVert \mathbf{w}^{(k)} \rVert^2,
</annotation></semantics></math> and <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>J</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>ğ–</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mfrac><mn>1</mn><mn>2</mn></mfrac><munderover><mo>âˆ‘</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>K</mi><mo>âˆ’</mo><mn>1</mn></mrow></munderover><mrow><mo stretchy="true" form="prefix">â€–</mo><msup><mi>ğ°</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>k</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo stretchy="true" form="postfix">â€–</mo></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">
J(\mathbf{W}) = \frac{1}{2} \sum_{k = 1}^{K - 1} \lVert \mathbf{w}^{(k)} \rVert,
</annotation></semantics></math> where <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>K</mi><annotation encoding="application/x-tex">K</annotation></semantics></math> is the number of layers of our neural network model, and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>ğ°</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>k</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup><annotation encoding="application/x-tex">\mathbf{w}^{(k)}</annotation></semantics></math> is the vector of weights from the units in the layer <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>L</mi><mi>k</mi></msub><annotation encoding="application/x-tex">L_k</annotation></semantics></math> to the layer <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>L</mi><mrow><mi>k</mi><mo>+</mo><mn>1</mn></mrow></msub><annotation encoding="application/x-tex">L_{k + 1}</annotation></semantics></math>. Note that, these two regularization terms are analogous to the Ridge and Lasso penalizations, and they play the exact same role in neural networks as its analogous versions do for the linear models <span class="citation" data-cites="efron2016computer">(<a href="#ref-efron2016computer" role="doc-biblioref">Efron and Hastie 2016</a>)</span>. Mixtures of these two regularization terms, as in the elastic net <span class="citation" data-cites="zou2005regularization">(<a href="#ref-zou2005regularization" role="doc-biblioref">Zou and Hastie 2005</a>)</span>, are also common.</p>
</section>
<section id="subsec:fit-nn" class="level3">
<h3 class="anchored" data-anchor-id="subsec:fit-nn">Fitting a Neural Network</h3>
<p>Supposing a user has set the number of layers, units, an activation function and a loss function, to fit a neural network we seek the set of weights <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>ğ–</mi><mo>=</mo><mo stretchy="false" form="prefix">{</mo><msup><mi>ğ–</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo>,</mo><mi>â€¦</mi><mo>,</mo><msup><mi>ğ–</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>k</mi><mo>âˆ’</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo stretchy="false" form="postfix">}</mo></mrow><annotation encoding="application/x-tex">\mathbf{W}
= \{ \mathbf{W}^{(1)}, \ldots, \mathbf{W}^{(k - 1)} \}</annotation></semantics></math> such that the cost function is minimized, that is <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><munder><mo>min</mo><mi>ğ–</mi></munder><mrow><mo stretchy="true" form="prefix">{</mo><mi>ğ’</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>ğ²</mi><mo>;</mo><mi>ğ—</mi><mo>,</mo><mi>ğ–</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">}</mo></mrow><mi>.</mi></mrow><annotation encoding="application/x-tex"> 
\min_{\mathbf{W}} \left \{ \mathcal{C}(\mathbf{y}; \mathbf{X}, \mathbf{W}) \right \}.
</annotation></semantics></math> Therefore, the neural network fit has turned into an optimization problem. The most common algorithm used to solve this optimization problem is the Backpropagation algorithm, which is described in the next section for a general situation.</p>
</section>
<section id="subsec:backpro" class="level3">
<h3 class="anchored" data-anchor-id="subsec:backpro">Backpropagation</h3>
<p>Backpropagation (or gradient descent) is the method used to find the weights which minimize the chosen cost and loss functions for a given neural network. It is an iterative algorithm that is guaranteed to converge whenever the cost function has a single local minima <span class="citation" data-cites="efron2016computer">(<a href="#ref-efron2016computer" role="doc-biblioref">Efron and Hastie 2016</a>)</span>. However, even if the cost function does not have a single local minima, the algorithm works fairly well. The updates for a weight matrix, <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>ğ–</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>k</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup><annotation encoding="application/x-tex">\mathbf{W}^{(k)}</annotation></semantics></math> letâ€™s say, is done as follows <span id="eq-iter"><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>ğ–</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>k</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo>=</mo><msup><mi>ğ–</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>k</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo>âˆ’</mo><mi>Î±</mi><mfrac><mrow><mi>âˆ‚</mi><mi>ğ’</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>ğ²</mi><mo>;</mo><mi>ğ—</mi><mo>,</mo><mi>ğ–</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><mrow><mi>âˆ‚</mi><msup><mi>ğ–</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>k</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup></mrow></mfrac><mo>,</mo><mspace width="2.0em"></mspace><mrow><mo stretchy="true" form="prefix">(</mo><mn>8</mn><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">
\mathbf{W}^{(k)} = \mathbf{W}^{(k)} - \alpha \frac{\partial
\mathcal{C}(\mathbf{y}; \mathbf{X}, \mathbf{W})}{\partial \mathbf{W}^{(k)}},
 \qquad(8)</annotation></semantics></math></span> where <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Î±</mi><annotation encoding="application/x-tex">\alpha</annotation></semantics></math> is a tuning parameter called <em>learning rate</em>. The name backpropagation comes from the fact that the derivatives (or gradients) are computed according to something called a <em>computation graph</em> in a backward fashion. It is heavily based on the chain rule for differentiation.</p>
<p>Given initial values for the <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>ğ–</mi><annotation encoding="application/x-tex">\mathbf{W}</annotation></semantics></math> matrices, the method repeats the update rule <a href="#eq-iter" class="quarto-xref">8</a> until convergence. Provided that the columns of the design matrix are rescaled to mean 0 and variance 1, <span class="citation" data-cites="hastie2009elements">Hastie, Tibshirani, and Friedman (<a href="#ref-hastie2009elements" role="doc-biblioref">2009</a>)</span> suggest the use of random starting values for the weights as uniform random variables on the interval <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="true" form="prefix">[</mo><mi>âˆ’</mi><mn>.75</mn><mo>,</mo><mn>.75</mn><mo stretchy="true" form="postfix">]</mo></mrow><annotation encoding="application/x-tex">[-.75, .75]</annotation></semantics></math>.</p>
</section>
</section>
<section id="sec:imple" class="level2">
<h2 class="anchored" data-anchor-id="sec:imple">Implementation</h2>
<p>I created functions for the implementation of a Neural Network with a single hidden layer model for generic activation functions. The implementation considers the cost function defined as <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>C</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>ğ²</mi><mo>;</mo><mi>ğ—</mi><mo>,</mo><mi>ğ˜</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mfrac><mn>1</mn><mi>n</mi></mfrac><msup><mrow><mo stretchy="true" form="prefix">â€–</mo><mi>ğ²</mi><mo>âˆ’</mo><mover><mi>ğ²</mi><mo accent="true">Ì‚</mo></mover><mo stretchy="true" form="postfix">â€–</mo></mrow><mn>2</mn></msup><mi>.</mi></mrow><annotation encoding="application/x-tex">
C(\mathbf{y}; \mathbf{X}, \mathbf{Y}) = \frac{1}{n} \lVert \mathbf{y} -
\hat{\mathbf{y}} \rVert^2.
</annotation></semantics></math></p>
<p>The inputs for the implemented function are:</p>
<ul>
<li><p>A design matrix <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>ğ—</mi><annotation encoding="application/x-tex">\mathbf{X}</annotation></semantics></math>, including the columns of ones for the intercept;</p></li>
<li><p>A column vector <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>ğ²</mi><annotation encoding="application/x-tex">\mathbf{y}</annotation></semantics></math> containing the response variable;</p></li>
<li><p>The number of units for the hidden layer;</p></li>
<li><p>The activation function for the hidden layer;</p></li>
<li><p>The activation function for the output layer;</p></li>
<li><p>A scalar for the learning rate <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Î±</mi><annotation encoding="application/x-tex">\alpha</annotation></semantics></math>;</p></li>
<li><p>Two control parameters for the convergence of the algorithm. The maximum number of iterations allowed, and a relative error <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Ïµ</mi><annotation encoding="application/x-tex">\epsilon</annotation></semantics></math> which controls when to stop the iteration algorithm.</p></li>
</ul>
<p>The function returns a <code>list</code> of size 5. Its first element is the predicted vector for <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>ğ²</mi><annotation encoding="application/x-tex">\mathbf{y}</annotation></semantics></math>, the second contains the values of the cost function for each iteration of the algorithm. The third position of this <code>list</code> stores the weight matrices <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>ğ–</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow></msup><annotation encoding="application/x-tex">\mathbf{W}^{(1)}</annotation></semantics></math> and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>ğ–</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>2</mn><mo stretchy="true" form="postfix">)</mo></mrow></msup><annotation encoding="application/x-tex">\mathbf{W}^{(2)}</annotation></semantics></math>, while the last two positions store the number of iterations until attain the convergence and a string indicating whether the algorithm converged or not, respectively.</p>
<p>See below the implementation of some activation functions (and their derivatives)</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="do">##--- activation functions and their derivatives ----</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="do">## ReLU</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>relu <span class="ot">&lt;-</span> <span class="cf">function</span>(x) {</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>    <span class="fu">pmax</span>(x, <span class="dv">0</span>)</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="do">## derivative leaky ReLU</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>d_relu <span class="ot">&lt;-</span> <span class="cf">function</span>(x) {</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>    <span class="fu">ifelse</span>(x <span class="sc">&gt;</span> <span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">0</span>)</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="do">## leaky ReLU</span></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>lrelu <span class="ot">&lt;-</span> <span class="cf">function</span>(x) {</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>    <span class="fu">pmax</span>(x <span class="sc">*</span> .<span class="dv">01</span>, x)</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a><span class="do">## derivative leaky ReLU</span></span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>d_lrelu <span class="ot">&lt;-</span> <span class="cf">function</span>(x) {</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>    <span class="fu">ifelse</span>(x <span class="sc">&gt;</span> <span class="dv">0</span>, <span class="dv">1</span>, .<span class="dv">01</span>)</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a><span class="do">## derivative tanh</span></span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a>d_tanh <span class="ot">&lt;-</span> <span class="cf">function</span>(x) {</span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>    <span class="dv">1</span> <span class="sc">-</span> (<span class="fu">tanh</span>(x)<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a><span class="do">## derivative logit</span></span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a>d_logit <span class="ot">&lt;-</span> <span class="cf">function</span>(x) {</span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a>    <span class="fu">plogis</span>(x) <span class="sc">*</span> ( <span class="dv">1</span> <span class="sc">-</span> <span class="fu">plogis</span>(x) )</span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a><span class="do">## derivative identity</span></span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a>d_ident <span class="ot">&lt;-</span> <span class="cf">function</span>(x) {</span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a>    <span class="fu">pmax</span>( <span class="sc">-</span><span class="dv">2</span> <span class="sc">*</span> <span class="fu">abs</span>(x), <span class="dv">1</span> )</span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Now, letâ€™s implement some helper functions to fit our neural network models. First, the cost function used in our examples is given by</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="do">## cost function</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>cost_f <span class="ot">&lt;-</span> <span class="cf">function</span>(y, yhat) {</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>    <span class="fu">crossprod</span>(yhat <span class="sc">-</span> y) <span class="sc">/</span> <span class="fu">NROW</span>(y)</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The implementation of the functions that will need to be executed at each step of the optimization algorithm are defined below. <code>compute_nn</code> computes the hidden layers given the matrix of covariates (or features) <code>X</code>, the list containing the the weights <code>W</code> associated to each layer connection, and two activation functions <code>act_hidden</code> and <code>act_out</code> for the hidden and output layers, respectively (this is a the implementation for a 2 layers network). The <code>compute_grad</code> function computes the gradient and needs some further information like <code>y</code> (the response variable), <code>n</code> the sample size, and the derivatives of the activation functions. <code>update_aux</code> and <code>update_w</code> are helper functions used to update the weights.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="do">##--- functiosn to fit the neural network ----</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="do">## computing the forward step of the neural network</span></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>compute_nn <span class="ot">&lt;-</span> <span class="cf">function</span>(X, W, act_hidden, act_out) {</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>    Z <span class="ot">&lt;-</span> <span class="fu">vector</span>(<span class="at">mode =</span> <span class="st">"list"</span>, <span class="at">length =</span> <span class="dv">2</span>)</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>    Z[[<span class="dv">1</span>]] <span class="ot">&lt;-</span> X <span class="sc">%*%</span> W[[<span class="dv">1</span>]]</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>    A <span class="ot">&lt;-</span> <span class="fu">act_hidden</span>(Z[[<span class="dv">1</span>]])</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>    Z[[<span class="dv">2</span>]] <span class="ot">&lt;-</span> A <span class="sc">%*%</span> W[[<span class="dv">2</span>]]</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>    <span class="fu">return</span>( <span class="fu">list</span>(<span class="at">y =</span> <span class="fu">act_out</span>(Z[[<span class="dv">2</span>]]),</span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>                 <span class="at">z =</span> Z) )</span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a><span class="do">## computing the gradient of the neural network</span></span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a>compute_grad <span class="ot">&lt;-</span> <span class="cf">function</span>(y, X, W, act_hidden, act_out,</span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a>                         d1_hidden, d1_out, n) {</span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a>    nn    <span class="ot">&lt;-</span> <span class="fu">compute_nn</span>(X, W, act_hidden, act_out)</span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a>    aux_out <span class="ot">&lt;-</span> (nn<span class="sc">$</span>y <span class="sc">-</span> y) <span class="sc">*</span> <span class="fu">d1_out</span>(nn<span class="sc">$</span>z[[<span class="dv">2</span>]])</span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true" tabindex="-1"></a>    aux_hid <span class="ot">&lt;-</span> <span class="fu">tcrossprod</span>(aux_out, W[[<span class="dv">2</span>]]) <span class="sc">*</span></span>
<span id="cb3-22"><a href="#cb3-22" aria-hidden="true" tabindex="-1"></a>        <span class="fu">d1_hidden</span>(nn<span class="sc">$</span>z[[<span class="dv">1</span>]])</span>
<span id="cb3-23"><a href="#cb3-23" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb3-24"><a href="#cb3-24" aria-hidden="true" tabindex="-1"></a>    <span class="fu">return</span>(</span>
<span id="cb3-25"><a href="#cb3-25" aria-hidden="true" tabindex="-1"></a>        <span class="fu">list</span>(<span class="fu">crossprod</span>(X, aux_hid) <span class="sc">/</span> n,</span>
<span id="cb3-26"><a href="#cb3-26" aria-hidden="true" tabindex="-1"></a>             <span class="fu">crossprod</span>(<span class="fu">act_hidden</span>(nn<span class="sc">$</span>z[[<span class="dv">1</span>]]), aux_out) <span class="sc">/</span> n)</span>
<span id="cb3-27"><a href="#cb3-27" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb3-28"><a href="#cb3-28" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb3-29"><a href="#cb3-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-30"><a href="#cb3-30" aria-hidden="true" tabindex="-1"></a><span class="do">## aux function for updating W</span></span>
<span id="cb3-31"><a href="#cb3-31" aria-hidden="true" tabindex="-1"></a>update_aux <span class="ot">&lt;-</span> <span class="cf">function</span>(w, dw, alpha) {</span>
<span id="cb3-32"><a href="#cb3-32" aria-hidden="true" tabindex="-1"></a>    w <span class="sc">-</span> alpha <span class="sc">*</span> dw</span>
<span id="cb3-33"><a href="#cb3-33" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb3-34"><a href="#cb3-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-35"><a href="#cb3-35" aria-hidden="true" tabindex="-1"></a><span class="do">## update the weights of a neural network</span></span>
<span id="cb3-36"><a href="#cb3-36" aria-hidden="true" tabindex="-1"></a>update_w <span class="ot">&lt;-</span> <span class="cf">function</span>(W, alpha, y, X, act_hidden, act_out,</span>
<span id="cb3-37"><a href="#cb3-37" aria-hidden="true" tabindex="-1"></a>                     d1_hidden, d1_out, n) {</span>
<span id="cb3-38"><a href="#cb3-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-39"><a href="#cb3-39" aria-hidden="true" tabindex="-1"></a>    grad_w <span class="ot">&lt;-</span> <span class="fu">compute_grad</span>(y, X, W, act_hidden, act_out,</span>
<span id="cb3-40"><a href="#cb3-40" aria-hidden="true" tabindex="-1"></a>                           d1_hidden, d1_out, n)</span>
<span id="cb3-41"><a href="#cb3-41" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb3-42"><a href="#cb3-42" aria-hidden="true" tabindex="-1"></a>    <span class="fu">return</span>( <span class="fu">Map</span>(<span class="at">f =</span> update_aux, <span class="at">w =</span> W,</span>
<span id="cb3-43"><a href="#cb3-43" aria-hidden="true" tabindex="-1"></a>                <span class="at">dw =</span> grad_w, <span class="at">alpha =</span> alpha) )</span>
<span id="cb3-44"><a href="#cb3-44" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Finally, all these functions previously describer are used to build the <code>fit_nn</code> function (which is used to compute the optimal weights for the neural network). The <code>alpha</code> is the <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Î±</mi><annotation encoding="application/x-tex">\alpha</annotation></semantics></math> previously mentioned in this post, <code>maxit</code> and <code>eps</code> are parameters used in the optimization process. The first one stands for the maximum number of iterations to be used in the optimization process, while the second stand for the â€œoptimization errorâ€. That is, if, from one iteration to another, the change between the weights does not exceed <code>eps</code>, then we consider that the algorithm converged and a (maybe local) optimum has been found.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>fit_nn <span class="ot">&lt;-</span> <span class="cf">function</span>(y, X, hid_units,</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>                   act_hidden, act_out,</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>                   d1_hidden, d1_out,</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>                   <span class="at">alpha =</span> .<span class="dv">25</span>,</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>                   <span class="at">maxit =</span> <span class="dv">500</span>L,</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>                   <span class="at">eps   =</span> <span class="fl">1e-05</span>) {</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>    m <span class="ot">&lt;-</span> hid_units</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>    p <span class="ot">&lt;-</span> <span class="fu">ncol</span>(X)</span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>    N <span class="ot">&lt;-</span> <span class="fu">NROW</span>(y)</span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a>    W <span class="ot">&lt;-</span> <span class="fu">list</span>(<span class="fu">matrix</span>(<span class="fu">runif</span>(m <span class="sc">*</span> p, <span class="sc">-</span>.<span class="dv">75</span>, .<span class="dv">75</span>),</span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a>                     <span class="at">ncol =</span> m, <span class="at">nrow =</span> p),</span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a>              <span class="fu">matrix</span>(<span class="fu">runif</span>(m, <span class="sc">-</span>.<span class="dv">75</span>, .<span class="dv">75</span>), <span class="at">ncol =</span> <span class="dv">1</span>))</span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a>    nn   <span class="ot">&lt;-</span> <span class="fu">vector</span>(<span class="at">mode =</span> <span class="st">"list"</span>, <span class="at">length =</span> maxit)</span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a>    cost <span class="ot">&lt;-</span> <span class="fu">vector</span>(<span class="at">mode =</span> <span class="st">"numeric"</span>, <span class="at">length =</span> maxit)</span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a>    <span class="do">## initialiazing</span></span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a>    nn[[<span class="dv">1</span>]] <span class="ot">&lt;-</span> <span class="fu">compute_nn</span>(X, W, act_hidden, act_out)</span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-22"><a href="#cb4-22" aria-hidden="true" tabindex="-1"></a>    cost[<span class="dv">1</span>] <span class="ot">&lt;-</span> <span class="fu">cost_f</span>(y, nn[[<span class="dv">1</span>]]<span class="sc">$</span>y)</span>
<span id="cb4-23"><a href="#cb4-23" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb4-24"><a href="#cb4-24" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span>( i <span class="cf">in</span> <span class="fu">seq_len</span>(maxit)[<span class="sc">-</span><span class="dv">1</span>] ) {</span>
<span id="cb4-25"><a href="#cb4-25" aria-hidden="true" tabindex="-1"></a>        W <span class="ot">&lt;-</span> <span class="fu">update_w</span>(W, alpha, y, X,</span>
<span id="cb4-26"><a href="#cb4-26" aria-hidden="true" tabindex="-1"></a>                      act_hidden, act_out,</span>
<span id="cb4-27"><a href="#cb4-27" aria-hidden="true" tabindex="-1"></a>                      d1_hidden, d1_out,</span>
<span id="cb4-28"><a href="#cb4-28" aria-hidden="true" tabindex="-1"></a>                      <span class="at">n =</span> N)</span>
<span id="cb4-29"><a href="#cb4-29" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb4-30"><a href="#cb4-30" aria-hidden="true" tabindex="-1"></a>        nn[[i]] <span class="ot">&lt;-</span> <span class="fu">compute_nn</span>(X, W, act_hidden, act_out)</span>
<span id="cb4-31"><a href="#cb4-31" aria-hidden="true" tabindex="-1"></a>        cost[i] <span class="ot">&lt;-</span> <span class="fu">cost_f</span>(y, nn[[i]]<span class="sc">$</span>y)</span>
<span id="cb4-32"><a href="#cb4-32" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb4-33"><a href="#cb4-33" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span>( <span class="fu">abs</span>(cost[i] <span class="sc">-</span> cost[i <span class="sc">-</span> <span class="dv">1</span>]) <span class="sc">&lt;</span> eps ) {</span>
<span id="cb4-34"><a href="#cb4-34" aria-hidden="true" tabindex="-1"></a>            output <span class="ot">&lt;-</span> <span class="fu">list</span>(<span class="at">nn   =</span> nn[[i <span class="sc">-</span> <span class="dv">1</span>]],</span>
<span id="cb4-35"><a href="#cb4-35" aria-hidden="true" tabindex="-1"></a>                           <span class="at">cost =</span> cost[<span class="dv">1</span><span class="sc">:</span>(i <span class="sc">-</span> <span class="dv">1</span>)],</span>
<span id="cb4-36"><a href="#cb4-36" aria-hidden="true" tabindex="-1"></a>                           <span class="at">W    =</span> W,</span>
<span id="cb4-37"><a href="#cb4-37" aria-hidden="true" tabindex="-1"></a>                           <span class="at">it   =</span> (i <span class="sc">-</span> <span class="dv">1</span>),</span>
<span id="cb4-38"><a href="#cb4-38" aria-hidden="true" tabindex="-1"></a>                           <span class="at">conv =</span> <span class="st">"yes"</span>)</span>
<span id="cb4-39"><a href="#cb4-39" aria-hidden="true" tabindex="-1"></a>            <span class="cf">break</span></span>
<span id="cb4-40"><a href="#cb4-40" aria-hidden="true" tabindex="-1"></a>        }</span>
<span id="cb4-41"><a href="#cb4-41" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb4-42"><a href="#cb4-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-43"><a href="#cb4-43" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span>( i <span class="sc">==</span> maxit ) {</span>
<span id="cb4-44"><a href="#cb4-44" aria-hidden="true" tabindex="-1"></a>        output <span class="ot">&lt;-</span> <span class="fu">list</span>(<span class="at">yhat =</span> nn[[maxit]]<span class="sc">$</span>y,</span>
<span id="cb4-45"><a href="#cb4-45" aria-hidden="true" tabindex="-1"></a>                       <span class="at">cost =</span> cost[<span class="dv">1</span><span class="sc">:</span>maxit],</span>
<span id="cb4-46"><a href="#cb4-46" aria-hidden="true" tabindex="-1"></a>                       <span class="at">W    =</span> W,</span>
<span id="cb4-47"><a href="#cb4-47" aria-hidden="true" tabindex="-1"></a>                       <span class="at">it   =</span> maxit,</span>
<span id="cb4-48"><a href="#cb4-48" aria-hidden="true" tabindex="-1"></a>                       <span class="at">conv =</span> <span class="st">"no"</span>)</span>
<span id="cb4-49"><a href="#cb4-49" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb4-50"><a href="#cb4-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-51"><a href="#cb4-51" aria-hidden="true" tabindex="-1"></a>    <span class="fu">return</span>(output)</span>
<span id="cb4-52"><a href="#cb4-52" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Having all these functions, we can play with some numerical examples!</p>
</section>
<section id="sec:ne" class="level2">
<h2 class="anchored" data-anchor-id="sec:ne">Numerical Examples</h2>
<section id="example-1-equivalence-between-neural-network-and-linear-model" class="level3">
<h3 class="anchored" data-anchor-id="example-1-equivalence-between-neural-network-and-linear-model">Example 1: Equivalence between Neural Network and Linear Model</h3>
<p>Consider a simulated dataset where <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>ğ²</mi><mo>âˆ¼</mo><mi>N</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>ğ—</mi><mi>ğ›ƒ</mi><mo>,</mo><msup><mi>Ïƒ</mi><mn>2</mn></msup><msub><mi>ğˆ</mi><mi>n</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">
\mathbf{y} \sim N(\mathbf{X} \boldsymbol{\beta}, \sigma^2 \mathbf{I}_n),
</annotation></semantics></math> where <span class="math inline">$\mathbf{X} \in {\rm I\!R}^{n \times 3}$</span>, with the first column being the intercept term. To simulate the model we used <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>ğ›ƒ</mi><mo>=</mo><mrow><mo stretchy="true" form="prefix">(</mo><mn>2</mn><mo>,</mo><mn>3</mn><mo>,</mo><mn>1.5</mn><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\boldsymbol{\beta} = (2, 3,
1.5)</annotation></semantics></math>. Additionally, suppose <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mo>=</mo><mn>2000</mn></mrow><annotation encoding="application/x-tex">n = 2000</annotation></semantics></math>.</p>
<p>Considering the identity function as the activation function for both layers, the goal here is to show that the <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>ğ–</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow></msup><msup><mi>ğ–</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>2</mn><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo>=</mo><mover><mi>ğ›ƒ</mi><mo accent="true">Ì‚</mo></mover></mrow><annotation encoding="application/x-tex">\mathbf{W}^{(1)} \mathbf{W}^{(2)} =
\hat{\boldsymbol{\beta}}</annotation></semantics></math>, where <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mover><mi>ğ›ƒ</mi><mo accent="true">Ì‚</mo></mover><annotation encoding="application/x-tex">\hat{\boldsymbol{\beta}}</annotation></semantics></math> is the least squares solution for a linear model established as <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>ğ²</mi><mo>=</mo><mi>ğ—</mi><mi>ğ›ƒ</mi></mrow><annotation encoding="application/x-tex">\mathbf{y} = \mathbf{X}
\boldsymbol{\beta}</annotation></semantics></math>, and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>ğ–</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo>,</mo><mspace width="0.167em"></mspace><msup><mi>ğ–</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>2</mn><mo stretchy="true" form="postfix">)</mo></mrow></msup></mrow><annotation encoding="application/x-tex">\mathbf{W}^{(1)}, \, \mathbf{W}^{(2)}</annotation></semantics></math> are the optimal weights according to the Neural Network fitted to the data, as proved in the subsection @ref(subsec:act).</p>
<p><a href="#tbl-ls" class="quarto-xref">Table&nbsp;1</a> displays the results from the simulated example. The two different approaches have yielded the exactly same results. If we were to make predictions, the two methods would provide the same predicted values under these circumstances.</p>
<div class="cell">
<div id="tbl-ls" class="cell quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-ls-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;1: Comparing the LS solution and the product of the neural network weight matrices.
</figcaption>
<div aria-describedby="tbl-ls-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell-output-display">
<table class="do-not-create-environment cell caption-top table table-sm table-striped small">
<thead>
<tr class="header">
<th style="text-align: center;"><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mover><mi>ğ›ƒ</mi><mo accent="true">Ì‚</mo></mover><annotation encoding="application/x-tex">\hat{\boldsymbol{\beta}}</annotation></semantics></math></th>
<th style="text-align: center;"><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>ğ–</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow></msup><msup><mi>ğ–</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>2</mn><mo stretchy="true" form="postfix">)</mo></mrow></msup></mrow><annotation encoding="application/x-tex">\mathbf{W}^{(1)} \mathbf{W}^{(2)}</annotation></semantics></math></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">2.996</td>
<td style="text-align: center;">2.996</td>
</tr>
<tr class="even">
<td style="text-align: center;">3.004</td>
<td style="text-align: center;">3.004</td>
</tr>
<tr class="odd">
<td style="text-align: center;">1.512</td>
<td style="text-align: center;">1.512</td>
</tr>
</tbody>
</table>
</div>
</div>
</figure>
</div>
</div>
<p>See below the code used on this example.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="do">##--- numerical examples ----</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a><span class="do">##--- example 1 ----</span></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">2000</span></span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>x1 <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(n)</span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>x2 <span class="ot">&lt;-</span> <span class="fu">as.numeric</span>( <span class="fu">scale</span>( <span class="fu">rexp</span>(n) ) )</span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> <span class="dv">3</span> <span class="sc">+</span> <span class="dv">3</span> <span class="sc">*</span> x1 <span class="sc">+</span> <span class="fl">1.5</span> <span class="sc">*</span> x2 <span class="sc">+</span> <span class="fu">rnorm</span>(n, <span class="at">sd =</span> .<span class="dv">5</span>)</span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a>my_x <span class="ot">&lt;-</span> <span class="fu">cbind</span>( <span class="fu">rep</span>(<span class="dv">1</span>, n), x1, x2 )</span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a><span class="fu">colnames</span>(my_x) <span class="ot">&lt;-</span> <span class="cn">NULL</span></span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a>dt <span class="ot">&lt;-</span> <span class="fu">as.data.frame</span>( <span class="fu">cbind</span>(y, my_x[, <span class="dv">2</span><span class="sc">:</span><span class="dv">3</span>]) )</span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a><span class="fu">names</span>(dt) <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">"y"</span>, <span class="st">"x1"</span>, <span class="st">"x2"</span>)</span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-20"><a href="#cb5-20" aria-hidden="true" tabindex="-1"></a>m <span class="ot">&lt;-</span> <span class="dv">6</span></span>
<span id="cb5-21"><a href="#cb5-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-22"><a href="#cb5-22" aria-hidden="true" tabindex="-1"></a>fit_1 <span class="ot">&lt;-</span></span>
<span id="cb5-23"><a href="#cb5-23" aria-hidden="true" tabindex="-1"></a>    <span class="fu">fit_nn</span>(<span class="at">y =</span> y, <span class="at">X =</span> my_x,</span>
<span id="cb5-24"><a href="#cb5-24" aria-hidden="true" tabindex="-1"></a>           <span class="at">hid_units =</span> m,</span>
<span id="cb5-25"><a href="#cb5-25" aria-hidden="true" tabindex="-1"></a>           <span class="at">act_hidden =</span> identity,</span>
<span id="cb5-26"><a href="#cb5-26" aria-hidden="true" tabindex="-1"></a>           <span class="at">act_out    =</span> identity,</span>
<span id="cb5-27"><a href="#cb5-27" aria-hidden="true" tabindex="-1"></a>           <span class="at">d1_hidden  =</span> d_ident,</span>
<span id="cb5-28"><a href="#cb5-28" aria-hidden="true" tabindex="-1"></a>           <span class="at">d1_out     =</span> d_ident,</span>
<span id="cb5-29"><a href="#cb5-29" aria-hidden="true" tabindex="-1"></a>           <span class="at">alpha =</span> .<span class="dv">05</span>,</span>
<span id="cb5-30"><a href="#cb5-30" aria-hidden="true" tabindex="-1"></a>           <span class="at">maxit =</span> <span class="dv">1000</span>L,</span>
<span id="cb5-31"><a href="#cb5-31" aria-hidden="true" tabindex="-1"></a>           <span class="at">eps   =</span> <span class="fl">1e-16</span>)</span>
<span id="cb5-32"><a href="#cb5-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-33"><a href="#cb5-33" aria-hidden="true" tabindex="-1"></a>beta_hat <span class="ot">&lt;-</span> <span class="fu">coef</span>(<span class="fu">lm</span>(y <span class="sc">~</span> x1 <span class="sc">+</span> x2, <span class="at">data =</span> dt))</span>
<span id="cb5-34"><a href="#cb5-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-35"><a href="#cb5-35" aria-hidden="true" tabindex="-1"></a>tbl_1 <span class="ot">&lt;-</span> <span class="fu">as.data.frame</span>(<span class="fu">cbind</span>(beta_hat,</span>
<span id="cb5-36"><a href="#cb5-36" aria-hidden="true" tabindex="-1"></a>                             fit_1<span class="sc">$</span>W[[<span class="dv">1</span>]] <span class="sc">%*%</span> fit_1<span class="sc">$</span>W[[<span class="dv">2</span>]]))</span>
<span id="cb5-37"><a href="#cb5-37" aria-hidden="true" tabindex="-1"></a><span class="fu">names</span>(tbl_1) <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">"$</span><span class="sc">\\</span><span class="st">hat{</span><span class="sc">\\</span><span class="st">boldsymbol{</span><span class="sc">\\</span><span class="st">beta}}$"</span>,</span>
<span id="cb5-38"><a href="#cb5-38" aria-hidden="true" tabindex="-1"></a>                  <span class="st">"$</span><span class="sc">\\</span><span class="st">mathbf{W}^{(1)} </span><span class="sc">\\</span><span class="st">mathbf{W}^{(2)}$"</span>)</span>
<span id="cb5-39"><a href="#cb5-39" aria-hidden="true" tabindex="-1"></a><span class="fu">rownames</span>(tbl_1) <span class="ot">&lt;-</span> <span class="cn">NULL</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="example-2-nonlinear-relationship-and-number-of-hidden-units" class="level3">
<h3 class="anchored" data-anchor-id="example-2-nonlinear-relationship-and-number-of-hidden-units">Example 2: Nonlinear relationship and number of hidden units</h3>
<p>Consider now the following model <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>y</mi><mi>i</mi></msub><mo>=</mo><msub><mi>Î²</mi><mn>0</mn></msub><mo>+</mo><msub><mi>Î²</mi><mn>1</mn></msub><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>x</mi><mn>2</mn></msup><mo stretchy="true" form="postfix">)</mo></mrow><mo>+</mo><msub><mi>Îµ</mi><mi>i</mi></msub><mi>.</mi></mrow><annotation encoding="application/x-tex">
y_i = \beta_0 + \beta_1 (x^2) + \varepsilon_i.
</annotation></semantics></math></p>
<p>In practice, we do not know before-hand the relationship between the response and explanatory variables is not linear. In fig-fit-nn2, we show the fitted curves the linear model and for neural networks under different settings for a dataset simulated from this example. The Neural Network deals nicely with the nonlinearity at the cost of possibly overfit the data.</p>
<div class="cell">
<div class="cell-output-display">
<div id="fig-fit-nn2" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-fit-nn2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="2021-06-23-lmnnet_files/figure-html/fig-fit-nn2-1.png" class="img-fluid figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-fit-nn2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4: Different models fitted to the same simulated dataset.
</figcaption>
</figure>
</div>
</div>
</div>
<p>See the code used on this example below.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="do">##--- example 2 ----</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">124</span>)</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>x12 <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(n)</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>y2 <span class="ot">&lt;-</span> <span class="dv">5</span> <span class="sc">-</span> <span class="fl">2.5</span> <span class="sc">*</span> (x12<span class="sc">^</span><span class="dv">2</span>) <span class="sc">+</span> <span class="fu">rnorm</span>(n, <span class="at">sd =</span> .<span class="dv">5</span>)</span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>my_x2 <span class="ot">&lt;-</span> <span class="fu">cbind</span>(<span class="fu">rep</span>(<span class="dv">1</span>, n), x12)</span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a><span class="fu">colnames</span>(my_x2) <span class="ot">&lt;-</span> <span class="cn">NULL</span></span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a>dt2 <span class="ot">&lt;-</span> <span class="fu">as.data.frame</span>( <span class="fu">cbind</span>(y2, my_x2[, <span class="dv">2</span>]) )</span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a><span class="fu">names</span>(dt2) <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">"y"</span>, <span class="st">"x1"</span>)</span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a>n_pred <span class="ot">&lt;-</span> <span class="dv">4000</span></span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-17"><a href="#cb6-17" aria-hidden="true" tabindex="-1"></a><span class="do">## fitting linear model</span></span>
<span id="cb6-18"><a href="#cb6-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-19"><a href="#cb6-19" aria-hidden="true" tabindex="-1"></a>my_lm2 <span class="ot">&lt;-</span> <span class="fu">lm</span>(y <span class="sc">~</span> x1, <span class="at">data =</span> dt2)</span>
<span id="cb6-20"><a href="#cb6-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-21"><a href="#cb6-21" aria-hidden="true" tabindex="-1"></a><span class="do">## fitting neural network with tanh as the activation function for the hidden</span></span>
<span id="cb6-22"><a href="#cb6-22" aria-hidden="true" tabindex="-1"></a><span class="do">## layer - 5 hidden units</span></span>
<span id="cb6-23"><a href="#cb6-23" aria-hidden="true" tabindex="-1"></a>fit_2 <span class="ot">&lt;-</span></span>
<span id="cb6-24"><a href="#cb6-24" aria-hidden="true" tabindex="-1"></a>    <span class="fu">fit_nn</span>(<span class="at">y =</span> y2, <span class="at">X =</span> my_x2,</span>
<span id="cb6-25"><a href="#cb6-25" aria-hidden="true" tabindex="-1"></a>           <span class="at">hid_units  =</span> <span class="dv">5</span>,</span>
<span id="cb6-26"><a href="#cb6-26" aria-hidden="true" tabindex="-1"></a>           <span class="at">act_hidden =</span> tanh,</span>
<span id="cb6-27"><a href="#cb6-27" aria-hidden="true" tabindex="-1"></a>           <span class="at">act_out    =</span> identity,</span>
<span id="cb6-28"><a href="#cb6-28" aria-hidden="true" tabindex="-1"></a>           <span class="at">d1_hidden  =</span> d_tanh,</span>
<span id="cb6-29"><a href="#cb6-29" aria-hidden="true" tabindex="-1"></a>           <span class="at">d1_out     =</span> d_ident,</span>
<span id="cb6-30"><a href="#cb6-30" aria-hidden="true" tabindex="-1"></a>           <span class="at">alpha =</span> .<span class="dv">05</span>,</span>
<span id="cb6-31"><a href="#cb6-31" aria-hidden="true" tabindex="-1"></a>           <span class="at">maxit =</span> <span class="dv">1000</span>L,</span>
<span id="cb6-32"><a href="#cb6-32" aria-hidden="true" tabindex="-1"></a>           <span class="at">eps   =</span> <span class="fl">1e-04</span>)</span>
<span id="cb6-33"><a href="#cb6-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-34"><a href="#cb6-34" aria-hidden="true" tabindex="-1"></a><span class="do">## fitting neural network with tanh as the activation function for the hidden</span></span>
<span id="cb6-35"><a href="#cb6-35" aria-hidden="true" tabindex="-1"></a><span class="do">## layer - 15 hidden units</span></span>
<span id="cb6-36"><a href="#cb6-36" aria-hidden="true" tabindex="-1"></a>fit_3 <span class="ot">&lt;-</span></span>
<span id="cb6-37"><a href="#cb6-37" aria-hidden="true" tabindex="-1"></a>    <span class="fu">fit_nn</span>(<span class="at">y =</span> y2, <span class="at">X =</span> my_x2,</span>
<span id="cb6-38"><a href="#cb6-38" aria-hidden="true" tabindex="-1"></a>           <span class="at">hid_units  =</span> <span class="dv">15</span>,</span>
<span id="cb6-39"><a href="#cb6-39" aria-hidden="true" tabindex="-1"></a>           <span class="at">act_hidden =</span> tanh,</span>
<span id="cb6-40"><a href="#cb6-40" aria-hidden="true" tabindex="-1"></a>           <span class="at">act_out    =</span> identity,</span>
<span id="cb6-41"><a href="#cb6-41" aria-hidden="true" tabindex="-1"></a>           <span class="at">d1_hidden  =</span> d_tanh,</span>
<span id="cb6-42"><a href="#cb6-42" aria-hidden="true" tabindex="-1"></a>           <span class="at">d1_out     =</span> d_ident,</span>
<span id="cb6-43"><a href="#cb6-43" aria-hidden="true" tabindex="-1"></a>           <span class="at">alpha =</span> .<span class="dv">05</span>,</span>
<span id="cb6-44"><a href="#cb6-44" aria-hidden="true" tabindex="-1"></a>           <span class="at">maxit =</span> <span class="dv">1000</span>L,</span>
<span id="cb6-45"><a href="#cb6-45" aria-hidden="true" tabindex="-1"></a>           <span class="at">eps   =</span> <span class="fl">1e-04</span>)</span>
<span id="cb6-46"><a href="#cb6-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-47"><a href="#cb6-47" aria-hidden="true" tabindex="-1"></a><span class="do">## fitting neural network with leaky ReLU as the activation function for the</span></span>
<span id="cb6-48"><a href="#cb6-48" aria-hidden="true" tabindex="-1"></a><span class="do">## hidden layer - 10 hidden units</span></span>
<span id="cb6-49"><a href="#cb6-49" aria-hidden="true" tabindex="-1"></a>fit_4 <span class="ot">&lt;-</span></span>
<span id="cb6-50"><a href="#cb6-50" aria-hidden="true" tabindex="-1"></a>    <span class="fu">fit_nn</span>(<span class="at">y =</span> y2, <span class="at">X =</span> my_x2,</span>
<span id="cb6-51"><a href="#cb6-51" aria-hidden="true" tabindex="-1"></a>           <span class="at">hid_units  =</span> <span class="dv">10</span>,</span>
<span id="cb6-52"><a href="#cb6-52" aria-hidden="true" tabindex="-1"></a>           <span class="at">act_hidden =</span> lrelu,</span>
<span id="cb6-53"><a href="#cb6-53" aria-hidden="true" tabindex="-1"></a>           <span class="at">act_out    =</span> identity,</span>
<span id="cb6-54"><a href="#cb6-54" aria-hidden="true" tabindex="-1"></a>           <span class="at">d1_hidden  =</span> d_lrelu,</span>
<span id="cb6-55"><a href="#cb6-55" aria-hidden="true" tabindex="-1"></a>           <span class="at">d1_out     =</span> d_ident,</span>
<span id="cb6-56"><a href="#cb6-56" aria-hidden="true" tabindex="-1"></a>           <span class="at">alpha =</span> .<span class="dv">05</span>,</span>
<span id="cb6-57"><a href="#cb6-57" aria-hidden="true" tabindex="-1"></a>           <span class="at">maxit =</span> <span class="dv">1000</span>L,</span>
<span id="cb6-58"><a href="#cb6-58" aria-hidden="true" tabindex="-1"></a>           <span class="at">eps   =</span> <span class="fl">1e-04</span>)</span>
<span id="cb6-59"><a href="#cb6-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-60"><a href="#cb6-60" aria-hidden="true" tabindex="-1"></a>pred_data <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">x =</span> <span class="fu">seq</span>(<span class="at">from =</span> <span class="fu">min</span>(x12), <span class="at">to =</span> <span class="fu">max</span>(x12),</span>
<span id="cb6-61"><a href="#cb6-61" aria-hidden="true" tabindex="-1"></a>                                <span class="at">length.out =</span> n_pred))</span>
<span id="cb6-62"><a href="#cb6-62" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-63"><a href="#cb6-63" aria-hidden="true" tabindex="-1"></a>pred_data <span class="ot">&lt;-</span> <span class="fu">transform</span>(pred_data,</span>
<span id="cb6-64"><a href="#cb6-64" aria-hidden="true" tabindex="-1"></a>                       <span class="at">lm =</span> <span class="fu">coef</span>(my_lm2)[[<span class="dv">1</span>]] <span class="sc">+</span> <span class="fu">coef</span>(my_lm2)[[<span class="dv">1</span>]] <span class="sc">*</span> x)</span>
<span id="cb6-65"><a href="#cb6-65" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-66"><a href="#cb6-66" aria-hidden="true" tabindex="-1"></a>pred_data <span class="ot">&lt;-</span> <span class="fu">transform</span>(pred_data,</span>
<span id="cb6-67"><a href="#cb6-67" aria-hidden="true" tabindex="-1"></a>                       <span class="at">nn_tanh_1 =</span></span>
<span id="cb6-68"><a href="#cb6-68" aria-hidden="true" tabindex="-1"></a>                           <span class="fu">compute_nn</span>(<span class="at">X =</span> <span class="fu">cbind</span>(<span class="fu">rep</span>(<span class="dv">1</span>, <span class="dv">4000</span>),</span>
<span id="cb6-69"><a href="#cb6-69" aria-hidden="true" tabindex="-1"></a>                                                x),</span>
<span id="cb6-70"><a href="#cb6-70" aria-hidden="true" tabindex="-1"></a>                                      <span class="at">W =</span> fit_2<span class="sc">$</span>W,</span>
<span id="cb6-71"><a href="#cb6-71" aria-hidden="true" tabindex="-1"></a>                                      <span class="at">act_hidden =</span> tanh,</span>
<span id="cb6-72"><a href="#cb6-72" aria-hidden="true" tabindex="-1"></a>                                      <span class="at">act_out =</span> identity)<span class="sc">$</span>y)</span>
<span id="cb6-73"><a href="#cb6-73" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-74"><a href="#cb6-74" aria-hidden="true" tabindex="-1"></a>pred_data <span class="ot">&lt;-</span> <span class="fu">transform</span>(pred_data,</span>
<span id="cb6-75"><a href="#cb6-75" aria-hidden="true" tabindex="-1"></a>                       <span class="at">nn_tanh_2 =</span></span>
<span id="cb6-76"><a href="#cb6-76" aria-hidden="true" tabindex="-1"></a>                           <span class="fu">compute_nn</span>(<span class="at">X =</span> <span class="fu">cbind</span>(<span class="fu">rep</span>(<span class="dv">1</span>, <span class="dv">4000</span>),</span>
<span id="cb6-77"><a href="#cb6-77" aria-hidden="true" tabindex="-1"></a>                                                x),</span>
<span id="cb6-78"><a href="#cb6-78" aria-hidden="true" tabindex="-1"></a>                                      <span class="at">W =</span> fit_3<span class="sc">$</span>W,</span>
<span id="cb6-79"><a href="#cb6-79" aria-hidden="true" tabindex="-1"></a>                                      <span class="at">act_hidden =</span> tanh,</span>
<span id="cb6-80"><a href="#cb6-80" aria-hidden="true" tabindex="-1"></a>                                      <span class="at">act_out =</span> identity)<span class="sc">$</span>y)</span>
<span id="cb6-81"><a href="#cb6-81" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-82"><a href="#cb6-82" aria-hidden="true" tabindex="-1"></a>pred_data <span class="ot">&lt;-</span> <span class="fu">transform</span>(pred_data,</span>
<span id="cb6-83"><a href="#cb6-83" aria-hidden="true" tabindex="-1"></a>                       <span class="at">nn_lrelu =</span></span>
<span id="cb6-84"><a href="#cb6-84" aria-hidden="true" tabindex="-1"></a>                           <span class="fu">compute_nn</span>(<span class="at">X =</span> <span class="fu">cbind</span>(<span class="fu">rep</span>(<span class="dv">1</span>, <span class="dv">4000</span>),</span>
<span id="cb6-85"><a href="#cb6-85" aria-hidden="true" tabindex="-1"></a>                                                x),</span>
<span id="cb6-86"><a href="#cb6-86" aria-hidden="true" tabindex="-1"></a>                                      <span class="at">W =</span> fit_4<span class="sc">$</span>W,</span>
<span id="cb6-87"><a href="#cb6-87" aria-hidden="true" tabindex="-1"></a>                                      <span class="at">act_hidden =</span> lrelu,</span>
<span id="cb6-88"><a href="#cb6-88" aria-hidden="true" tabindex="-1"></a>                                      <span class="at">act_out =</span> identity)<span class="sc">$</span>y)</span>
<span id="cb6-89"><a href="#cb6-89" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-90"><a href="#cb6-90" aria-hidden="true" tabindex="-1"></a><span class="fu">setDT</span>(pred_data)</span>
<span id="cb6-91"><a href="#cb6-91" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-92"><a href="#cb6-92" aria-hidden="true" tabindex="-1"></a>pred_data <span class="ot">&lt;-</span> <span class="fu">melt</span>(pred_data, <span class="at">id =</span> <span class="dv">1</span>,</span>
<span id="cb6-93"><a href="#cb6-93" aria-hidden="true" tabindex="-1"></a>                  <span class="at">value.name =</span> <span class="st">"pred"</span>,</span>
<span id="cb6-94"><a href="#cb6-94" aria-hidden="true" tabindex="-1"></a>                  <span class="at">variable.name =</span> <span class="st">"method"</span>)</span>
<span id="cb6-95"><a href="#cb6-95" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-96"><a href="#cb6-96" aria-hidden="true" tabindex="-1"></a>pred_data[, method <span class="sc">:</span><span class="er">=</span> <span class="fu">fcase</span>(method <span class="sc">==</span> <span class="st">"nn_tanh_1"</span>, <span class="st">"tanh - 5"</span>,</span>
<span id="cb6-97"><a href="#cb6-97" aria-hidden="true" tabindex="-1"></a>                            method <span class="sc">==</span> <span class="st">"nn_tanh_2"</span>, <span class="st">"tanh - 15"</span>,</span>
<span id="cb6-98"><a href="#cb6-98" aria-hidden="true" tabindex="-1"></a>                            method <span class="sc">==</span> <span class="st">"nn_lrelu"</span>, <span class="st">"leaky ReLU - 10"</span>,</span>
<span id="cb6-99"><a href="#cb6-99" aria-hidden="true" tabindex="-1"></a>                            <span class="at">default =</span> <span class="st">"lm"</span>)]</span>
<span id="cb6-100"><a href="#cb6-100" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-101"><a href="#cb6-101" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(<span class="at">data =</span> pred_data) <span class="sc">+</span></span>
<span id="cb6-102"><a href="#cb6-102" aria-hidden="true" tabindex="-1"></a>    <span class="fu">geom_point</span>(<span class="at">data =</span> dt2, <span class="fu">aes</span>(<span class="at">x =</span> x1, <span class="at">y =</span> y),</span>
<span id="cb6-103"><a href="#cb6-103" aria-hidden="true" tabindex="-1"></a>               <span class="at">alpha =</span> .<span class="dv">5</span>) <span class="sc">+</span></span>
<span id="cb6-104"><a href="#cb6-104" aria-hidden="true" tabindex="-1"></a>    <span class="fu">geom_line</span>(<span class="fu">aes</span>(<span class="at">x =</span> x, <span class="at">y =</span> pred, <span class="at">color =</span> method),</span>
<span id="cb6-105"><a href="#cb6-105" aria-hidden="true" tabindex="-1"></a>              <span class="at">lwd =</span> <span class="fl">1.05</span>) <span class="sc">+</span></span>
<span id="cb6-106"><a href="#cb6-106" aria-hidden="true" tabindex="-1"></a>    <span class="fu">scale_color_discrete</span>(<span class="at">name =</span> <span class="cn">NULL</span>) <span class="sc">+</span></span>
<span id="cb6-107"><a href="#cb6-107" aria-hidden="true" tabindex="-1"></a>    <span class="fu">theme_bw</span>() <span class="sc">+</span></span>
<span id="cb6-108"><a href="#cb6-108" aria-hidden="true" tabindex="-1"></a>    <span class="fu">theme</span>(</span>
<span id="cb6-109"><a href="#cb6-109" aria-hidden="true" tabindex="-1"></a>        <span class="at">legend.position =</span> <span class="st">"bottom"</span>,</span>
<span id="cb6-110"><a href="#cb6-110" aria-hidden="true" tabindex="-1"></a>        <span class="at">legend.margin =</span> <span class="fu">margin</span>(<span class="dv">6</span>, <span class="dv">6</span>, <span class="dv">6</span>, <span class="dv">6</span>)</span>
<span id="cb6-111"><a href="#cb6-111" aria-hidden="true" tabindex="-1"></a>    ) <span class="sc">+</span></span>
<span id="cb6-112"><a href="#cb6-112" aria-hidden="true" tabindex="-1"></a>    <span class="fu">labs</span>(<span class="at">x =</span> <span class="st">"X"</span>, <span class="at">y =</span> <span class="st">"Y"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
</section>
<section id="final-thoughts" class="level2">
<h2 class="anchored" data-anchor-id="final-thoughts">Final Thoughts</h2>
<p>The Neural Network Regression models are very interesting but certainly are not magical as it is sold in the market. By the end of the day, these models consist of simple linear algebra allied to the use of element-wise nonlinear functions and optimization algorithms. Speaking on optimization algorithm, the gradient descent looks like a fixed-point iteration algorithm. These kind of algorithms have the advantage of not need the second derivative of the functions, however their convergence can be slow. I believe that using different learning rates for different parameters could improve the speed on which the algorithm converges.</p>
<p>Although these models do not make any distributional assumption on the data, we can easily make it more suitable for certain distributions by working with the cost and activation functions on an appropriate fashion.</p>
<p>There are several variants of these models suited for different problems, like text and image classification, for example. The idea is the same, what changes is the way the researchers deal with the hidden layers. I think an interesting application is to try to use neural networks to estimate non-parametrically covariance matrices for spatial data.</p>
</section>
<section id="references" class="level1">




</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-cheng1994neural" class="csl-entry" role="listitem">
Cheng, Bing, and D Michael Titterington. 1994. <span>â€œNeural Networks: A Review from a Statistical Perspective.â€</span> <em>Statistical Science</em>, 2â€“30.
</div>
<div id="ref-efron2020prediction" class="csl-entry" role="listitem">
Efron, Bradley. 2020. <span>â€œPrediction, Estimation, and Attribution.â€</span> <em>Journal of the American Statistical Association</em> 115 (530): 636â€“55.
</div>
<div id="ref-efron2016computer" class="csl-entry" role="listitem">
Efron, Bradley, and Trevor Hastie. 2016. <span>â€œNeural Networks and Deep Learning.â€</span> In <em>Computer Age Statistical Inference: Algorithms, Evidence, and Data Science</em>. Institute of Mathematical Statistics Monographs. Cambridge University Press. <a href="https://doi.org/10.1017/CBO9781316576533.019">https://doi.org/10.1017/CBO9781316576533.019</a>.
</div>
<div id="ref-hastie2009elements" class="csl-entry" role="listitem">
Hastie, Trevor, Robert Tibshirani, and Jerome Friedman. 2009. <em>The Elements of Statistical Learning: Data Mining, Inference, and Prediction</em>. Springer Science &amp; Business Media.
</div>
<div id="ref-hastie2015statistical" class="csl-entry" role="listitem">
Hastie, Trevor, Robert Tibshirani, and Martin Wainwright. 2015. <em>Statistical Learning with Sparsity: The Lasso and Generalizations</em>. CRC press.
</div>
<div id="ref-mcculloch1943logical" class="csl-entry" role="listitem">
McCulloch, W, and W Pitts. 1943. <span>â€œA Logical Calculus of the Ideas Imminent in Nervous Activity.â€</span> <em>Bulletin of Mathematical Biophisics</em> 5: 115â€“33.
</div>
<div id="ref-stern1996neural" class="csl-entry" role="listitem">
Stern, Hal S. 1996. <span>â€œNeural Networks in Applied Statistics.â€</span> <em>Technometrics</em> 38 (3): 205â€“14.
</div>
<div id="ref-warner1996understanding" class="csl-entry" role="listitem">
Warner, Brad, and Manavendra Misra. 1996. <span>â€œUnderstanding Neural Networks as Statistical Tools.â€</span> <em>The American Statistician</em> 50 (4): 284â€“93.
</div>
<div id="ref-zou2005regularization" class="csl-entry" role="listitem">
Zou, Hui, and Trevor Hastie. 2005. <span>â€œRegularization and Variable Selection via the Elastic Net.â€</span> <em>Journal of the Royal Statistical Society: Series B (Statistical Methodology)</em> 67 (2): 301â€“20.
</div>
</div></section><section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>Features are the name given for predictors in the neural networks literature<a href="#fnref1" class="footnote-back" role="doc-backlink">â†©ï¸</a></p></li>
<li id="fn2"><p>Sometimes referred to as <em>multi-layer-perceptron</em>, and <em>back-propagation</em>.<a href="#fnref2" class="footnote-back" role="doc-backlink">â†©ï¸</a></p></li>
</ol>
</section><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{godoy2021,
  author = {Godoy, Lucas},
  title = {Estimating Regression Coefficients Using a {Neural} {Network}
    (from Scratch)},
  date = {2021-07-28},
  url = {https://lcgodoy.me/posts/lmnet/2021-06-23-lmnnet.html},
  langid = {en}
}
</code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-godoy2021" class="csl-entry quarto-appendix-citeas" role="listitem">
Godoy, Lucas. 2021. <span>â€œEstimating Regression Coefficients Using a
Neural Network (from Scratch).â€</span> July 28, 2021. <a href="https://lcgodoy.me/posts/lmnet/2021-06-23-lmnnet.html">https://lcgodoy.me/posts/lmnet/2021-06-23-lmnnet.html</a>.
</div></div></section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "î§‹";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/lcgodoy\.me");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<script src="https://giscus.app/client.js" data-repo="lcgodoy/lcgodoy.github.io" data-repo-id="MDEwOlJlcG9zaXRvcnkyMTY0Njk1MTI=" data-category="General" data-category-id="DIC_kwDODOcQCM4CcNDy" data-mapping="title" data-reactions-enabled="1" data-emit-metadata="0" data-input-position="top" data-theme="light" data-lang="en" crossorigin="anonymous" async="">
</script>
<script type="application/javascript">
  const giscusIframeObserver = new MutationObserver(function (mutations) {
    mutations.forEach(function (mutation) {
      mutation.addedNodes.forEach(function (addedNode) {
        if (addedNode.matches && addedNode.matches('div.giscus')) {
          const giscusIframe = addedNode.querySelector('iframe.giscus-frame');
          if(giscusIframe) {
            giscusIframe.addEventListener("load", function() {
              window.setTimeout(() => {
                toggleGiscusIfUsed(hasAlternateSentinel(), authorPrefersDark);
              }, 100);
            });
            giscusIframeObserver.disconnect();
          }
        }
      });
    });
  });
  giscusIframeObserver.observe(document.body, { childList: true, subtree: true });
</script>
<input type="hidden" id="giscus-base-theme" value="light">
<input type="hidden" id="giscus-alt-theme" value="dark">
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p>Copyright 2025, Lucas Godoy</p>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    <div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/lcgodoy/lcgodoy.github.io/issues/new" class="toc-action"><i class="bi bi-github"></i>Report an issue</a></li></ul></div></div>
    <div class="nav-footer-right">
      &nbsp;
    </div>
  </div>
</footer>




</body></html>