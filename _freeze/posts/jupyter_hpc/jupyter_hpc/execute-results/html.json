{
  "hash": "b5ef2ca91ea62e0072930d6e2804209b",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: Using jupyter on an HPC cluster\nauthor: Lucas Godoy\ndate: '2022-06-09'\nexecute:\n  echo: true\n  eval: false\nslug: jupyter_hpc\ncategories:\n  - Computing\ntags: []\nsubtitle: ''\nsummary: |\n  A short tutorial on how to execute jupyter notebooks on HPC clusters.\nlastmod: '2025-07-29'\n---\n\n\n::: {.cell}\n\n:::\n\n\n> This post was originally written for the [CBC-UCONN HC\n> wiki](https://github.com/CBC-UCONN/software-example-guide/wiki)\n\n\nThe purpose of this post is to enable cluster users to run `jupyter` on the\ncluster interactively, enabling them to conduct data analysis and\nvisualization. There are different \"flavors\" of `jupyter` notebooks, the most\nappropriate are going to be pointed out at [picking a container](#pick-cont).\n\n\n## Preliminaries\n\n\nWe assume that the user has access to `ssh` through a `terminal`. In addition,\nit is necessary to have [SingularityCE](https://sylabs.io/singularity/)\n(`singularity` for short) installed on a computer on which you have `root`\nprivileges.\n\n> In order to use `singularity` on a Windows (or Mac) machine, a Linux _Virtual\n> Machine_(VM) needs to be set up. Setting up a VM and installing SingularityCE\n> os beyond the scope of this document.\n\nIn addition, `singularity` is assumed to be available on the HPC that you have\naccess to. Usually, users have to run\n\n\n::: {.cell}\n\n```{.bash .cell-code}\nmodule load singularity/<version>\n```\n:::\n\n\nbefore using it.\n\n\nFamiliarity with containers is helpful but not necessary. Loosely speaking, a\ncontainer allows us to \"isolate\" a set of tools and software in order to\nguarantee code reproducibility and portability. Moreover, `singularity` was\ndeveloped (among other reasons) to integrate these tools with HPC clusters.\n\n\n## Picking a container {#pick-cont}\n\n\nThe [Jupyter Docker\nStacks](https://jupyter-docker-stacks.readthedocs.io/en/latest/index.html)\ncontains several useful `docker` containers that can be easily used to build\n`singularity` containers.\n\n\nA list of the containers (along with their principal features) maintained by the\n[Jupyter](https://jupyter.org/about) team can be found\n[here](https://jupyter-docker-stacks.readthedocs.io/en/latest/using/selecting.html).\nSome of these containers are detailed below\n\n* `jupyter/r-notebook`: a container containing a basic installation for Machine\n  Learning using `R`.\n* `jupyter/scipy-notebook`: contains popular libraries for scientific computing\n  using `python`.\n* `jupyter/tensorflow-notebook`: this is the `jupyter/scipy-notebook` with\n  `tensorflow` installed on it.\n* `jupyter/datascience-notebook`: includes libraries for data analysis from the\n  `julia`, `python, and `R` communities.\n  \n\n## Converting a `docker` into a `singularity` container\n\n\nOnce you have chosen a container suitable for your needs (and have root access\nto a machine with `singularity`), a `singularity` container can be generated by\nexecuting the following chunk of code in the terminal.\n\n\n::: {.cell}\n\n```{.bash .cell-code}\n## singularity pull <choose-a-name>.sif docker://jupyter/<preferred-notebook>\nsingularity pull mycontainer.sif docker://jupyter/datascience-notebook\n```\n:::\n\n\nIn the example above, I choose to use the `datascience-notebook`. After doing\nso, the `.sif` file generated by singularity needs to be transferred to the\ncluster.  My personal preference is to use either `scp` or `rsync`, for example\n\n\n::: {.cell}\n\n```{.bash .cell-code}\nrsync mycontainer.sif <username>@<hpc-url>:<location>\n```\n:::\n\n\n\n## Using the `singularity` container on the cluster\n\n\nAfter transferring the `.sif` file to the cluster, follow the following\nsteps. Firstly, set up the VPN and log-in to the cluster using `ssh`, then\nnavigate to the location where you transferred the container (`.sif`) to. Next,\nyou will have to start a interactive job. If the workload manager used in the\nHPC that you have access to is [SLURM](https://slurm.schedmd.com/overview.html),\nthis can be done either with `srun` or `fisbatch`. To start an interactive job\nwith `srun` use\n\n\n::: {.cell}\n\n```{.bash .cell-code}\nsrun --partition=<partition-name> --qos=<queue-name> --mem=64G --pty bash\n```\n:::\n\n\nThe same task can be achieved with `fisbatch` (if available) with\n\n\n::: {.cell}\n\n```{.bash .cell-code}\nfisbatch --partition=<partition-name> --qos=<queue-name> --mem=64G\n```\n:::\n\n\nEither of these commands will allocate your job to a specific node. It is\n**important to save the name of the node** that your job has been allocated\nto. Next, load `singularity` on that node as follows\n\n\n::: {.cell}\n\n```{.bash .cell-code}\nmodule load singularity/<version>\n```\n:::\n\n\nThe penultimate step is to start the `jupyter` instance. It is done as follows\n\n\n::: {.cell}\n\n```{.bash .cell-code}\nsingularity exec --nv mycontainer.sif jupyter notebook --no-browser --ip='*'\n```\n:::\n\n\nAfter executing the last chunk of code, the terminal will be \"busy\" and will\nprovide three URLs, they will look somewhat like \"http://127.0.0.1:8888/\"\nthis. Copy the last address provided by the output. The last step before being\nable to access the notebook through the provided address is to create a `ssh`\ntunnel. To do so, open another terminal window and execute\n\n\n::: {.cell}\n\n```{.bash .cell-code}\nssh -NL localhost:8888:<node>:8888 <username>@<hpc-url>\n```\n:::\n\n\nwhere `<node>` should be replaced by the node to which the job submitted using\n`srun` (or `fisbatch`) was submitted to. This tunnel will keep the other\nterminal window busy to.\n\nFinally, copy the address provided by the notebook (e.g.,\n\"http://127.0.0.1:8888/\") and paste it into your browser.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}