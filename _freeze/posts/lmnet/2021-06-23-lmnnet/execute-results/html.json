{
  "hash": "da3e749d33acfd27a67ed6fba443532e",
  "result": {
    "markdown": "---\ntitle: Estimating regression coefficients using a Neural Network (from scratch)\nauthor: Lucas Godoy\nexecute:\n  echo: false\ndate: '2021-07-28'\nslug: lmnnet\ncategories:\n  - statistics\n  - neural networks\nsummary: |\n  Despite the amount of “fancy” terms, Neural Network literature shares several\n  elements with the statistical literature. This post aims to provide a gentle\n  introduction of Neural Network Regression for statisticians. Additionally, we\n  elucidade how the famous Backpropagation algorithm is used to estimate the\n  parameters associated with a Neural Network model\nlastmod: '2023-02-20'\nbibliography: ref-nnet.bib\nbiblio-style: asa\n---\n\n::: {.cell}\n\n:::\n\n\n## Intro\n\nThe idea behind the Neural Networks models, as its nomenclature suggests, is to\nmimic the way human brain learns to execute of some tasks. Some works in the\nliterature (@cheng1994neural, @stern1996neural, @warner1996understanding)\nattribute of the first attempts to build a \"Neural Network emulator\" to\n@mcculloch1943logical. The popularity of this method in the past decades was\nheld down by the computation intensive calculations needed for such\nprocedures. However, the computation resources advances in the last few years\nallied to the algorithmic nature of Neural Networks have contributed to the\nadoption of the methodology by computer scientists. These days, this models are\nvery popular in the industry and are applied to several interesting applications\nsuch as speech recognition, image classification, and automatic text\ntranslation.\n\n## Neural Network Regression\n\nA neural network is a highly parametrized model that, provided we have enough\ndata, can approximate any functional relationship between a set of\n_features_^[Features are the name given for predictors in the neural networks\nliterature] $\\mathbf{x}$ and a response variable $y$ (@efron2016computer, pages\n151-152). Although there are several possible structures for neural networks,\nfor this post we are going to consider only the _feed-forward_^[Sometimes\nreferred to as _multi-layer-perceptron_, and _back-propagation_.] neural\nnetworks. In order to explain how these neural networks are designed, let's\nconsider its graphical representation (see @fig-nn1). We have\nvertices, which are called a units (or neurons), ordered horizontally by\nlayers. An edge coming from one vertex can only be connected to vertices\nassociated with \"higher\" layers. These connections represent a information flow\nfrom left to right (hence, the name feed-forward), where each unit computed by\n1) giving weights to each of its inputs, 2) calculating the dot product between\nweights and inputs, 3) adding a constant( usually referred to as _bias_) to it,\nand, finally, 4) applying an element-wise _activation_ function $f(\\cdot)$ to\nit. These _activation functions_ are used to establish non-linear relationships\nbetween units.\n\nThe number of hidden layers as well as the number of units associated with every\nlayer can both be regard as tuning parameters. The design and architecture of a\nneural network is a complex task. In summary, when having a single hidden layer,\nthe number of units associated with the hidden layer determines the number of\nparameters associated with the model. @efron2016computer suggest that, under\nthis scenario, it is better to consider several units for the hidden layer and\nuse some kind of regularization to avoid overfitting. Penalizations analogous to\nthe Ridge and Lasso penalty for linear models are often used in the\nregularization context for neural networks (@hastie2015statistical, pages\n210-211).\n\nAn important remark regarding the neural network models is that they are \"pure\nprediction algorithms\". That is, these models are focused only on prediction,\nneglecting the estimation, as pointed by @efron2020prediction. The strategy is\nsimple and consists in searching for high predictive accuracy. That being said,\nthese algorithms make no assumption on the probability distribution of the data\nand, as one of the consequences of losing these assumptions, it is not possible\nto make interval predictions or to calculate confidence intervals for the\n\"estimated\" parameters.\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![A _feed-forward_ neural network with a single hidden layer.](2021-06-23-lmnnet_files/figure-html/fig-nn1-1.png){#fig-nn1 fig-align='center' width=672}\n:::\n:::\n\n\n### Single neuron feed-forward networks {#subsec:single}\n\nA single neuron feed-forward network does not possess any hidden layer in its\nstructure. The absence of hidden layers makes these models resemble the\nstatistical models we are most used to, like, for example, the linear regression\nand logistic regression. By analyzing the graphical representation of a single\nlayer feed-forward network (@fig-nn2), it is easy to see that by\ntaking the identity as the _activation function_, the functional relationship\nbetween $\\mathbf{x}$ and $y$ considered by the neural network is equivalent to\nthe one used for the general linear model. Considering the same representation,\nif we take $f(x) = \\textrm{logit}(x)$ (_sigmoid_ function, according to the\nneural network models literature) and $y \\in \\{ 0, 1 \\}$, then the neural\nnetwork provides the same relationship between $\\mathbf{x}$ and $y$ as the one\nused by the logistic regression.\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![A single layer _feed-forward_ neural network.](2021-06-23-lmnnet_files/figure-html/fig-nn2-1.png){#fig-nn2 fig-align='center' width=672}\n:::\n:::\n\n\nAlthough the functional relationship between $\\mathbf{x}$ and $y$ assumed by the\nsingle layer neural network coincides with some statistical models, we cannot\npromptly claim an equivalence between models because the way the neural networks\n_learn_, that is estimates, the weights can lead to different solutions\ndepending on the _loss_ and _cost_ functions selected, we are going to talk more\nabout these functions in the next section.\n\n### Activation functions {#subsec:act}\n\nActivation functions are applied in every \"Layer connection\" in neural network\nmodels. Suppose, for example, we have a design matrix $\\mathbf{X} \\in {\\rm\nI\\!R}^{n \\times p}$, and a response variable $\\mathbf{y}$. Then, given\nappropriate choices of the $K - 1$ (one for each layer connection), the\nmathematical model, for a single observation, behind the neural network, can be\nwritten in a vectorial notation as follows\n$$\n\\mathbf{z}^{(k)} = \\mathbf{W}^{(k - 1)} \\mathbf{a}^{(k - 1)}\n$$\n$$\n\\mathbf{a}^{(k)} = f_{(k)} \\left( \\mathbf{z}^{(k)} \\right),\n$$\nwhere $\\mathbf{W}^{(k - 1)} \\in {\\rm I\\!R}^{m_{k - 1} \\times m_{k}}$ is the\nmatrix of weights that go from from the layer $L_{k - 1}$ to the layer $L_{k}$,\n$\\mathbf{a}^{(k)} \\in {\\rm I\\!R}^{m_k \\times m_{k + 1}}$ matrix of units at\nlayer $L_k$, and $f_{(k)}$ is a (element-wise) activation function used at the\nlayer $L_k$. Note that, when $k = 0$, then $\\mathbf{a}^{(0)} = \\mathbf{X}$.\nObserve that $m_k$ is the number of units in the layer $k$ and, consequently,\nfor the input and output layers, respectively, we have $m_0 = p$ and $m_K = 1$.\n\nFrom this example, it is clear that we can apply different activation functions\nwhen connecting different layers. Nevertheless, the activation for one layer is\nthe same along all of its units.\n\nAlthough, theoretically, there exists no restriction on which functions to use\nas activation function, we want these functions to be at least one time\ndifferentiable. This is due to the fact that most of the methods used to find\nthe optimal weights are based on gradients. Another aspect to be considered when\nchoosing an activation function is the domain of the output variable $y$. That\nis, if $y \\in [0, 1]$, we want an activation function that maps real values to\nthe $[0, 1]$ interval. In summary, for the output layer, we use a activation\nfunction that makes predictions on the same domain as the output variable,\nwhile, for hidden layers, we have no restrictions on the activation functions,\nbesides the ones already mentioned.\n\nSome commonly used link functions are the $\\textrm{logit}$, or sigmoid,\nfunction, defined as\n$$\nf(x) = \\frac{1}{1 + e^{-x}},\n$$\nthe hyperbolic tangent function, referred to as $\\textrm{tanh}$,\n$$\nf(x) = \\frac{e^z - e^{-z}}{e^{z} + e^{-z}}.\n$$\nNote that the $\\textrm{tanh}$ is mapping from the real line to the $(-1, 1)$\ninterval.\nThe Rectified Linear Unit (ReLU) is also a popular choice and is defined as\n$$\nf(x) = x_{+} = \\max(0, x),\n$$\nthe main advantage of this function is a cheap to compute gradient. A different\nversion of the ReLU called leaky ReLU is also quite popular, its definition is\ngiven as follows\n$$\nf(x) = x_{+} = \\max(.01 * x, x),\n$$\n\nThese are only some examples of commonly used activation functions and they are\nillustrated in @fig-act-funs. The user does need to be restrict to\nthese options since there are several other functions implemented in the\nsoftware available to compute neural networks. However, if you want to use a\nactivation function that is not implemented yet, you may have to implement your\nown version for the algorithm.\n\n\n::: {.cell}\n::: {.cell-output-display}\n![The most popular activation functions (figure inspired by Figure 18.6 from @efron2016computer ).](2021-06-23-lmnnet_files/figure-html/fig-act-funs-1.png){#fig-act-funs width=672}\n:::\n:::\n\n\nAlthough there are no restrictions on the functions used as activation functions\nin the hidden layers (besides being differentiable functions), it is not\nadvisable to use the identity function because it implies a waste of\ncomputational power. This is due to the fact that using a linear function in a\nhidden layer, makes the units from that layer a linear combination of the units\nfrom the previous layer. To make this clear, let's prove that a Neural Network\nmodel with a single hidden layer collapses to a Generalized Linear Model when\nthe identity function is used as the activation function.\n\nSuppose a $n$-dimensional vector $\\mathbf{y}$ is assumed to follow a\ndistribution $\\mathcal{P}$, where $\\mathcal{P}$ belongs to the exponential\nfamily of distributions. Then, given a design matrix $\\mathbf{X} \\in\n{\\rm I\\!R}^{n \\times p}$, the Generalized Linear Model for $\\mathbf{y}$ is\ncomposed by the _random component_, given by the probability density function\nassociated with the distribution $\\mathcal{P}$, the _systematic component_,\ndefined by\n$$\n\\boldsymbol{\\eta} = \\mathbf{X} \\boldsymbol{\\beta},\n$$ {#eq-syst-comp}\nand a (canonical) link function $g(\\cdot)$ such that\n$$\n\\boldsymbol{\\mu} = g(\\boldsymbol{\\eta}).\n$$ {#eq-link-f}\nOnce we estimate the parameters $\\boldsymbol{\\beta}$, we have\n$$\n\\hat{\\mathbf{y}} = g( \\mathbf{X} \\hat{\\boldsymbol{\\beta}} ).\n$$\n\nDefine now our Neural Network model having a single hidden layer with $M$\nunits. The activation function for the hidden layer is $f_h(x) = g(x)$, that is,\nthe same as the identity function. The weights we want to find are\n$\\mathbf{W}^{(1)} \\in {\\rm I\\!R}^{p \\times 1}$, and $\\mathbf{W}^{(2)} \\in\n{\\rm I\\!R}^{M \\times n}$. The activation function for the activation layer is\nthe previously mentioned canonical link function. Finally, let the loss be the\ndeviance residual associated with the distribution $\\mathcal{P}$, and the cost\nfunction be the average of the losses. Then, the mathematical representation of\nthe Neural Network becomes\n$$\n\\mathbf{z}^{(1)} = \\mathbf{X} \\mathbf{W}^{(1)} = \\mathbf{a}^{(1)},\n$$ {#eq-example-glm-nn-linear}\nbecause the activation function for the hidden layer is the identity.\nThen, we have\n$$\n\\mathbf{z}^{(2)} = \\mathbf{a}^{(1)} \\mathbf{W}^{(2)}\n$$ {#eq-example-glm-nn-hidden}\n$$\n\\mathbf{y} = \\mathbf{a}^{(2)} = g( \\mathbf{\\mathbf{z}^{(2)}} ).\n$${#eq-example-glm-nn-out}\nHowever, note that, by combining [-@eq-example-glm-nn-linear],\n[-@eq-example-glm-nn-hidden], and [-@eq-example-glm-nn-out] we get\n\\begin{align*}\n\\mathbf{y} & = g( \\mathbf{\\mathbf{z}^{(2)}} ) \\\\\n& = g( \\mathbf{a}^{(1)} \\mathbf{W}^{(2)} ) \\\\\n& = g( \\mathbf{X} \\underbrace{\\mathbf{W}^{(1)} \\mathbf{W}^{(2)}}_{{\\rm I\\!R}_{p\n\\times 1}} ),\n\\end{align*}\nwhich yields to optimal weights (see [Fitting a Neural Network](#subsec:fit-nn)\n and [Backpropagation](#subsec:backpro), for more information on how to fit a\n neural network model) satisfying\n$$\n\\underbrace{\\mathbf{W}^{(1)} \\mathbf{W}^{(2)}}_{{\\rm I\\!R}_{p\n\\times 1}} = \\hat{\\boldsymbol{\\beta}},\n$$\nwhere $\\hat{\\boldsymbol{\\beta}}$ is the Maximum Likelihood Estimator for\n$\\boldsymbol{\\beta}$ that can be obtained using the Iterative Reweighted Least\nSquares for the model defined by the probability density function associated\nwith the distribution $\\mathcal{P}$, the systematic component\n[-@eq-syst-comp] and a (canonical) link function [-@eq-link-f].\n\n### Cost functions\n\nWhenever we want to fit a neural network to a dataset we need to specify a\n_Cost_ function, which is usually based on _loss_ functions. A loss function, in\nthe context of Neural Network models, measures how far our predictions\n$f(\\mathbf{x}; \\mathbf{W})$ are from the true value $y$. Examples of commonly\nused loss functions, for a single observation, are the mean square error loss\nand the binomial deviance defined, respectively, as\n$$\nL(\\mathbf{w}, \\mathbf{x}; y) = \\frac{1}{2} (f(\\mathbf{x}; \\mathbf{w}) - y)^{2},\n$$ {#eq-loss-mse}\nand\n$$\nL(\\mathbf{W}, \\mathbf{x}; y) = y \\log \\left( \\frac{y}{f(\\mathbf{x}, \\mathbf{w})}\n\\right) + \n(1 - y) \\log \\left( \\frac{1 - y}{1 - f(\\mathbf{x}, \\mathbf{w})} \\right).\n$$ {#eq-loss-bdev}\nThe loss function [-@eq-loss-mse] is usually employed when the output (response)\nvariable assumes continuous values, while the [-@eq-loss-bdev] is used for binary\noutput variables.\n\nAfter choosing an appropriate loss function, the cost function is defined as the\naverage of the loss function over all the observation, that is\n$$\nC(\\mathbf{y}; \\mathbf{x}, \\mathbf{W}) = \\frac{1}{n} \\sum_{i = 1}^{n}\nL(\\mathbf{w_i, \\mathbf{x}_i; y_i}) + \\lambda J(\\mathbf{W}),\n$$\nwhere $J(\\mathbf{W})$ is a non-negative regularization term and $\\lambda \\geq\n0$ is a tuning parameter. \n\nIn practice, we may have a regularization term for each layer, each one having\nits own $\\lambda$. Some commonly used regularization terms are \n$$\nJ(\\mathbf{W}) = \\frac{1}{2} \\sum_{k = 1}^{K - 1} \\lVert \\mathbf{w}^{(k)} \\rVert^2,\n$$\nand\n$$\nJ(\\mathbf{W}) = \\frac{1}{2} \\sum_{k = 1}^{K - 1} \\lVert \\mathbf{w}^{(k)} \\rVert,\n$$\nwhere $K$ is the number of layers of our neural network model, and\n$\\mathbf{w}^{(k)}$ is the vector of weights from the units in the layer $L_k$ to\nthe layer $L_{k + 1}$. Note that, these two regularization terms are analogous\nto the Ridge and Lasso penalizations, and they play the exact same role in\nneural networks as its analogous versions do for the linear models\n[@efron2016computer]. Mixtures of these two regularization terms, as in the\nelastic net [@zou2005regularization], are also common.\n\n### Fitting a Neural Network  {#subsec:fit-nn}\n\nSupposing a user has set the number of layers, units, an activation function and\na loss function, to fit a neural network we seek the set of weights $\\mathbf{W}\n= \\{ \\mathbf{W}^{(1)}, \\ldots, \\mathbf{W}^{(k - 1)} \\}$ such that the cost\nfunction is minimized, that is\n$$ \n\\min_{\\mathbf{W}} \\left \\{ \\mathcal{C}(\\mathbf{y}; \\mathbf{X}, \\mathbf{W}) \\right \\}.\n$$\nTherefore, the neural network fit has turned into an optimization problem. The\nmost common algorithm used to solve this optimization problem is the\nBackpropagation algorithm, which is described in the next section for a general\nsituation.\n\n### Backpropagation {#subsec:backpro}\n\nBackpropagation (or gradient descent) is the method used to find the weights\nwhich minimize the chosen cost and loss functions for a given neural network. It\nis an iterative algorithm that is guaranteed to converge whenever the cost\nfunction has a single local minima [@efron2016computer]. However, even if the\ncost function does not have a single local minima, the algorithm works fairly\nwell. The updates for a weight matrix, $\\mathbf{W}^{(k)}$ let's say, is done as\nfollows\n$$\n\\mathbf{W}^{(k)} = \\mathbf{W}^{(k)} - \\alpha \\frac{\\partial\n\\mathcal{C}(\\mathbf{y}; \\mathbf{X}, \\mathbf{W})}{\\partial \\mathbf{W}^{(k)}},\n$$ {#eq-iter}\nwhere $\\alpha$ is a tuning parameter called _learning rate_.  The name\nbackpropagation comes from the fact that the derivatives (or gradients) are\ncomputed according to something called a _computation graph_ in a backward\nfashion. It is heavily based on the chain rule for differentiation.\n\nGiven initial values for the $\\mathbf{W}$ matrices, the method repeats the\nupdate rule [-@eq-iter] until convergence. Provided that the columns of the design\nmatrix are rescaled to mean 0 and variance 1, @hastie2009elements suggest the\nuse of random starting values for the weights as uniform random variables on the\ninterval $[-.75, .75]$.\n\n## Implementation {#sec:imple}\n\nI created functions for the implementation of a Neural Network with a single\nhidden layer model for generic activation functions. The implementation\nconsiders the cost function defined as\n$$\nC(\\mathbf{y}; \\mathbf{X}, \\mathbf{Y}) = \\frac{1}{n} \\lVert \\mathbf{y} -\n\\hat{\\mathbf{y}} \\rVert^2.\n$$\n\nThe inputs for the implemented function are:\n\n* A design matrix $\\mathbf{X}$, including the columns of ones for the intercept;\n\n* A column vector $\\mathbf{y}$ containing the response variable;\n\n* The number of units for the hidden layer;\n\n* The activation function for the hidden layer;\n\n* The activation function for the output layer;\n\n* A scalar for the learning rate $\\alpha$;\n\n* Two control parameters for the convergence of the algorithm. The maximum\n  number of iterations allowed, and a relative error $\\epsilon$ which controls\n  when to stop the iteration algorithm.\n\nThe function returns a `list` of size 5. Its first element is the predicted\nvector for $\\mathbf{y}$, the second contains the values of the cost function for\neach iteration of the algorithm. The third position of this `list` stores the\nweight matrices $\\mathbf{W}^{(1)}$ and $\\mathbf{W}^{(2)}$, while the last two\npositions store the number of iterations until attain the convergence and a\nstring indicating whether the algorithm converged or not, respectively.\n\nSee below the implementation of some activation functions (and their\nderivatives)\n\n::: {.cell}\n\n```{.r .cell-code}\n##--- activation functions and their derivatives ----\n\n## ReLU\nrelu <- function(x) {\n    pmax(x, 0)\n}\n\n## derivative leaky ReLU\nd_relu <- function(x) {\n    ifelse(x > 0, 1, 0)\n}\n\n## leaky ReLU\nlrelu <- function(x) {\n    pmax(x * .01, x)\n}\n\n## derivative leaky ReLU\nd_lrelu <- function(x) {\n    ifelse(x > 0, 1, .01)\n}\n\n## derivative tanh\nd_tanh <- function(x) {\n    1 - (tanh(x)^2)\n}\n\n## derivative logit\nd_logit <- function(x) {\n    plogis(x) * ( 1 - plogis(x) )\n}\n\n## derivative identity\nd_ident <- function(x) {\n    pmax( -2 * abs(x), 1 )\n}\n```\n:::\n\n\nNow, let's implement some helper functions to fit our neural network\nmodels. First, the cost function used in our examples is given by\n\n::: {.cell}\n\n```{.r .cell-code}\n## cost function\ncost_f <- function(y, yhat) {\n    crossprod(yhat - y) / NROW(y)\n}\n```\n:::\n\n\nThe implementation of the functions that will need to be executed at each step\nof the optimization algorithm are defined below. `compute_nn` computes the\nhidden layers given the matrix of covariates (or features) `X`, the list\ncontaining the the weights `W` associated to each layer connection, and two\nactivation functions `act_hidden` and `act_out` for the hidden and output\nlayers, respectively (this is a the implementation for a 2 layers network).  The\n`compute_grad` function computes the gradient and needs some further information\nlike `y` (the response variable), `n` the sample size, and the derivatives of\nthe activation functions. `update_aux` and `update_w` are helper functions used\nto update the weights.\n\n::: {.cell}\n\n```{.r .cell-code}\n##--- functiosn to fit the neural network ----\n\n## computing the forward step of the neural network\ncompute_nn <- function(X, W, act_hidden, act_out) {\n    Z <- vector(mode = \"list\", length = 2)\n    \n    Z[[1]] <- X %*% W[[1]]\n    A <- act_hidden(Z[[1]])\n\n    Z[[2]] <- A %*% W[[2]]\n\n    return( list(y = act_out(Z[[2]]),\n                 z = Z) )\n}\n\n## computing the gradient of the neural network\ncompute_grad <- function(y, X, W, act_hidden, act_out,\n                         d1_hidden, d1_out, n) {\n    nn    <- compute_nn(X, W, act_hidden, act_out)\n    aux_out <- (nn$y - y) * d1_out(nn$z[[2]])\n    aux_hid <- tcrossprod(aux_out, W[[2]]) *\n        d1_hidden(nn$z[[1]])\n    \n    return(\n        list(crossprod(X, aux_hid) / n,\n             crossprod(act_hidden(nn$z[[1]]), aux_out) / n)\n    )\n}\n\n## aux function for updating W\nupdate_aux <- function(w, dw, alpha) {\n    w - alpha * dw\n}\n\n## update the weights of a neural network\nupdate_w <- function(W, alpha, y, X, act_hidden, act_out,\n                     d1_hidden, d1_out, n) {\n\n    grad_w <- compute_grad(y, X, W, act_hidden, act_out,\n                           d1_hidden, d1_out, n)\n    \n    return( Map(f = update_aux, w = W,\n                dw = grad_w, alpha = alpha) )\n}\n```\n:::\n\n\nFinally, all these functions previously describer are used to build the `fit_nn`\nfunction (which is used to compute the optimal weights for the neural\nnetwork). The `alpha` is the $\\alpha$ previously mentioned in this post, `maxit`\nand `eps` are parameters used in the optimization process.  The first one stands\nfor the maximum number of iterations to be used in the optimization process,\nwhile the second stand for the \"optimization error\". That is, if, from one\niteration to another, the change between the weights does not exceed `eps`, then\nwe consider that the algorithm converged and a (maybe local) optimum has been\nfound.\n\n::: {.cell}\n\n```{.r .cell-code}\nfit_nn <- function(y, X, hid_units,\n                   act_hidden, act_out,\n                   d1_hidden, d1_out,\n                   alpha = .25,\n                   maxit = 500L,\n                   eps   = 1e-05) {\n    m <- hid_units\n    p <- ncol(X)\n    N <- NROW(y)\n\n    \n    W <- list(matrix(runif(m * p, -.75, .75),\n                     ncol = m, nrow = p),\n              matrix(runif(m, -.75, .75), ncol = 1))\n\n    nn   <- vector(mode = \"list\", length = maxit)\n    cost <- vector(mode = \"numeric\", length = maxit)\n\n    ## initialiazing\n    nn[[1]] <- compute_nn(X, W, act_hidden, act_out)\n\n    cost[1] <- cost_f(y, nn[[1]]$y)\n    \n    for( i in seq_len(maxit)[-1] ) {\n        W <- update_w(W, alpha, y, X,\n                      act_hidden, act_out,\n                      d1_hidden, d1_out,\n                      n = N)\n        \n        nn[[i]] <- compute_nn(X, W, act_hidden, act_out)\n        cost[i] <- cost_f(y, nn[[i]]$y)\n        \n        if( abs(cost[i] - cost[i - 1]) < eps ) {\n            output <- list(nn   = nn[[i - 1]],\n                           cost = cost[1:(i - 1)],\n                           W    = W,\n                           it   = (i - 1),\n                           conv = \"yes\")\n            break\n        }\n    }\n\n    if( i == maxit ) {\n        output <- list(yhat = nn[[maxit]]$y,\n                       cost = cost[1:maxit],\n                       W    = W,\n                       it   = maxit,\n                       conv = \"no\")\n    }\n\n    return(output)\n}\n```\n:::\n\n\nHaving all these functions, we can play with some numerical examples!\n\n## Numerical Examples {#sec:ne}\n\n\n::: {.cell}\n\n:::\n\n\n### Example 1: Equivalence between Neural Network and Linear Model\n\n\n::: {.cell}\n\n:::\n\n\nConsider a simulated dataset where\n$$\n\\mathbf{y} \\sim N(\\mathbf{X} \\boldsymbol{\\beta}, \\sigma^2 \\mathbf{I}_n),\n$$\nwhere $\\mathbf{X} \\in {\\rm I\\!R}^{n \\times 3}$, with the first column being the\nintercept term. To simulate the model we used $\\boldsymbol{\\beta} = (2, 3,\n1.5)$. Additionally, suppose $n = 2000$. \n\nConsidering the identity function as the activation function for both layers,\nthe goal here is to show that the $\\mathbf{W}^{(1)} \\mathbf{W}^{(2)} =\n\\hat{\\boldsymbol{\\beta}}$, where $\\hat{\\boldsymbol{\\beta}}$ is the least squares\nsolution for a linear model established as $\\mathbf{y} = \\mathbf{X}\n\\boldsymbol{\\beta}$, and $\\mathbf{W}^{(1)}, \\, \\mathbf{W}^{(2)}$ are the optimal\nweights according to the Neural Network fitted to the data, as proved in the\nsubsection \\@ref(subsec:act).\n\n@tbl-ls displays the results from the simulated example. The two different\napproaches have yielded the exactly same results. If we were to make\npredictions, the two methods would provide the same predicted values under these\ncircumstances.\n\n\n::: {#tbl-ls .cell tbl-cap='Comparing the LS solution and the product of the neural network weight matrices.'}\n::: {.cell-output-display}\n| $\\hat{\\boldsymbol{\\beta}}$ | $\\mathbf{W}^{(1)} \\mathbf{W}^{(2)}$ |\n|:--------------------------:|:-----------------------------------:|\n|           2.996            |                2.996                |\n|           3.004            |                3.004                |\n|           1.512            |                1.512                |\n:::\n:::\n\n\nSee below the code used on this example.\n\n::: {.cell}\n\n```{.r .cell-code}\n##--- numerical examples ----\n\n##--- example 1 ----\n\nset.seed(123)\n\nn <- 2000\n\nx1 <- rnorm(n)\nx2 <- as.numeric( scale( rexp(n) ) )\n\ny <- 3 + 3 * x1 + 1.5 * x2 + rnorm(n, sd = .5)\n\nmy_x <- cbind( rep(1, n), x1, x2 )\ncolnames(my_x) <- NULL\n\ndt <- as.data.frame( cbind(y, my_x[, 2:3]) )\nnames(dt) <- c(\"y\", \"x1\", \"x2\")\n\nm <- 6\n\nfit_1 <-\n    fit_nn(y = y, X = my_x,\n           hid_units = m,\n           act_hidden = identity,\n           act_out    = identity,\n           d1_hidden  = d_ident,\n           d1_out     = d_ident,\n           alpha = .05,\n           maxit = 1000L,\n           eps   = 1e-16)\n\nbeta_hat <- coef(lm(y ~ x1 + x2, data = dt))\n\ntbl_1 <- as.data.frame(cbind(beta_hat,\n                             fit_1$W[[1]] %*% fit_1$W[[2]]))\nnames(tbl_1) <- c(\"$\\\\hat{\\\\boldsymbol{\\\\beta}}$\",\n                  \"$\\\\mathbf{W}^{(1)} \\\\mathbf{W}^{(2)}$\")\nrownames(tbl_1) <- NULL\n```\n:::\n\n\n### Example 2: Nonlinear relationship and number of hidden units\n\n\n::: {.cell}\n\n:::\n\n\nConsider now the following model\n$$\ny_i = \\beta_0 + \\beta_1 (x^2) + \\varepsilon_i.\n$$\n\nIn practice, we do not know before-hand the relationship between the response\nand explanatory variables is not linear. In fig-fit-nn2, we show the\nfitted curves the linear model and for neural networks under different settings\nfor a dataset simulated from this example. The Neural Network deals nicely with\nthe nonlinearity at the cost of possibly overfit the data.\n\n::: {.cell}\n::: {.cell-output-display}\n![Different models fitted to the same simulated dataset.](2021-06-23-lmnnet_files/figure-html/fig-fit-nn2-1.png){#fig-fit-nn2 width=672}\n:::\n:::\n\n\nSee the code used on this example below.\n\n::: {.cell}\n\n```{.r .cell-code}\n##--- example 2 ----\n\nset.seed(124)\n\nx12 <- rnorm(n)\n\ny2 <- 5 - 2.5 * (x12^2) + rnorm(n, sd = .5)\n\nmy_x2 <- cbind(rep(1, n), x12)\ncolnames(my_x2) <- NULL\n\ndt2 <- as.data.frame( cbind(y2, my_x2[, 2]) )\nnames(dt2) <- c(\"y\", \"x1\")\n\nn_pred <- 4000\n\n## fitting linear model\n\nmy_lm2 <- lm(y ~ x1, data = dt2)\n\n## fitting neural network with tanh as the activation function for the hidden\n## layer - 5 hidden units\nfit_2 <-\n    fit_nn(y = y2, X = my_x2,\n           hid_units  = 5,\n           act_hidden = tanh,\n           act_out    = identity,\n           d1_hidden  = d_tanh,\n           d1_out     = d_ident,\n           alpha = .05,\n           maxit = 1000L,\n           eps   = 1e-04)\n\n## fitting neural network with tanh as the activation function for the hidden\n## layer - 15 hidden units\nfit_3 <-\n    fit_nn(y = y2, X = my_x2,\n           hid_units  = 15,\n           act_hidden = tanh,\n           act_out    = identity,\n           d1_hidden  = d_tanh,\n           d1_out     = d_ident,\n           alpha = .05,\n           maxit = 1000L,\n           eps   = 1e-04)\n\n## fitting neural network with leaky ReLU as the activation function for the\n## hidden layer - 10 hidden units\nfit_4 <-\n    fit_nn(y = y2, X = my_x2,\n           hid_units  = 10,\n           act_hidden = lrelu,\n           act_out    = identity,\n           d1_hidden  = d_lrelu,\n           d1_out     = d_ident,\n           alpha = .05,\n           maxit = 1000L,\n           eps   = 1e-04)\n\npred_data <- data.frame(x = seq(from = min(x12), to = max(x12),\n                                length.out = n_pred))\n\npred_data <- transform(pred_data,\n                       lm = coef(my_lm2)[[1]] + coef(my_lm2)[[1]] * x)\n\npred_data <- transform(pred_data,\n                       nn_tanh_1 =\n                           compute_nn(X = cbind(rep(1, 4000),\n                                                x),\n                                      W = fit_2$W,\n                                      act_hidden = tanh,\n                                      act_out = identity)$y)\n\npred_data <- transform(pred_data,\n                       nn_tanh_2 =\n                           compute_nn(X = cbind(rep(1, 4000),\n                                                x),\n                                      W = fit_3$W,\n                                      act_hidden = tanh,\n                                      act_out = identity)$y)\n\npred_data <- transform(pred_data,\n                       nn_lrelu =\n                           compute_nn(X = cbind(rep(1, 4000),\n                                                x),\n                                      W = fit_4$W,\n                                      act_hidden = lrelu,\n                                      act_out = identity)$y)\n\nsetDT(pred_data)\n\npred_data <- melt(pred_data, id = 1,\n                  value.name = \"pred\",\n                  variable.name = \"method\")\n\npred_data[, method := fcase(method == \"nn_tanh_1\", \"tanh - 5\",\n                            method == \"nn_tanh_2\", \"tanh - 15\",\n                            method == \"nn_lrelu\", \"leaky ReLU - 10\",\n                            default = \"lm\")]\n\nggplot(data = pred_data) +\n    geom_point(data = dt2, aes(x = x1, y = y),\n               alpha = .5) +\n    geom_line(aes(x = x, y = pred, color = method),\n              lwd = 1.05) +\n    scale_color_discrete(name = NULL) +\n    theme_bw() +\n    theme(\n        legend.position = \"bottom\",\n        legend.margin = margin(6, 6, 6, 6)\n    ) +\n    labs(x = \"X\", y = \"Y\")\n```\n:::\n\n\n## Final Thoughts\n\nThe Neural Network Regression models are very interesting but certainly are not\nmagical as it is sold in the market. By the end of the day, these models consist\nof simple linear algebra allied to the use of element-wise nonlinear functions\nand optimization algorithms. Speaking on optimization algorithm, the gradient\ndescent looks like a fixed-point iteration algorithm. These kind of algorithms\nhave the advantage of not need the second derivative of the functions, however\ntheir convergence can be slow. I believe that using different learning rates for\ndifferent parameters could improve the speed on which the algorithm converges.\n\nAlthough these models do not make any distributional assumption on the data, we\ncan easily make it more suitable for certain distributions by working with the\ncost and activation functions on an appropriate fashion.\n\nThere are several variants of these models suited for different problems, like\ntext and image classification, for example. The idea is the same, what changes\nis the way the researchers deal with the hidden layers. I think an interesting\napplication is to try to use neural networks to estimate non-parametrically\ncovariance matrices for spatial data.\n\n# References\n",
    "supporting": [
      "2021-06-23-lmnnet_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}